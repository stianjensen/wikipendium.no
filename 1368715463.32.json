[{"pk": 10, "model": "contenttypes.contenttype", "fields": {"model": "article", "name": "article", "app_label": "wiki"}}, {"pk": 11, "model": "contenttypes.contenttype", "fields": {"model": "articlecontent", "name": "article content", "app_label": "wiki"}}, {"pk": 4, "model": "contenttypes.contenttype", "fields": {"model": "contenttype", "name": "content type", "app_label": "contenttypes"}}, {"pk": 2, "model": "contenttypes.contenttype", "fields": {"model": "group", "name": "group", "app_label": "auth"}}, {"pk": 7, "model": "contenttypes.contenttype", "fields": {"model": "logentry", "name": "log entry", "app_label": "admin"}}, {"pk": 9, "model": "contenttypes.contenttype", "fields": {"model": "migrationhistory", "name": "migration history", "app_label": "south"}}, {"pk": 1, "model": "contenttypes.contenttype", "fields": {"model": "permission", "name": "permission", "app_label": "auth"}}, {"pk": 8, "model": "contenttypes.contenttype", "fields": {"model": "registrationprofile", "name": "registration profile", "app_label": "registration"}}, {"pk": 5, "model": "contenttypes.contenttype", "fields": {"model": "session", "name": "session", "app_label": "sessions"}}, {"pk": 6, "model": "contenttypes.contenttype", "fields": {"model": "site", "name": "site", "app_label": "sites"}}, {"pk": 3, "model": "contenttypes.contenttype", "fields": {"model": "user", "name": "user", "app_label": "auth"}}, {"pk": "7a8372bedc65d19412b96f2f786df334", "model": "sessions.session", "fields": {"expire_date": "2013-02-17T03:40:30.700Z", "session_data": "ZmIwMjBlYWUzN2JjOGNiNjM2NDQ1YmRiOGFiNzJhZWUyNzM2MzBlNDqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLAXUu\n"}}, {"pk": "fc52cb23615120ea8d1b1789986e2dfa", "model": "sessions.session", "fields": {"expire_date": "2013-02-17T03:35:54.706Z", "session_data": "NmY2Njg4OWNjMDBjYzQ3NzFiMTFkNTdkZTUyOWRiNzM2ZTlmNjE4YjqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsCdS4=\n"}}, {"pk": "d323c1dad43112bfd8c5c76a4ba1d859", "model": "sessions.session", "fields": {"expire_date": "2013-02-17T03:46:44.590Z", "session_data": "ZmIwMjBlYWUzN2JjOGNiNjM2NDQ1YmRiOGFiNzJhZWUyNzM2MzBlNDqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLAXUu\n"}}, {"pk": "a240c2c4e73766aa08afc9ba8017d6c6", "model": "sessions.session", "fields": {"expire_date": "2013-02-17T04:54:39.647Z", "session_data": "NGUyZGE3OTY3ZjY4YWNjZDgwOTE4OGYzNWJkNTZjZTFiYWYyY2YzZDqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZHECVRJfYXV0aF91c2VyX2JhY2tlbmRVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kVQ1fYXV0aF91c2VyX2lkSwF1Lg==\n"}}, {"pk": "3093a9a582acbcc630fcbc03e993b7d6", "model": "sessions.session", "fields": {"expire_date": "2013-02-17T08:06:54.521Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3f0d9ccce8e909ee56b4d45b2845630b", "model": "sessions.session", "fields": {"expire_date": "2013-02-17T08:08:12.760Z", "session_data": "ZDliMWFjNTZlZDJlZmU0ZjhjMzVmNjY3ODZiNTgzYzc2ZWVmNDA4ZjqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsDdS4=\n"}}, {"pk": "e4a3369912b48dcf0dcc35e25ec577ba", "model": "sessions.session", "fields": {"expire_date": "2013-02-17T08:13:07.060Z", "session_data": "MzE4ZjAwNzAzYzQyNDViMTEyMmRhNGEzZWFiNzk3OTgwZDNlOTc0NDqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "6257dc2569fee698014588d2d5afa6ca", "model": "sessions.session", "fields": {"expire_date": "2013-02-19T01:23:08.568Z", "session_data": "MzE4ZjAwNzAzYzQyNDViMTEyMmRhNGEzZWFiNzk3OTgwZDNlOTc0NDqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "ad032efd798710900c5eb3b99f7679ea", "model": "sessions.session", "fields": {"expire_date": "2013-02-27T22:59:28.718Z", "session_data": "MzE4ZjAwNzAzYzQyNDViMTEyMmRhNGEzZWFiNzk3OTgwZDNlOTc0NDqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "69215b93eb5eca113aeee8271f53a882", "model": "sessions.session", "fields": {"expire_date": "2013-03-08T22:28:39.502Z", "session_data": "MzE4ZjAwNzAzYzQyNDViMTEyMmRhNGEzZWFiNzk3OTgwZDNlOTc0NDqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "7141bd29b79e6cb0d35c9d59492ebe2f", "model": "sessions.session", "fields": {"expire_date": "2013-02-18T05:15:44.816Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4112dac5dcba130dc030d3b4bb3efe02", "model": "sessions.session", "fields": {"expire_date": "2013-02-18T05:15:47.014Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "abe69ded11531b7e2e251f81fd916f3e", "model": "sessions.session", "fields": {"expire_date": "2013-02-18T05:27:11.908Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "58660239f0278fec586f3fbe574ae16a", "model": "sessions.session", "fields": {"expire_date": "2013-02-18T13:53:08.488Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e65d44a794d86bd66a083a5017357fe1", "model": "sessions.session", "fields": {"expire_date": "2013-02-18T16:50:32.194Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "43664a4ab34efe2b3cdd48689bd52f58", "model": "sessions.session", "fields": {"expire_date": "2013-02-18T20:13:15.478Z", "session_data": "ZDMxNjZmOThiN2E4ZjMxZTBhYWUwMGJhNzc4ZWU4ZGE3YjQzZTQ0ZjqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "988f0f5a8ede5cc296a4a66f55bf99e0", "model": "sessions.session", "fields": {"expire_date": "2013-02-18T23:10:09.652Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b5aa8229127191391946b33238d4fa34", "model": "sessions.session", "fields": {"expire_date": "2013-02-20T10:47:17.407Z", "session_data": "MjgxNWY2YjQzMWZhNDllOTIyZmQ0ZjJiY2IzZTFlODgxODliYWQ1YTqAAn1xAShVCnRlc3Rjb29r\naWVxAlUGd29ya2VkcQNVDV9hdXRoX3VzZXJfaWRLAlUSX2F1dGhfdXNlcl9iYWNrZW5kVSlkamFu\nZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHUu\n"}}, {"pk": "9215dc7c2618c7dcb7e1e06f8cae0981", "model": "sessions.session", "fields": {"expire_date": "2013-02-19T04:05:11.944Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "03d5a02dfe5b9277689c30d67dbc96fa", "model": "sessions.session", "fields": {"expire_date": "2013-02-20T18:31:51.996Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "babb87c48644255ba63675c6017c3457", "model": "sessions.session", "fields": {"expire_date": "2013-02-21T13:53:38.626Z", "session_data": "ZDMxNjZmOThiN2E4ZjMxZTBhYWUwMGJhNzc4ZWU4ZGE3YjQzZTQ0ZjqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "52c9132cb1ae512eac0cfb1461f39f74", "model": "sessions.session", "fields": {"expire_date": "2013-02-20T21:29:40.987Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "eccb164a48681d28971dc353219bb28b", "model": "sessions.session", "fields": {"expire_date": "2013-02-20T21:32:21.259Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "99b0b2784eef7f57d6e68e5298d34367", "model": "sessions.session", "fields": {"expire_date": "2013-02-21T16:50:37.051Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c72e2b18dc3763b942e51d78cb698864", "model": "sessions.session", "fields": {"expire_date": "2013-02-21T20:11:46.409Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "91641fe2c21fafc803d057e133a35737", "model": "sessions.session", "fields": {"expire_date": "2013-02-21T23:06:12.592Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7a2a632e1907a82c585abdaab8dfb3dd", "model": "sessions.session", "fields": {"expire_date": "2013-02-21T23:10:17.083Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2c74474648eee7a92c761763567de660", "model": "sessions.session", "fields": {"expire_date": "2013-02-22T05:07:23.384Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "16433ec4c94625eaee28c4c80cf90178", "model": "sessions.session", "fields": {"expire_date": "2013-02-22T07:22:26.602Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "58b4784ae26e90dde01811a9a72c745a", "model": "sessions.session", "fields": {"expire_date": "2013-02-22T10:44:49.276Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "052b8763ad235e87860a06755e58ab6b", "model": "sessions.session", "fields": {"expire_date": "2013-02-22T11:07:29.009Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c4ae41e71dcfc22e135b4fe2ac99ca7f", "model": "sessions.session", "fields": {"expire_date": "2013-02-22T12:10:10.502Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2d402e25bbe99eb2cc41f49988003fba", "model": "sessions.session", "fields": {"expire_date": "2013-02-22T16:59:02.101Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1bd7f264802668068aba1abf4f02d9f2", "model": "sessions.session", "fields": {"expire_date": "2013-02-22T19:50:32.457Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a3d3ff6f23425b6a96a0c4bc5755a6b8", "model": "sessions.session", "fields": {"expire_date": "2013-02-22T22:19:25.313Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ec1e2dd5a044daeeccdab81fe8871f1c", "model": "sessions.session", "fields": {"expire_date": "2013-02-22T22:52:21.280Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b98662d4838c576f39583b4996213108", "model": "sessions.session", "fields": {"expire_date": "2013-02-23T13:17:21.080Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "804a77c5d5c26c03454c73c2358ea623", "model": "sessions.session", "fields": {"expire_date": "2013-02-23T14:23:12.320Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "98c2e7cc717c5cac0953a4a0e1306e96", "model": "sessions.session", "fields": {"expire_date": "2013-02-23T19:49:42.443Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "752cec8af6f68fd4466c64ea010f1fce", "model": "sessions.session", "fields": {"expire_date": "2013-02-24T04:10:35.735Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d8547162837c93cabc1337dacfcf7c1b", "model": "sessions.session", "fields": {"expire_date": "2013-02-24T11:35:22.739Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6b97e07ce525c947044c8243c3c4f2ec", "model": "sessions.session", "fields": {"expire_date": "2013-02-24T13:25:59.257Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "be0c3712869e61e996e08f8279b32dc7", "model": "sessions.session", "fields": {"expire_date": "2013-02-24T14:35:19.912Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e9689ab7a20704d6e17ec73c692f0b7c", "model": "sessions.session", "fields": {"expire_date": "2013-02-24T16:53:02.302Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0323dbedc0d14b9630e562f968693769", "model": "sessions.session", "fields": {"expire_date": "2013-02-25T07:16:15.182Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "270917114b9bd0c06b19913395da1e39", "model": "sessions.session", "fields": {"expire_date": "2013-02-25T16:59:03.348Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bff5c0446d2302a929dd03372bc4172b", "model": "sessions.session", "fields": {"expire_date": "2013-02-25T19:18:23.539Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3cd62db3680a69a840c145889f2e23f9", "model": "sessions.session", "fields": {"expire_date": "2013-02-25T22:19:28.239Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "194cbabf14f5190ae98a4e0fa79141fa", "model": "sessions.session", "fields": {"expire_date": "2013-02-25T23:35:22.163Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c2a2b4c680d86e98dec7ccfa64cdfdc1", "model": "sessions.session", "fields": {"expire_date": "2013-02-26T05:46:13.443Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "45a1ebf9f36f21e6d08999320fc8905f", "model": "sessions.session", "fields": {"expire_date": "2013-02-26T06:53:29.171Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5c3e6eaa0ec138879761bebb4a43483b", "model": "sessions.session", "fields": {"expire_date": "2013-02-26T11:11:57.895Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f3ce7e85a8042e93c21ded691a728bb2", "model": "sessions.session", "fields": {"expire_date": "2013-02-26T11:36:54.209Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ece7cf906f8c91ae55c6071e9c9da611", "model": "sessions.session", "fields": {"expire_date": "2013-02-26T12:38:43.149Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c17175aff95933f67109bd16cd3f7bb6", "model": "sessions.session", "fields": {"expire_date": "2013-02-26T13:04:36.870Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "408a232bef51a44cedfce75617bf67d1", "model": "sessions.session", "fields": {"expire_date": "2013-02-26T16:18:50.896Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e9add84cc389b7989d61a9c3978e64be", "model": "sessions.session", "fields": {"expire_date": "2013-02-26T17:46:42.497Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9ebda4a88b859963d401033309db8638", "model": "sessions.session", "fields": {"expire_date": "2013-02-26T23:32:58.446Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ec0cf9ac99623be3cb5fe837794369d2", "model": "sessions.session", "fields": {"expire_date": "2013-02-27T05:15:43.385Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "946209438a5f028c29e9d368c456bfa4", "model": "sessions.session", "fields": {"expire_date": "2013-03-04T08:45:39.674Z", "session_data": "ZDMxNjZmOThiN2E4ZjMxZTBhYWUwMGJhNzc4ZWU4ZGE3YjQzZTQ0ZjqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "00088ca37389422711b5c769943f0d7f", "model": "sessions.session", "fields": {"expire_date": "2013-02-27T13:56:39.132Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e10ae8d4ceaae8e1a5262797c568b4f0", "model": "sessions.session", "fields": {"expire_date": "2013-02-27T13:58:26.060Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "92883594ea7df5f1ac61763636bf38a4", "model": "sessions.session", "fields": {"expire_date": "2013-02-27T13:59:49.763Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "46aa1ed61954ed9c233e27fdf9b3f7f8", "model": "sessions.session", "fields": {"expire_date": "2013-02-27T19:25:04.142Z", "session_data": "ZDMxNjZmOThiN2E4ZjMxZTBhYWUwMGJhNzc4ZWU4ZGE3YjQzZTQ0ZjqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "55112c98a650ceddb5883230b84ca514", "model": "sessions.session", "fields": {"expire_date": "2013-02-27T15:00:48.666Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "76b54f13ddc5eb8c58ff0a524adc15e0", "model": "sessions.session", "fields": {"expire_date": "2013-02-27T15:50:07.508Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f69bae7ec7e7489cc8d7d0efd106cc46", "model": "sessions.session", "fields": {"expire_date": "2013-02-27T23:18:21.175Z", "session_data": "MzE4ZjAwNzAzYzQyNDViMTEyMmRhNGEzZWFiNzk3OTgwZDNlOTc0NDqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "c33b63d51ef3d3bd8f4ef1f60954d543", "model": "sessions.session", "fields": {"expire_date": "2013-02-27T23:19:01.867Z", "session_data": "ZjM5ZjQyNDQwNjFkMmQ0YTg0OGIwZDE0ZTA2ZTFlMjRkNTQxOGI3MjqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLAnUu\n"}}, {"pk": "d9c6cb136095d96059c7c8bb9d888043", "model": "sessions.session", "fields": {"expire_date": "2013-02-28T02:43:33.025Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dcd15c9e4dcca765321c2a714e82f7f0", "model": "sessions.session", "fields": {"expire_date": "2013-02-28T02:43:33.858Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e1ba5cffa79c3eca9a747a624761120c", "model": "sessions.session", "fields": {"expire_date": "2013-02-28T05:40:28.505Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5de9ce9d6f897110fcc9aa4df7aedd8f", "model": "sessions.session", "fields": {"expire_date": "2013-02-28T08:51:18.826Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7f018292a258e624af3ad42b5f7224e9", "model": "sessions.session", "fields": {"expire_date": "2013-02-28T11:41:50.874Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "80c41a8bcf8fd76500bf8cc95afe4090", "model": "sessions.session", "fields": {"expire_date": "2013-02-28T20:23:39.370Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2e539c5e1c6fc3447222a1a819829806", "model": "sessions.session", "fields": {"expire_date": "2013-02-28T20:23:41.112Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ecfb6e2f7eee12bd95c892ecfd53a17b", "model": "sessions.session", "fields": {"expire_date": "2013-02-28T23:19:58.301Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "18c6bdcef34b216d8ecefb4df150a3b4", "model": "sessions.session", "fields": {"expire_date": "2013-03-01T04:01:41.821Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1f44d874964343e0a30ccac082c05707", "model": "sessions.session", "fields": {"expire_date": "2013-03-01T17:46:49.798Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8d40065d27d4fd30f66d2836ced4833d", "model": "sessions.session", "fields": {"expire_date": "2013-03-01T18:33:12.303Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cb77ac47dd553451c7f22ced62fd17ee", "model": "sessions.session", "fields": {"expire_date": "2013-03-01T18:51:10.499Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "66527cfa41a5956df2eb729046013243", "model": "sessions.session", "fields": {"expire_date": "2013-03-02T20:14:05.365Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "505fbb92a6a2e0e2f290320572cdf488", "model": "sessions.session", "fields": {"expire_date": "2013-03-03T10:38:25.958Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "929e3aacbca1678c009442565116599f", "model": "sessions.session", "fields": {"expire_date": "2013-03-03T13:27:58.871Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "014a5db8fc1caa79c405e837214893a1", "model": "sessions.session", "fields": {"expire_date": "2013-03-03T14:41:03.423Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "75dc2bac1309e9fdaa47e9c0c930de44", "model": "sessions.session", "fields": {"expire_date": "2013-03-03T19:33:00.553Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "de19d1db3823b0aaf4e19ddea5197cce", "model": "sessions.session", "fields": {"expire_date": "2013-03-03T20:23:41.692Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "88714adb3cabd1a7c3e6753092954572", "model": "sessions.session", "fields": {"expire_date": "2013-03-03T20:23:46.410Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "45e1f65d85ce8856dc0e25ba11bab317", "model": "sessions.session", "fields": {"expire_date": "2013-03-04T01:20:49.725Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "73890a47cbfe49499684825c308f3898", "model": "sessions.session", "fields": {"expire_date": "2013-03-04T04:17:29.991Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bbca7e36bd66a11f881f04a1b75167dc", "model": "sessions.session", "fields": {"expire_date": "2013-03-04T17:11:58.378Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ce3c09dc9908f029c3671b04b86ef124", "model": "sessions.session", "fields": {"expire_date": "2013-03-04T17:45:57.005Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5d18b374558256fee908148c43e1672c", "model": "sessions.session", "fields": {"expire_date": "2013-03-04T17:46:51.371Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1dcfdf211a83d54a3bc41134a3feb41f", "model": "sessions.session", "fields": {"expire_date": "2013-03-04T17:46:52.073Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "469405315d8531e858630a87593a0156", "model": "sessions.session", "fields": {"expire_date": "2013-03-04T20:17:37.834Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8df08c8f1db6d9c10cd4bd199424a17b", "model": "sessions.session", "fields": {"expire_date": "2013-03-04T20:21:02.450Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "beeecc2d3e8a3036b39f4b8fadb67bdc", "model": "sessions.session", "fields": {"expire_date": "2013-03-05T02:01:02.457Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c0b8698d08b5064beebfbbc228092c81", "model": "sessions.session", "fields": {"expire_date": "2013-03-05T06:48:29.194Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "59f96377617cb4ad1849c09d0ffc0fc4", "model": "sessions.session", "fields": {"expire_date": "2013-03-05T14:36:40.429Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "08cd8e0ea6233d785e3a19bde18d4109", "model": "sessions.session", "fields": {"expire_date": "2013-03-20T22:37:05.313Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "477589579cf9f670afd918f86c509cfb", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T05:13:39.090Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0abae8c92196ccd82df5b7cd750a9479", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T05:39:35.501Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "78ae9cc9b17ad9fe3363fc55308ac8c9", "model": "sessions.session", "fields": {"expire_date": "2013-03-05T15:26:51.037Z", "session_data": "MGRjN2ZjYTNmNDM1NjcxMmU2OWE0ZGRlM2EyM2QxMjA4MWQxNjdkODqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLBXUu\n"}}, {"pk": "a1be5d6b1e74c56dd25400ae2dce6909", "model": "sessions.session", "fields": {"expire_date": "2013-03-05T15:27:10.163Z", "session_data": "MGRjN2ZjYTNmNDM1NjcxMmU2OWE0ZGRlM2EyM2QxMjA4MWQxNjdkODqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLBXUu\n"}}, {"pk": "872ee282e456bfce99459d01510eced9", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T05:51:11.630Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8cf0c15b08bc3423d938edcffe1e2d9f", "model": "sessions.session", "fields": {"expire_date": "2013-03-05T18:34:53.283Z", "session_data": "MjI3ODUzMTRjNWUzODNiMWVkNmFjNWI4ZmNkOTVhYWZiNTIwMDU1YjqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsGdS4=\n"}}, {"pk": "ef0098ca0906fa75a39347efc3c4f264", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T05:57:38.359Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1d448260b5af59b8a5f96eae18436b2c", "model": "sessions.session", "fields": {"expire_date": "2013-03-05T18:53:23.586Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f74c2edabbb0afe53e0c2fe4def901ab", "model": "sessions.session", "fields": {"expire_date": "2013-03-06T06:16:29.645Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "92ba22cd186ebc0d3c678cbe31d10df9", "model": "sessions.session", "fields": {"expire_date": "2013-03-06T07:02:37.332Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "320ae89ef2b6d7bda5020c6f56a43ae8", "model": "sessions.session", "fields": {"expire_date": "2013-03-06T10:23:31.045Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9656a988639e61045ac9d16b83f582d6", "model": "sessions.session", "fields": {"expire_date": "2013-03-06T13:00:11.849Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "49fbbbc46ea7201c83110f4ac460881e", "model": "sessions.session", "fields": {"expire_date": "2013-03-06T13:11:42.767Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3722622eb3f8cf966d183eacd9d50249", "model": "sessions.session", "fields": {"expire_date": "2013-03-06T14:23:31.408Z", "session_data": "MGI1ODJlNzJhZjFjODNlYzgwZDFhNmFhYjIxYWJhYjBmYjk5YTA3NDqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsIdS4=\n"}}, {"pk": "279ed75cc26beed30a6263764abb0f28", "model": "sessions.session", "fields": {"expire_date": "2013-03-06T20:23:48.267Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "869fd3dd7f973882ababcfc5c29bd80d", "model": "sessions.session", "fields": {"expire_date": "2013-03-06T20:23:51.189Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0265f43694a4cc13574f70332396165b", "model": "sessions.session", "fields": {"expire_date": "2013-03-07T08:26:20.582Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bc4afeeda76d3ae5b3dc22745b597f28", "model": "sessions.session", "fields": {"expire_date": "2013-03-07T08:26:21.039Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d366d23a5df380223de25cc7972cf054", "model": "sessions.session", "fields": {"expire_date": "2013-03-07T12:58:06.303Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e0b278943e5c76c524b56126988d4f3a", "model": "sessions.session", "fields": {"expire_date": "2013-03-07T13:32:38.645Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d839e19fc32f660f1513a87561341665", "model": "sessions.session", "fields": {"expire_date": "2013-03-07T17:46:52.659Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ceb1117ee44d35c50b3ce8c41c5a429b", "model": "sessions.session", "fields": {"expire_date": "2013-03-08T04:54:56.852Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c61064bb151c8e947f55780ba888393d", "model": "sessions.session", "fields": {"expire_date": "2013-03-08T14:09:28.564Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "84741e4acf3ed86c87f3e6d7cceb180c", "model": "sessions.session", "fields": {"expire_date": "2013-03-08T16:24:27.724Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ba166b2be8201d4128bb7eb46523e28b", "model": "sessions.session", "fields": {"expire_date": "2013-03-08T22:10:33.355Z", "session_data": "YjMwMzQ5ODQ2ZTgwZDY5NzY3MTAzMGUxMzMwNTBhMTBhYjk2OWQ5NjqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4af3c579c8abd13f0a14400f030e0ebf", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T06:15:16.837Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ed3b26c14c0b7061c3b445c165cb7e7e", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T06:21:44.817Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "87e39c5962b86e05b510c5029014cad4", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T06:40:03.477Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cd949a6e244a2bdd5a161142f45c2bd2", "model": "sessions.session", "fields": {"expire_date": "2013-03-09T14:12:10.719Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "94489f296eecf1717c588cac6b21a9b5", "model": "sessions.session", "fields": {"expire_date": "2013-03-09T20:23:54.745Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6977350f639979372ecd80fedb644b55", "model": "sessions.session", "fields": {"expire_date": "2013-03-11T09:11:48.739Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "aa9654e00243316d95d3a1c02eb78a52", "model": "sessions.session", "fields": {"expire_date": "2013-03-11T13:00:00.671Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "7188ef4a5c6764c521a2805c1a16bc9c", "model": "sessions.session", "fields": {"expire_date": "2013-03-20T14:10:50.536Z", "session_data": "OGY2ODNmZTdkOGFlOWQ2YWNiOTQ0NWMxZTc1MmFiNjc1NDJhOTUzNjqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "964510365fd0a16f666147a1af6c3060", "model": "sessions.session", "fields": {"expire_date": "2013-03-20T22:37:05.502Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c01ace442fe7176c4796e34b3478d51d", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T05:30:31.137Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4ffbe05cb0b0ecf54a4d965bdcade96b", "model": "sessions.session", "fields": {"expire_date": "2013-03-09T18:48:33.478Z", "session_data": "MDcyNTM3MzcxZDUxZjUxMzc2MGVkOWI3YzkzNDcwNzEzYzlhMjZjNDqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLAnUu\n"}}, {"pk": "7aeebdcb3c2be42aa0d7cf2d5a677e9a", "model": "sessions.session", "fields": {"expire_date": "2013-03-09T20:23:54.085Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b9543b03af8757633309141063f2223b", "model": "sessions.session", "fields": {"expire_date": "2013-03-09T21:31:58.048Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ca61582080db0c21ed341955807f9e9c", "model": "sessions.session", "fields": {"expire_date": "2013-03-10T07:23:48.255Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4864315f46e46226061609b3657351d1", "model": "sessions.session", "fields": {"expire_date": "2013-03-10T10:27:45.714Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "75169b220fb649aa60e3a8e913921c10", "model": "sessions.session", "fields": {"expire_date": "2013-03-10T13:20:11.634Z", "session_data": "OGY2ODNmZTdkOGFlOWQ2YWNiOTQ0NWMxZTc1MmFiNjc1NDJhOTUzNjqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "fdfca4830370138963b05ccaa23dc25f", "model": "sessions.session", "fields": {"expire_date": "2013-03-10T14:21:21.749Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "51d4aa35b858400b8f954358094113ca", "model": "sessions.session", "fields": {"expire_date": "2013-03-10T16:17:05.453Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "40cd0f524b035912b6a035d7dfd1a88c", "model": "sessions.session", "fields": {"expire_date": "2013-03-10T17:46:54.646Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "72bbc28e8bce81211612b780ef9943d6", "model": "sessions.session", "fields": {"expire_date": "2013-03-10T18:33:51.746Z", "session_data": "MTg2YzYyZjIxMmE2NjJiNzAwN2U0ZDU1YjE5Y2I2MzlkZTNiOTA0MTqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLBXUu\n"}}, {"pk": "3cfccb08eb0abcc2972e0854ee91f1a4", "model": "sessions.session", "fields": {"expire_date": "2013-03-10T19:42:27.799Z", "session_data": "NzZhYzk3YTAwNTFjMmNiYzkwYzViMWFjZDY3YzRlYzJmZTE4NzI3OTqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLB3Uu\n"}}, {"pk": "a27bcdc7e40b187db6bee69c3595004a", "model": "sessions.session", "fields": {"expire_date": "2013-03-11T08:37:42.356Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f5ad698aeb508446a84f5907aa7fb880", "model": "sessions.session", "fields": {"expire_date": "2013-03-11T19:08:40.394Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "49d6251c85ebdb40775100aa387dcc4f", "model": "sessions.session", "fields": {"expire_date": "2013-03-11T23:16:24.523Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "52cc6cfeda647110ad00810978ffc1be", "model": "sessions.session", "fields": {"expire_date": "2013-03-12T07:29:31.153Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b3cd95c7e78ddd0263f30950558ce278", "model": "sessions.session", "fields": {"expire_date": "2013-03-12T10:49:44.290Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "48287a9bbf25a7968914f92755e36495", "model": "sessions.session", "fields": {"expire_date": "2013-03-12T17:46:53.504Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4d565f72a4ca2c8e10eaca5d2f92ee38", "model": "sessions.session", "fields": {"expire_date": "2013-03-12T17:46:58.664Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "076d43407577c4fcf61ae2688468f632", "model": "sessions.session", "fields": {"expire_date": "2013-03-12T19:12:25.395Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "466522c96f1b297b6d061ee53fc0b1fc", "model": "sessions.session", "fields": {"expire_date": "2013-03-13T08:43:12.027Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5114650abbe4f7e69080c4fdf1acf82d", "model": "sessions.session", "fields": {"expire_date": "2013-03-13T14:41:22.357Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "165d801745e796c4edd22d21a5b92577", "model": "sessions.session", "fields": {"expire_date": "2013-03-13T14:41:25.772Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1822ff503d021c220e0be7112ef9b058", "model": "sessions.session", "fields": {"expire_date": "2013-03-13T15:06:05.320Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "78e87dc7139c58ad8e6e9919131626e2", "model": "sessions.session", "fields": {"expire_date": "2013-03-13T20:41:40.072Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ee2426ecf99a472b44ebcd167e0f892f", "model": "sessions.session", "fields": {"expire_date": "2013-03-13T22:39:35.853Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b72040a257a236d9f78b0df7c6d84ec5", "model": "sessions.session", "fields": {"expire_date": "2013-03-14T03:17:35.187Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "731d7d2f71bb62a2992c01eebe387c72", "model": "sessions.session", "fields": {"expire_date": "2013-03-14T03:31:06.829Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cab04f63584879a69fe0f15dc97ee5bd", "model": "sessions.session", "fields": {"expire_date": "2013-03-14T03:31:07.052Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "aa3f137be425a6640e3d4653665674a5", "model": "sessions.session", "fields": {"expire_date": "2013-03-14T03:41:51.811Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f31ef5e95d631f5b01937e6174061e62", "model": "sessions.session", "fields": {"expire_date": "2013-03-14T08:52:19.979Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ff29e9678e4e5ba0b12eb35eca35b71c", "model": "sessions.session", "fields": {"expire_date": "2013-03-14T11:12:46.192Z", "session_data": "OGY2ODNmZTdkOGFlOWQ2YWNiOTQ0NWMxZTc1MmFiNjc1NDJhOTUzNjqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "2687947b20bed6c7ef87baa7ead48265", "model": "sessions.session", "fields": {"expire_date": "2013-03-14T20:13:00.090Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4e7b93bfe9e8ad150f7017df7e0b11c6", "model": "sessions.session", "fields": {"expire_date": "2013-03-15T02:04:53.656Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cd223034e68c1efa20013a17acdf087d", "model": "sessions.session", "fields": {"expire_date": "2013-03-15T02:04:53.897Z", "session_data": "Zjg1YWYyYmFlOWZkNjg1NjJiMTY3YzM2OWMyZjhjNmIzMDdlMWU3NjqAAn1xAS4=\n"}}, {"pk": "4069db3ad5594188ba7affce81a2c01b", "model": "sessions.session", "fields": {"expire_date": "2013-03-15T04:39:20.113Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a31bc7dddbb5e1ecb6ebd5cca148865c", "model": "sessions.session", "fields": {"expire_date": "2013-03-15T07:15:42.588Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cce52d8187df79a13097da3bf6b40709", "model": "sessions.session", "fields": {"expire_date": "2013-03-15T09:12:15.091Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8435d48dae48dbacd736b2dd8e62dd24", "model": "sessions.session", "fields": {"expire_date": "2013-03-15T22:37:11.974Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f9bd127efd90b5d43c3816af60901e97", "model": "sessions.session", "fields": {"expire_date": "2013-03-16T08:53:51.420Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a4c94df514ea632aa3f11abe45a5a8bb", "model": "sessions.session", "fields": {"expire_date": "2013-03-16T10:22:46.375Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5439eacbb2cebebe1a8d466670663ab8", "model": "sessions.session", "fields": {"expire_date": "2013-03-16T11:59:58.443Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3d63116d0ef6d7b8cb05a893247ee6fc", "model": "sessions.session", "fields": {"expire_date": "2013-03-16T21:21:22.539Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2d3dc52978a220d84a885cbaa9180efc", "model": "sessions.session", "fields": {"expire_date": "2013-03-16T22:24:45.837Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a5efa4dcb3dca7c8f0f4bad0b8b092fe", "model": "sessions.session", "fields": {"expire_date": "2013-03-17T00:50:38.653Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1bae9042be2dfaf31b675e3904f867f4", "model": "sessions.session", "fields": {"expire_date": "2013-03-17T06:09:58.855Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c0cf4b2047700db2100bc550f6bb9d72", "model": "sessions.session", "fields": {"expire_date": "2013-03-17T07:52:19.955Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3b0907cb6f2625fa9d9a64a807ebf421", "model": "sessions.session", "fields": {"expire_date": "2013-03-17T13:26:49.196Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f54b64ab56d831aaeb33f3994d350409", "model": "sessions.session", "fields": {"expire_date": "2013-03-17T14:19:52.501Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "864c6e1dc965c78870fb0820cc64add9", "model": "sessions.session", "fields": {"expire_date": "2013-03-20T17:21:09.478Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a56fab799d988b45032d6f34d38da7fa", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T05:07:20.194Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e7b16d252e5a0bced38557c168ef3b32", "model": "sessions.session", "fields": {"expire_date": "2013-03-17T23:17:55.357Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "da5c09ece506f2e1adcaa903c6da4d3d", "model": "sessions.session", "fields": {"expire_date": "2013-03-18T01:26:56.570Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b0485a5ec6637113868357a5c897e139", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T05:36:31.352Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bbed91f66d60e37b704d6c5392b80e77", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T05:43:11.997Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3ecb93e280d7f14505ee192a04a4e9c9", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T05:54:37.525Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5b25b456b0d40ba403d3dfd35f3685c7", "model": "sessions.session", "fields": {"expire_date": "2013-03-18T08:54:09.930Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "bdd60cc02a6d85c7489c55537d447121", "model": "sessions.session", "fields": {"expire_date": "2013-03-18T10:37:06.520Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "acdaef34a987e938dced5a18377e80f7", "model": "sessions.session", "fields": {"expire_date": "2013-03-18T13:18:37.116Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2e363ccab08d2ca1537f1498fd97599a", "model": "sessions.session", "fields": {"expire_date": "2013-03-19T00:30:20.649Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1b2a89c224bdf04593c999b30566be0d", "model": "sessions.session", "fields": {"expire_date": "2013-03-19T01:12:18.105Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8deb0c854e23e52c27e04881f401c2c9", "model": "sessions.session", "fields": {"expire_date": "2013-03-19T01:13:04.580Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3ca1e5fe51440a3b37c672dc1c03d762", "model": "sessions.session", "fields": {"expire_date": "2013-03-19T15:27:38.709Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6f4d1086d558bd50ead6106841efe51a", "model": "sessions.session", "fields": {"expire_date": "2013-03-19T17:25:22.513Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7c8c3cdc75f0412de0915aae6985bb98", "model": "sessions.session", "fields": {"expire_date": "2013-03-20T02:27:38.117Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "31ff5a12172ac3f5dbfa26555d2a7186", "model": "sessions.session", "fields": {"expire_date": "2013-03-20T04:52:06.347Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e5c4854c9f1efb327afae32e4eb563e4", "model": "sessions.session", "fields": {"expire_date": "2013-03-20T09:52:25.346Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e92a837c268337214f8bd4882183371f", "model": "sessions.session", "fields": {"expire_date": "2013-03-20T10:26:39.813Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3893c1fb27e7bda51afc2673adae1d08", "model": "sessions.session", "fields": {"expire_date": "2013-03-20T12:05:27.470Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ac3e5b7409e65fc86235f6765515651a", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T06:05:09.349Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "30fa6648e8ad7dbb253e4f63cd482430", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T06:18:36.137Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9bac0f499364285a5f54898dcf80deec", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T06:28:11.340Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8af1dcc263025b93d6a3dc179b3d080f", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T06:46:46.535Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "44e473492434f69288e3f007fc6057f1", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T06:55:17.985Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a03693b7d2ff07ad55ac72b3ae60aef4", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T07:03:27.321Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "507bab266a88ad5b8d8c53aeb5390c8d", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T07:06:55.507Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "caf8afb677347f9047fe60a6dfe8ddbd", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T07:15:02.016Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fc9ce06a2bbd5dc43caa99827154a439", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T07:21:17.627Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "42ede4a81a0fb97c3a40f8ab540fa3e9", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T07:38:32.891Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b39bc7a96794270919a9d71276b74bc9", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T07:40:13.697Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fb60511b36937e36e6d9eccc7bdd7b6d", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T07:47:24.549Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0a0b18b924f3bd80a72208cc3f7084ec", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T07:54:45.177Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "99f27a64ce09143404a3f7601cfad85d", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T07:57:45.137Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "57fd631b37a898c1f444aee0e8ebb9ec", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T08:04:12.273Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3c74650b28ab2f02dd04babfaae7797f", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T08:08:16.585Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "eec389c35e34d46f5647d5d7b78ea5e7", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T08:13:53.108Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "746c407d7eef37962a3fbf50e6615a9c", "model": "sessions.session", "fields": {"expire_date": "2013-03-20T12:26:09.873Z", "session_data": "OGY2ODNmZTdkOGFlOWQ2YWNiOTQ0NWMxZTc1MmFiNjc1NDJhOTUzNjqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "f2279fc2b240bd36eb8db02976848ce4", "model": "sessions.session", "fields": {"expire_date": "2013-03-20T12:27:11.324Z", "session_data": "MDcyNTM3MzcxZDUxZjUxMzc2MGVkOWI3YzkzNDcwNzEzYzlhMjZjNDqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLAnUu\n"}}, {"pk": "682f8b9d4a500b5173fb13bc4c6a0d18", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T08:16:46.649Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d8051c3b5de55e890d7f234439645627", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T08:23:14.698Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "afa4224e32937dc2dce0b722ff9172e8", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T08:35:46.719Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1c8c58025e66f1c359b94ea4c178f4d0", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T08:38:20.553Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cc2074e4db2f2dc7761db2a383b00a67", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T08:39:17.391Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a57aa651de9f7601ec5c88dd0a43e66b", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T08:45:40.017Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "22fe299951675999b6e3b71c3907c39a", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T08:53:14.969Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1af3b19431fe9cea87b987e6a40f171c", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T08:58:46.769Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bb297fdcf55131c882d791ec951f34b8", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T09:02:01.798Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c8ae647e645e39a1d8f91cce395ea8cb", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T09:23:37.134Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6f390c2eea4a078e31e4ef6eeb12feb7", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T09:30:11.259Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "93e22f549ae2c8c511205a1655918d88", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T09:39:39.329Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "84d9fcd71e1ce3cb840aa6eaadc3fcfa", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T09:47:37.606Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1fe2e9a30e6de16a1d227683b6c6a593", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T09:51:52.049Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a834778f7cb72b1b7a27f77768cd7e50", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T09:58:27.222Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8ecd4e674ebb64841519ee5208339cec", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T10:02:42.430Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2cd0a738c17c983b7b877357c515940e", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T10:05:33.904Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6b831457900e3da1c20980ff2bb2ff7d", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T10:12:30.290Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ce98c04a08b17ef5d75fe05093c7f139", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T10:19:19.229Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9c01fb5082ff42df6bb00e99ec3608b8", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T10:22:12.426Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cf8ce3757fb71617730ebf661e4f767b", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T10:30:25.778Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ab4620d986f7fef5a7423b97e571907f", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T10:34:16.806Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "21be244303db4c1b773203dd8b2fb8eb", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T10:37:48.136Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1c47752cc6e70d0ae598f89ad0a92d91", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T10:41:24.941Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "30ef2d93ebe678b796d113e54168c8af", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T10:56:27.782Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bac9e24196ae4a23cdc96a59f8dbd2e9", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T11:07:15.190Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d5b373433383ce3255d0d702b0366c83", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T11:26:58.417Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9004af06a030e32d0c7afcab1abefb0d", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T11:46:05.401Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c724f29c7ff2e7f99c2eca97a2abe709", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T11:57:04.509Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "407a884aa8514057ce486ecb2ca9621f", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T12:03:29.377Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "517504c412f3eb849f386c6fe606224c", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T12:09:43.460Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "497c0beec4a05c7e3b914b57520418f0", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T12:12:43.328Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f0b8b9ac18d976488d13874b1c3a3d38", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T12:17:05.470Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5e8f095dc5fd8bb9fd60ddf9de0e3f14", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T12:25:20.452Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "97da91c1e0ddf2b7c07cba803fd393d9", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T12:34:23.181Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cbb635a117e8657ddeac0c5d1f1369e8", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T12:42:13.169Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0bd032ed5470b1a59e719348baf1c11c", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T12:51:14.055Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "511615ba9524d0aac510e2267800332b", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T12:58:22.949Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "70c2520c37d48172cc4ffac96daf0e73", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T13:03:36.501Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "935f296f095c987ebd851dd1604212d6", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T13:07:32.741Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "729a3892989a2918c1cebf24e7f34886", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T13:17:04.393Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d4735be927cb477f47c96bf2ef9c3c0e", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T13:19:22.881Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "790fc2e60e23a73567d5cca7ecadb9d2", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T13:22:59.077Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e5ee7f29ee1ce018aefc18990e3f1293", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T13:26:23.505Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4660a790912a5d1a42419c4323731fa0", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T13:32:45.670Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "86461619f704b01e4b4b5d39f71ef7eb", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T13:37:59.657Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fb0699db77aa39e2a6d8604fa9aae651", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T13:47:20.553Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "becef67ba31a59dd9e4ff0006124a589", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T13:50:56.625Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2291620970093c13aaa3dd184566fb5f", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T14:12:30.253Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "20ce54435f3024d9e0b1f8fb9a8dbd9c", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T14:17:00.901Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bee01e94ccb4e85960195553ccb793ad", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T14:21:21.269Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9c897af24f47c25cf9216d6e143a94ae", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T14:26:30.698Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b35553bbb59f1cdec317959581df8930", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T14:30:11.513Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9cd3d357d82a68bc9a6e04794ec064a5", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T14:37:11.429Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c388d44f2a4576ad6f8e93683ac0ec5e", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T14:40:00.527Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3a83d185c0dc2cdc7d99fcf55d8e628c", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T14:46:09.347Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f196b2a66c1b4cc366c55a6f5f436e60", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T14:55:13.513Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0e809bb82c805e86036eee0ef4930a01", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T14:58:11.284Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d46d68f8048972e8a6c52f17f400b7b6", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T15:03:32.309Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e131540757a805b009f0fe2f5ab72549", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T15:14:12.482Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dcc22b362d41f81136f8031ee6dac49b", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T15:32:09.115Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6cbc0d6061d0a1aa4c70d9084a7dc3ae", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T15:38:16.553Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "14fe9531bab08082072a804da771c4e1", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T15:41:31.081Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "095b203c95ebed06cd2e43eb62222692", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T15:50:55.882Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2713e2b5d484305c7376b162b8147d85", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T16:01:35.856Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "258235031518ae79d1a841cb85b58ba2", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T16:04:52.877Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "89613f5f7d445cb5d4b1befdcb6e9236", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T16:11:29.989Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "66430e79cac1f111460666cbb40aa683", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T16:19:20.102Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "59bfef38125870a657a119fbc6461c87", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T16:43:18.733Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "aad1addfc2897b08ffe411c41cfacf3f", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T16:46:26.729Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7317d129bcf699874b9456be9b3d2a83", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T17:01:27.267Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "96ff8c10e1a7f33cb08e2ad7a8ce15dd", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T17:03:24.676Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2b3dba4ec5cafe0345644209e7bc0731", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T17:09:44.332Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a0a5daff3ff3a7ed6c3dd3e1bb3f0d0a", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T17:25:36.715Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2487ad8dbdf067bf8b245d0afa99e14f", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T17:28:19.005Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8dec9ffe5438e8ec11ceeb42c043932d", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T17:34:52.917Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ff1157cfef37bcd68ee92274519f6b90", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T18:05:00.864Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "766a894244dd3726305348142272b5e4", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T18:19:49.007Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "02f68c2dd53e742a922c93141ebcb6b0", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T18:23:22.901Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4b9f850dfb6c7427b08551e8e93d2227", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T18:31:51.901Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f512993e85bf51fcacb4cf82645f99fc", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T18:42:13.057Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bf37e93b5252500fcc780deccda87b16", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T18:47:50.104Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "50be45c798d788a9554f479bf7adff09", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T18:56:13.730Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b70053080e3ab19b08b10efcbea0fdb2", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T19:15:08.614Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9dc8d8bcfea318553fdd38f6918f8faf", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T19:19:28.417Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c134de5a6a0989912d8544de9c840a0a", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T19:25:21.318Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dc5609c84e4aa540f0319ae2acc1dd85", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T19:31:17.689Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c4d954a1f536a12d24533e713d191959", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T19:37:39.773Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dbf210b9c9b6f9b0243893772402186c", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T19:40:33.173Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "567b3ca6eb01075c1aed9e5e0d712fc5", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T19:46:34.445Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e4d40468cb7a5741dcc145a2ec7b7c11", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T20:00:23.489Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dd6ac931e81555bb75d0319868d08d07", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T20:04:06.987Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7f96df448bbd9061cb652e19c6fb5be7", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T20:06:45.113Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4d618e2aed31e3334c976e07f98473fe", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T20:13:19.641Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "672370981d486de4925ae26f63315961", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T20:21:09.391Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c9dcc2a88583697ef5c0a70ba741dc67", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T20:23:48.057Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4f80bdfa5d3c3be870169b9cf1f02f17", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T20:29:54.418Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "115df52bfdb0ac89cd0a11380aea7f39", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T20:35:43.895Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d08311c9ae2819e7ed7126b6d80eedd0", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T20:39:12.573Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "26bcceb65f12cb9dbe1ead13e18fff83", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T20:42:30.069Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1e5800c69d9668cec214078e3c6bd1a5", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T20:52:22.767Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "49c8ac638aa548b85ccbcd85b8ebc905", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T21:05:01.041Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "549f13e331e4e74e36a05fb6d58a07f7", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T21:05:10.605Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "324f2f24f65ac198be5f11bb8f0bdcec", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T21:13:21.674Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a40e5ba4ab600313d941308a706caaab", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T21:21:10.909Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a3e6d14aa6835e951de7830eaab4808c", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T21:27:05.941Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3f12da68dac39ec42daeaa6fc26d4d0f", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T21:31:00.225Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "10a803231b55d920c4fdfe9eb695ab13", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T21:40:18.280Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "399c0a416a093bfd98aaef64d0b91d14", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T21:47:15.726Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "333a13579f2d2ea00abf094b23f46d6f", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T21:58:19.758Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f4b668c4a9940bccf6159db248caeaeb", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T22:11:28.834Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f9545d1c58821055f774f81e0f72ec3b", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T22:14:24.065Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ab6ed2efb9648c1b491907e290a7011b", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T22:17:16.487Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "08e57a712685a9bcac2a7b9893d48474", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T22:22:56.949Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1c8a7f9973ca9a2eba48b22afc26be38", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T22:28:18.199Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "45953a683c204b25214be86ad20f1b67", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T22:40:01.373Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8a2181ec402c52dc3e3ae774241a987f", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T22:52:15.702Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "652d6c78112bb2e6bc7774ec4629fcde", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T23:00:18.236Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b4c7f886b93cdd4533382ddaece893a0", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T23:03:14.229Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8331dd659a63cbab171e32f12dc5e6ad", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T23:06:45.481Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "44cff76666363426f6059a38a819a40d", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T23:10:03.699Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bbcd3b777a46e2a5ae173372959b4268", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T23:15:06.451Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2d91a4c5fbaf7e190816038c48e5dc05", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T23:20:37.141Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f2b715bf1a7e1dbf46e52e7ded92ab79", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T23:31:38.369Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fb4eaea6157ac301b66133a3ca68f168", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T23:34:45.345Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3599ad6732970a1edd9607febb0ff457", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T23:37:36.560Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7d412597ecf0b8962fb520e7868bccc8", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T23:40:59.664Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4409e290b6bee6c046957486df461fe5", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T23:49:12.111Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e22d6d344836bcdd09f31c090318b383", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T23:54:46.621Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a6519e6a966ea78c6f58a0c86d44d456", "model": "sessions.session", "fields": {"expire_date": "2013-03-21T23:58:10.428Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "827f39ad4317e753edcdc46560262a6d", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T00:11:32.793Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e0de197078b7ed2d3fa6dbd60163448d", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T00:17:20.649Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2caf215c3ddd7a0934485c6085fc5f99", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T00:23:18.305Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4304d0e5dace5f2558542beb4279c2b7", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T00:31:10.518Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "549d4757ea9fc9568ec4481ec37c2010", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T00:37:50.921Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8906d25b1c3f8f83e3e803511b3293f3", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T00:53:28.157Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "803e5f73c44a260286c60e318bd24416", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T00:58:27.337Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bd38e7f6986425ea42a84ae0f14bc593", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T01:03:46.223Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "07c0f0a3a3f6d2bce51666cb575e5b01", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T01:14:50.825Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bec80b771bfe36ede0a89ed33214a971", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T01:38:05.227Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7fac96b36da62b562890b22ef989c3d6", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T01:41:33.677Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0ced0701b69823db3335547b8a65d1d6", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T01:47:26.962Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b1c62f24c7a860a1de414ee487eec81d", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T01:49:55.647Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "204cb423cdd677c785ec9e92bd215443", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T01:58:13.079Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6089974ea44f3d4b24481fd12626f78f", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T02:04:18.504Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c0e882228a42f7a87e3e4805e5c6aac5", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T02:11:16.393Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "06e09550dc9ee2d54a1348d6dc2d03d2", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T02:16:41.217Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "580757c543005c991a9940c27ba2c860", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T02:20:07.649Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2246f789699ed65627093dfd1f99fa4a", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T02:24:41.527Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f53c6b5598bca43474fc425a96218eb3", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T02:48:34.979Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "235ff511805367049957616731272a83", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T02:53:28.113Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "13dd43f7a6d103ed51a47cd9806fa3f8", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T02:55:58.657Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "11ce166112fd4db759e5bce9dceaa8b1", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T02:58:24.417Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a01ff2bf69e370666d1410388059caba", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T03:03:19.255Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "94992547d00777cc6bc814ec10629a95", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T03:11:45.429Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9b331c1fbb841185fdadab89000dc3ac", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T03:17:30.387Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3d6852547d0e7fcb64d451a0a3281a8c", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T03:20:13.597Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4615d88a3891424ea5d6779b1d89f611", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T03:26:46.036Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ee3449fa240ca12f3afb5cf511b9d8b1", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T03:37:44.352Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6dca0282eb2477b3586482659d56bcaf", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T03:40:51.557Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6f2f2c3d6742466ff7430e6b15176473", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T03:46:35.686Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "33e57807b2164134a303831524edc7c0", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T03:55:21.532Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0d69a900a399fa8b3ace3595700451d1", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T03:58:21.942Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e353429a0c083ce07d42d99e97a2acda", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T04:02:22.567Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "12389683c5fd9370c1f6a53183e943e9", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T04:08:54.924Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "32cb2dabd1dbd49a9fb11af210b8c827", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T04:17:04.639Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "150c0dd93d962af6e81980ed00f44c38", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T04:20:14.005Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3f294708ec38585edd52281ecee31045", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T04:27:43.598Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "508833908b98abcfe49e741772847a63", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T04:32:41.837Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8c12739b570160ade6c5d02b736b0bd7", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T04:36:18.711Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "93615c11648cf98c0b08b2c5314d361a", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T04:43:53.549Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "232e3d3f623153456b10b393368c8fdf", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T04:50:23.111Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "adbc5c70197e62e4e236517cf75a4fdd", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T04:53:40.718Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6e3ab5f954a134f3ea100f98b641dbb7", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T05:00:01.101Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "628b4cb8b978f6eb6aca626f5dfb1f4e", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T05:00:32.687Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7485ba509a0f4d2eb1c84f5e7cf3a2c8", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T05:03:00.158Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fda6868b035d10a298f9de1f4b7ab551", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T05:15:08.645Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f0c0925ea7968ed8814ef8673f7b63e2", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T05:20:23.249Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dc6a692245f783c208912ad5200c7caa", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T05:26:08.178Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0ad2243f3465f73a2ede8a8a0466ea6f", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T05:31:49.634Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "87bdbb8b574d708fc2bc758ea0618cd9", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T05:50:36.709Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "43754e368585da57b26c67f49e0bf734", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T05:58:25.067Z", "session_data": "Zjg1YWYyYmFlOWZkNjg1NjJiMTY3YzM2OWMyZjhjNmIzMDdlMWU3NjqAAn1xAS4=\n"}}, {"pk": "bf1c9b97fde092ddf0f74da14afc4bcd", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T06:01:29.114Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8b1e37f79265627f9d3eee9d276dcc74", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T06:07:34.269Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "31b16e70729cd335521a242a85a38e0e", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T06:11:28.049Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "72dad8384b71424cd8389d7ee112f5cc", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T06:20:10.104Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5afa58dd0fc77df896188c797a0b086d", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T06:40:22.878Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "403df70cde79488d484bef89028f642c", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T06:46:46.899Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0662df1c64816e3cc24be0a34204a8b0", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T06:49:31.597Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fa74a514f800374afc06cf03731fd90c", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T06:53:56.849Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "154f85c3eb7908f486645b1b41cafb4f", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T07:13:18.305Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "398bbf6637c21a5f5857b5b5090bb9ee", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T07:16:56.240Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8836a2fec39be8f1bec37e5a08bd1765", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T07:20:29.973Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c0f5c25705eb262d76b253c5b6b6a33f", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T07:27:01.317Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "aefa7f5642def8bb00a35d99e2e0e4c2", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T07:30:34.193Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5dd88f44410082f5833299a4774dba91", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T07:33:48.325Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "02cb681a2c2f210daed60df67a8e4c38", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T07:37:50.753Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "25039fed29ce931948190cf4edf54259", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T07:46:21.962Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dcd56fb680db6e0fe0998185e7816025", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T07:49:13.458Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "906ae693489668a346012ac8548ec8ac", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T07:52:36.898Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "093cefbd4fdc4be08a5e9b6fe8e2e792", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T07:56:17.590Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c10af908526b88702af62f56762035ca", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T08:01:39.285Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c71c5edeb450452af79d5c65322e5422", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T08:07:30.585Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1b2affe345ab07ef7b3f1bbd0172555f", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T08:11:09.013Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8c0a427346162b03da4a36c45e70f177", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T08:17:45.333Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fdd19229b0dab879e841504bd47752a9", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T08:22:59.101Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a8d5d41be804aa49e5e625292930bdb5", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T08:29:04.710Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "09196c60659915c1963ec22a7e8573ef", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T08:31:55.510Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0edb8225fddf4cf2cad2e43426489b3b", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T08:37:46.276Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "97a73742ee8d9814fc4ad0b7fb6a4204", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T08:41:18.361Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cb03c84f3b3b968a7caede4641b976a8", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T08:48:50.915Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "44cc6aac25f7b02d1404c1e910915eff", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T08:54:20.956Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f700eedff7d3c1c10d418508a0a74229", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T09:05:34.080Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b319ca86ede9d9f773a70a04ed9abef8", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T09:09:15.045Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d4b2b87131cb1a492fe6243a42d33fa7", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T09:12:06.846Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3f56ae223edff8cd49c1387b6c3cacbc", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T09:14:50.503Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d55bd91776f431888ef7d05c6e9cd709", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T09:23:28.189Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b8a2ad9db07e52cd8012c6d147fe38dd", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T09:32:03.029Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "be921a2e0fc98b8ab63920636799b396", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T09:40:36.410Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b82e6cc74c6b51172998783527b066ba", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T09:46:11.879Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b7dca25899e60c02a130f976ea0934dd", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T09:49:40.486Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "918e0c9e96a2dffb9d433c6a6a474c74", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T09:55:45.731Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "432ffbc83bd1db5297281f95cf0b49e8", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T10:01:39.018Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e7235a28954b59d702b44ec2db71a798", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T10:10:11.749Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a3fad7069f0ff8e597fcdc27c4004c16", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T10:13:47.483Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0c828f616cd46ac014c3d480a0178490", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T10:16:38.568Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a3a9faaf0232a67327cd252797e566c8", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T10:20:24.577Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d1d8506a02065f59e71bd60a05c712da", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T10:32:59.538Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3eb2a56fbaeb9fdb53298a883fdc267f", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T10:41:38.334Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "08032598a8c7d774a705eed9d83048ba", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T11:04:40.969Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "80f5418302916844b911dcfebb1ed228", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T11:25:00.353Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "76e6ba00dda3aad76ee2e690980338d2", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T11:31:02.761Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "33b5d9619ebdd136076539e35ef99feb", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T11:34:33.521Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a777d38eee950ed0cb4519e1f932b65e", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T11:40:37.748Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3dc308f31f9082243db15fdb7f3b0348", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T11:47:36.909Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9e0728aa16a582eaa9a6409989f00705", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T12:01:23.333Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "953d90af967eec7c203343bd2ff2bf71", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T12:05:16.454Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c74b59bd14ca652dea59d200dd760fdc", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T12:10:49.201Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "da1f8a26e27a89bade7809805266b8b8", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T12:20:23.065Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d759a92041e613d3c48c67966e23dd24", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T12:31:04.477Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fe7fd06f5110139030bf69e74633064e", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T12:34:26.743Z", "session_data": "MTc2OGYwYTQ5OTVjYTYzMTk1ZjI3ZTZhNTcyYjc1NDFjNmYxZjgwMjqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsJdS4=\n"}}, {"pk": "3d5b96fc82696ef3e87a51627ce57a1f", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T12:37:51.457Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bbd702ebc7a782c61d549bcedef06716", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T12:41:00.582Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a5fdab63f93fa8decb9df80d542958be", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T12:48:43.660Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "49486a25dfe68c7ff537f5e641c1d3be", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T13:00:02.021Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "72a768a4f44dbc73adeb3c76b0cdb229", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T13:06:01.409Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "76f607cc2058d5d5888326bcc003bb95", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T13:09:10.688Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6cb8e9c48370b9f79834b948c3d8fa00", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T13:18:14.540Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "784e47f706e50f910b1d2eb3bf8e5a8d", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T13:25:03.433Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9897f7c1db8b5cd2b19a83b1cd9023f2", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T13:28:19.215Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "129ed54bda7f5b7b52aa4a9da95e987f", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T13:34:14.662Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f93bf30243db2bbb31c51bccdd65bb48", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T13:38:22.040Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bacd15c47da940b40e740ee63fd391fd", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T13:46:47.178Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e7bec173aac08ebc3d39ed3968634b11", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T13:57:38.513Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bdd096f0fa91738c8e7e0799f677df52", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T13:58:55.337Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "12d603591b1379d6c84b4eb92a2c58b5", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T14:04:41.921Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d2bffe1d3feba2cc9a43a1ed9e640e3f", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T14:11:19.542Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dc4eeec40dfce6ea161686e702f63ff0", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T14:17:52.261Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "769f1af0c6ff03adc092ee2d02ae2238", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T14:20:41.977Z", "session_data": "MTM3ZGEzZTljYWM2MTEzNTE4OTU1MWI4YjAxYTI3MTczY2MyNTA5YzqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLCXUu\n"}}, {"pk": "cb269bb10259df84f11e8d495ad297a7", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T14:23:12.641Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ae99d4aa9dc2309e6bc97113cbe05b1d", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T14:26:10.413Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d634e26a07de4472d0a9da6f9961dbb3", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T14:29:49.773Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d6e44d164dfcf00aec3334b04a4271dc", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T14:39:17.895Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e5e50eb11e2c156d31df69351008b020", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T14:45:36.748Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "81eede13d969e75a1989bf53638b4c98", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T14:51:44.610Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0f32f6000c6c55c51810ae4ecfa3faad", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T14:55:58.325Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a8fd34b5abb7cf52ddc06c3d34a3738a", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T14:57:18.578Z", "session_data": "Nzc0NjhjMjIxZWI4N2RkNGMyZWI0NmJjM2ExNzcwZWM2NDI5OTk4YTqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsKdS4=\n"}}, {"pk": "9c97311d21218ddb5103d4b10ec3dca5", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T14:58:54.081Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ed1e3b2923ed45699641c54fb7e87cdf", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T15:02:30.447Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9c12e6ae620f2df1b4a8c522af07dbde", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T15:09:17.429Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fca3523e2cc54a1ead8bae409070f57c", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T15:16:22.709Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "23674b7c22ac87c1f370e0e06b0d5083", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T15:19:16.330Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bd43458fb4888bd230ea90b53c71b834", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T15:24:29.107Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6f50ef9d8fe9fa0e260b69eec5b53678", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T15:28:24.982Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "086154157f09cd3d5b3303a79779b868", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T15:48:18.521Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7d12fd1a5a0a4c9df2dd6eb7909ac01c", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T15:50:56.201Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7158c872463398cff5a84d106dec5c1a", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T15:57:37.784Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8b4be45f7d4313d31a03a4c57a4d7e2d", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T16:00:33.745Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ea0c572af95d5fc9ea9e4e0ba390c295", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T16:10:19.281Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "75392d9e386471d896e03eb1561271a2", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T16:13:46.810Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0788b42c3ec18bcaa8dec72bd6930dca", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T16:21:07.110Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "14ec4a574d7c954d4ccd663a6b191b51", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T16:42:02.665Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b619bf8bc7272af1f2ca5a61d8dd6be2", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T17:01:28.390Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "131c3a3c51b9f4197dd2acd84836d8a0", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T17:04:21.153Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "238418723540aab817841ea8e7592e44", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T17:12:38.889Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7171b6a2df5c5dba3ef9fd20f00b9b79", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T17:22:50.450Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ad565324c8a84e53b2ea2a05b7997ba6", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T17:33:40.805Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0d13f475f7086fd467ed6cb04aeddf86", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T17:39:52.767Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "da6272a2bac4d7b61b445e81c7384ff1", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T17:49:06.652Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cc536abaa4d6b612209ed70dd729eea4", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T17:59:21.819Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8f6cb6b79402e7a4a8c54adc3d5ec23e", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T18:02:44.553Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "deae4d2cf74c3703ec9695e7bfb8561c", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T18:06:03.441Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "347af885d6085de0de2567d4028aff63", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T18:09:27.901Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "049fdae8647524403e47be89eedf93ec", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T18:15:22.670Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "40c0e2a2b720eacc082d19e127875c62", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T18:36:50.886Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ae253d028e2b5eb5bc61e52d7b7025c1", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T18:45:09.909Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "065405c07735a1df73434554656aaf18", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T18:50:58.051Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "5d0eb8665b3f3047d7f13e2109a6427b", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T18:51:16.465Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a7bf73c9e75c16f8ea21a4b1bc0cfe00", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T18:58:15.433Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3bc2cf626d3999428dbb288165660962", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T19:03:43.033Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cf9fe378136ea41dfeb0787c6204a413", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T19:09:32.841Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5938c9cecfdd343035648da33ad72074", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T19:13:45.837Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "272938797db6655b055a589af32ebd01", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T19:17:12.879Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a07ef3762b9c4567aa60c10273575650", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T19:26:27.706Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bfd4825c6ab9ecba1bcfab9d08efa947", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T19:37:57.255Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f972c0520348c8c6e92044c700e17f80", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T19:41:40.973Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "74aab82fa7ff024ec0e03f827890d8b1", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T19:46:11.566Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b2dec83737693d3886dc11e5385ec4cc", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T19:54:51.975Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e4b9d7a0e7157ed935cc5b96a77097e1", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T20:03:59.292Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ec849da4bba5a10bc3d925707dc9e301", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T20:09:04.498Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "73d28b8d837c4ec920c00ccdf2ea3a59", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T20:14:24.580Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cc19d0b00f03138c2bbcdce7b154edb5", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T20:18:07.689Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9489208a8002dffe7064bd79be0aae6e", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T20:26:19.044Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3ad767f0022fcc8d4f12a4619f20de41", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T20:29:18.885Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "abfdac727c03288954740219e6dedc97", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T20:33:04.984Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d4c5d7b4e3e2bed4f6b19e9e6b0ff309", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T20:38:38.006Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "97415c51863d9606dcb35f889a648443", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T20:56:48.921Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "032eabea28ea9631e81e57f0444f520f", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T21:05:20.162Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7df3cdccc524f26bc04f1df4d022fd89", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T21:10:11.368Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d1061239f45afa5bb0b681756d5360d2", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T21:12:57.330Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a6ed4383cb0c795919a3c0bdaa99e0ff", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T21:21:18.379Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b98b9884cd9a3b211bcb74bbf96d72bd", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T21:45:24.566Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1bda5638c8ef081fb2f7efa870bb2c05", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T21:53:51.532Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "85cfe3532f59b372d051e79dc91505e3", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T21:59:32.233Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "11b500bf3a2b240b684ceb0a999b1400", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T22:05:35.163Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c8795d91927b1d711f0375008d5884ef", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T22:20:34.054Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c992e3f6ceacc5f98d9f05dc269c7ca8", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T22:31:09.688Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "737967ada3bb54092fd1cfb02548a3e0", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T22:40:59.833Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c92812e8eca2b570571b562d71f3482b", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T22:43:25.646Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "757e7edb3c2c99604231edd668d99000", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T22:45:56.285Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "494f7922d04094c3b818f1633f01ba88", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T22:48:34.250Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d0d5ee2cda8020cde645b09e25eb6173", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T22:51:56.853Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "66cccf7f915639d3318649c264876993", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T22:56:22.724Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "50c825dae1a6a3114ba14d0792235b88", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T22:59:08.062Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "af18c3abafa2612386fa270aa082f035", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T23:11:05.221Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4926705a37f65901a76d0d9c9482e5ce", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T23:19:07.287Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "622640dd3b531b2c3f42190efb091ce9", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T23:26:26.229Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e6ba03b632d25b53f610de697e30e9e0", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T23:43:05.819Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f275c34c6cfd450d82de4e8f56723f3c", "model": "sessions.session", "fields": {"expire_date": "2013-03-22T23:53:40.306Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "33581bedff25d352964a8e20981095ac", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T00:01:51.327Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2d06084d22ba06648ef73fd99b9934f3", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T00:14:51.867Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c315b6e08f749169719ad0944e9bc7de", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T00:22:01.052Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cfece9742beb56205c74823416bb654d", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T00:25:21.957Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d6e68b530603435b40b0ed18783340a0", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T00:28:23.108Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "25ef91856931956ac708e7ea6f570f99", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T00:31:43.614Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c5a4f150221b983bafa2e26e83ee8d2a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T00:37:54.847Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f711e72fc9e0d942ac31347c2f6d502b", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T00:41:10.208Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5f3c31c3aa4a676b07ef7d95f3645981", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T00:47:28.989Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b61f0efd96f1872a41c0d0b21d6988cb", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T00:50:54.901Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "74e31e5e4b8048afbf076d78fcf4a5bb", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T01:02:26.509Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2de81a9b72b49839b5c9591ab709d750", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T01:05:59.981Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "48f35916543b3b477ea9b74678ce12a7", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T01:43:40.565Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f313118ed59cfbf01f47e8d44bc1e7c6", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T01:54:54.155Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0dd65b64fec3c48358394bc0373f8c82", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T01:58:18.477Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6bd28a84f7b6c472e6ac0b3c41424042", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T02:01:40.028Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5ba589ee619f88bc3b50a40c2c653568", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T02:05:50.483Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1f2f4cd7fc84ff4bb226cec1bd3a860a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T02:08:34.659Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "514cdf70db038d542f39ff2e6fd2a4ca", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T02:11:45.534Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ff14fc655f489f9b2577838cec36a1b2", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T02:15:36.357Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "512cc1899a795ab039480c7a07d164fd", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T02:21:14.107Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7803a9d15089ac5139e15ffc719c7e01", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T02:39:50.143Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c0b8541bc66addb096c609c23c1f7271", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T02:44:33.637Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6f53120334bacf5d4e6f5b52c165aae3", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T02:49:57.802Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7a6c94db29edc35319356473683f2327", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T02:54:04.597Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "29a9c081535780c5bfd3222739a8d9a9", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T03:00:00.055Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1722a7a5624e658bac1e9608dc1525e0", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T03:06:08.733Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cffab61c703e60717838d337d2885e07", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T03:17:19.730Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "93a86290a5f42ad3c3fc102af5a09420", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T03:22:34.385Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c8a5a57b9794aacb4138ebcc2f07bec8", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T03:26:10.393Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f53e639cc62bca0e6fcdf0c0a3cc9934", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T03:31:37.417Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6a73d38fd47575082f5299414c04a68f", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T03:38:17.741Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e20a823f0af8d2696e22731345629632", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T03:41:14.445Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "df079c7a651b1c7a7728d841c5eeff3b", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T03:46:56.737Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0f0a94cb790c3940472228420808fbb4", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T03:50:04.501Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "713f8f6a48675bf871590db90516fc73", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T03:58:55.355Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1c5ef32c76480e8373e05cdc5342fcd7", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T04:08:19.548Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8fbf785d1653fa40616e5fd5c1a108a0", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T04:11:21.956Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8c594fcc7e51b9cc36b061a99be3e5c3", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T04:32:31.222Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b9a7b034074eed5451356c78a7f491ac", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T04:35:31.863Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b516bdd638bcb553c9b9c4e9dfcb75af", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T04:51:30.904Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "76a778c4cb8b0b6676163286721117bc", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T04:59:49.905Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0dcd04d190d8ceabcc04159166b9f4fb", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T05:03:35.034Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0627046727c770ba9535bcb74f8c9c32", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T05:15:03.701Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "696207f71cd890128c07292ca31ad137", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T05:22:05.286Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "09686410fc7d9a2692a1aaec10485043", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T05:25:44.925Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "25934d1e5b3a4d996f89536198574779", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T05:29:28.816Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0fa1f512caadec1ed3c7bbae4f62bce0", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T05:32:35.807Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e6bfde312754e35f7a7b70332a3c300b", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T05:36:30.922Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d841ee038143297c4928ef4fe083a255", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T05:42:46.683Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b4f466d450aae9bce6f8f983e78ab0df", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T05:45:55.686Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "96c5780b98903f1eb6083463e250ff00", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T05:49:49.919Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "00cff26da72783a5d0baa4ed70ace7b8", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T06:00:20.060Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4751f3bc3c721f4ae415528827fa4c3a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T06:03:59.364Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "95544be4b889726f4f1e224f9dbb9690", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T06:12:01.265Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9b0ec0add5a0e2f0378c6486c7ac6d19", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T06:15:22.705Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1ce2c8ed23d2b26b602516fc108106eb", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T06:26:57.413Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c553b98fd310f39d53874263470eeb73", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T06:30:16.783Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c3031ab3b244fd61b16cdd73bac5a8e4", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T06:33:52.531Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8c3f8339c007d72cdb6e96f7f0f172f3", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T06:45:11.901Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "43bb66b7b5a682c66efe836e73a3472f", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T06:51:09.501Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "766cab7217f6869135d44d19acecae4b", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T06:53:49.065Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "98e304a9bccd3ba80eb8cbd3b52a0249", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T07:02:37.065Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cb8a34291ac1e4aed227df0de2ab9ea7", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T07:06:11.681Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8b0893cde4398bca9596dfad80e4da97", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T07:09:54.297Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2f3a0260dbf6215c2b771d3363f8807a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T07:12:45.609Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8ede4ec49a98fa03c114bfaa584c8965", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T07:18:39.743Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "64ab767f24e4a42738053b2070ddeaac", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T07:35:00.253Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "00289d164b60d912519190102a9ce006", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T07:37:29.189Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9935ff29f186360f1a09a0a7e24c5b93", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T07:40:49.005Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9e63a93b6d9e03e6389c252b78778892", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T07:44:19.902Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "45cd7ad2ffacbc6883c1aee01562663c", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T07:48:03.877Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4636817c62f1c49f98d0b7dab6361c40", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T07:51:41.205Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "543d13c8b64b9b6daf52da7da08cbc77", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T07:55:20.221Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4bd482a29c1e976cd04cf5be02d722d7", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T08:03:04.043Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bdbac6c524dfc1220788ed31be9fa72b", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T08:12:01.485Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3bb089ec5ca655463f72e2e4b27c63aa", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T08:30:34.823Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "044ac1a4cb898ee6f33500eae5710161", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T08:37:42.031Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9aeec82291c1da8c1c688a46f3bbf3c4", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T08:46:58.072Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2cb6448c6731a66711096e5e4bc5c60a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T08:49:41.073Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d852dca71336b19d6a98ab2fa39aa571", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T08:59:12.522Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "68a2c51ed68a792ff409ba10242180ea", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T09:02:06.250Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c930fddcbfc1bdddbfd94598bbd32d23", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T09:16:28.024Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1e80f3201097a14b7f46aa8278d5a6e5", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T09:20:39.381Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1ba15585b74c1a216ee04e67126450a9", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T09:23:24.946Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5d20c4f0b229ae7f170d4cb94c53aea1", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T09:36:09.525Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "eb49698d0d9018632942ac7d78ada4b7", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T09:42:19.317Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b7500b907dade5a1d352a0a5a84a423a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T09:46:56.031Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a8d3bdeb11716f37297c21d247a9d459", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T09:49:47.307Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a77036f633bfb67bfa7f264faa4560ad", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T09:54:04.527Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d3fce0626ce4cceeef06ed01292c1e98", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T09:57:23.253Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "321e29793214f499254156b3906ce372", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T10:06:51.965Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "08597fda03e2805a2ee87f46787ddb53", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T10:19:29.085Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d686089ef512f48870411e0f0537ca96", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T10:22:53.066Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f8b36a69dd64dcf87447c85ae682fb50", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T10:43:59.457Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5d28c558809348965b6da0b1081c7b5d", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T10:46:56.921Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8fe9f23141f61037e9ccf46860b7f6fd", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T10:49:32.977Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "51813511a679a629678e905de6ba2272", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T10:58:56.677Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d4ff3f550df7da6e7d506152fbeefc0e", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T11:15:31.394Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4fee1be2ab36c0e5b86d469075eeb5e1", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T11:29:00.186Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bb07ccc103d95be061829ae05c31e38f", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T11:31:31.705Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ec6d4b68e2999b0ef698a0412aed5188", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T11:41:24.673Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "98a99ba4c439bee97348d7006480311a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T11:44:02.749Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "298956fb5120027e864f86514f17d8a5", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T11:47:28.881Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2e520649ae4e75e95513cfa666fdde13", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T11:53:14.685Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fa3b65e48f228c01635e9ac35403d952", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T12:07:12.689Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a58bce4dfcebe1e05e02ebc209a585fd", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T12:12:41.800Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5800b1e8beb486bfd3881bce61b6b03a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T12:15:36.385Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "57b1b66b1dfc2a1c99c9a0d9330bcc61", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T12:20:50.415Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e98832c985c047431cdb5154d89b75f9", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T12:24:43.425Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "94b884baf2d2ac092498c5c7a9d4cfba", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T12:34:37.245Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "36ac1cd81c5abcf34acbdcdab3b4bdd0", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T12:45:25.448Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d17e36b2b1a2a10067ce6dfec4fe43e1", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T12:54:22.695Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b64e0a73d7f86ea946533a558fc96ae8", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T13:08:03.497Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f16f811a604fb086f18043a9892a11af", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T13:11:20.566Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3f18a33b56fe458dbab049c1e01bc120", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T13:26:36.513Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9f6cf2c198fbcf983c91d7de486beb70", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T13:30:44.365Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5bb57eef396da0f0e72cff90b4a7427e", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T13:36:44.614Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "833caa7391821d9e4276256bb5d6fea3", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T13:45:03.095Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f4067b5869c0edcde71286ab2829aa15", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T13:49:19.249Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a5492b4b077f52a13b7b8fac3525c055", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T13:52:02.811Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c7c14d63ce3ea3d4e9a3e8aef6e6f2f8", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T13:58:06.515Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "177ee9bc5fc3a4d582e7a17248ee92fe", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T14:08:14.031Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8eb69d08e564251be5f2bde4d3f7d79b", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T14:20:56.787Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0da751b42519d35715fdd7ba6d38b16d", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T14:24:32.341Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e8f75ca842b343ddc78832d909b4cb94", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T14:27:33.923Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8b8a4865a716c427f5430003c2c013cf", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T14:33:45.717Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ef131795e35dcb119506bf6463fc273b", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T14:41:09.129Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "812bc9e885d7a4082f5fac9742fae4ee", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T14:45:42.277Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5a5adf3870ba395dee45683dfe40cb81", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T14:49:40.503Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9cb057d174ced49a616b977272bf3330", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T14:55:18.385Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8c501c9481094d2f4a90cf394d49c2c9", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T15:01:08.856Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d863a262536f39b57dc2d04070030f2d", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T15:08:56.093Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cab80f966048de59e60d6a9ff96ae9bf", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T15:11:26.641Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "643f2b916210ffaeab9e4831b482e8df", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T15:21:40.281Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a4063f43b2643ae8a1a092066d4bdb7f", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T15:24:58.954Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b74c6e91c3427533f9ec3a2fca385ddf", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T15:30:21.575Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1223359a331606d7c0e895d19d8727ea", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T15:32:47.360Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "025597c0b36f15e960f13b2691f371fb", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T15:35:17.794Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "29596846237553a6ad8daa7c2acb2261", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T16:03:46.597Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9082987042b83102cc0c81ed631b0c22", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T16:07:47.497Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "768b406a9b7a6f7ac8a0a1a583a7f62f", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T16:10:38.809Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bb172432c788d73499357a30b144f110", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T16:21:51.905Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "13436b82f9db4e67f306993d9d0283e9", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T16:25:56.379Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "41d9cce23af2bbc0e6cc0dbbfc40de81", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T16:37:09.888Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9abdcbb1888717d544a1b014b3d13f98", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T16:37:53.039Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e069387d04776ba5739842ddd7030256", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T17:08:28.155Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ed94a59018e32a90e3232dd376185b6d", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T17:11:02.159Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0816444d4bff647aee65299a93c5abeb", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T17:17:03.329Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b1ffb6cf8b35eec75b69be50978360a7", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T17:23:14.077Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c410d985f977f9f58a8dfc1c46ef8d62", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T17:27:32.131Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a43952d370e6b347e17dea8045f4471a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T17:32:35.066Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ed3af6862bce8da40059e3600a5d6a60", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T17:40:26.349Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "33a0b73d89e147ccbbde951214583e95", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T17:47:16.302Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "47507b22266c07c0bf1e6bb7db59e219", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T17:56:04.914Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0ccb096a23f2c1dcbc49ac2f9bcb6d10", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T18:07:18.790Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dcfd46a06e85d21df13f5ce5f0a832fb", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T18:13:56.304Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dacac1bd6c81d70a923819f90ab29b24", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T18:22:14.248Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3bc17508b22cd0667a975317a91b34be", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T18:25:43.469Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9fde7665e95016e794a2b9f8d7f8dfc5", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T18:31:57.618Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "15c09e3e584b4b4571ff0e08f7c29e96", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T18:35:30.382Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "740986ec429abc4822ab4875a8a258e6", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T18:38:34.757Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "250bf8893082daed5483ca62054610ff", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T18:49:49.558Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1e4b9ec793fe598d0747531d94ac08cd", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T18:53:15.097Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f0487643ffd30b5cbb56e46109d4f36b", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T18:57:38.553Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d1fb75a1253f8d4fb4498d91a451adc5", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T19:03:58.553Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c893cca70fe263180d5e074f1bef0c28", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T19:06:47.644Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ef5514ecafac773e02b207b6245b84af", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T19:16:01.101Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "51756d831d8a7ba628425f78e54dfcaf", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T19:19:09.741Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dee627076e88065d6c492600df712f6a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T19:29:17.738Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d6f97f4aa2c4e3c75a5287bfa620f9b8", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T19:46:15.481Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "96a1a86382ee37c397b83902942e4b7a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T19:49:51.396Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7177172bb00353c209b2fa48a4b7dd56", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T19:58:06.114Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "22b2ec95b824d35a2fce124cac217821", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T20:01:44.574Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0311b584be563a426c2bfefacf64783f", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T20:04:31.226Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "36f99d624b08010415a8639227da0465", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T20:25:14.891Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "789ddfea82a588aa3aef51a03f107203", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T20:31:32.014Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a292a49a23e44bb1764738e60144b79f", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T20:37:49.997Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7fbb23eba841e745378f27137e088a76", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T20:57:36.488Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1ed642874eea1142db5e1497dd75108a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T21:12:10.066Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6621258c3fb40984c7a9404e1b156526", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T21:15:27.649Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "897539bf2c6d8894c538745df00b5da7", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T21:43:06.751Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "be44ffebf567000ca0e5815da8278960", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T21:48:46.171Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cfa94cacf6091e7b2a7acf5e37cc8a3e", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T21:56:50.422Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2556dcd6dc9920c842258a108217a9e9", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T21:59:56.773Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "50f71a32e6eca76443a65e5486f4151e", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T22:02:59.597Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "adb9dc0abcf5c7d63aa593c6bb96aafd", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T22:08:37.678Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "381089a3b52de383ca5f698babd571b8", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T22:25:14.077Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "570a377b0f85e96e62ac98a18f51ad2e", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T22:27:49.833Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d62098d947e1c83929cf5fafdb56e56b", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T22:33:09.825Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bd027a95a903337d6bcf9582c69e6344", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T22:39:34.116Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f205dab15fd2dfc0f84379d637d86964", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T22:43:11.941Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b3a1db752fd2bbd2fad59b80576804f0", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T22:49:10.713Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6f9f6acbcc916c884f5a21b78f67d73b", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T22:54:40.407Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d10beefa0dda87af2e57c9caeb3c26fb", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T23:02:39.796Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d62905acdbc0c649ce56e504475850ae", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T23:03:34.586Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7f574a0a8af640a4d2a4174a4ed2c920", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T23:16:01.711Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5ad7901ffc46d433e406764be7833021", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T23:19:37.389Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d89cbe5ab22ac95f6c42715659bf4e9a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T23:31:58.253Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "af01d0cd2954a47bf8d9e35b54c6629f", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T23:39:56.326Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "98fd3dea6f5d3b41b1578eb46f0a4dd2", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T23:43:08.929Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dc2c23ad6df24a5fff6c94c31d18770e", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T23:46:43.465Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "772ca3bb6ff1d0836196ca6513d5512a", "model": "sessions.session", "fields": {"expire_date": "2013-03-23T23:56:49.569Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4699a2b05c5cbad54c731c30bfe51454", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T00:12:59.578Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "752f289d9b4a5de583c8d32e65cd701c", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T00:15:58.987Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "44276c47e24d64226c3a349051b8303e", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T00:31:52.629Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a99c3cb92a9d5af8a757e908f07f42dd", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T00:35:59.833Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "559b103cddd57dc5f6efef18047d0005", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T00:55:07.518Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6932fd98777d8453789f173405521829", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T00:57:46.740Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "47827d33cef2127d00376920b1bca670", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T01:03:33.741Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "95192bce37fc06b2ab3d3902b1868321", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T01:06:10.754Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1722b05b9b5c3b00c5b6c985b04c6452", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T01:11:28.569Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9fc5e659403a2963ab0ba8cf9ade1a41", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T01:20:14.613Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "621310f4fe2add3ea8b2cbada098ab79", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T01:23:31.757Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e48234b05e22fa5029e00038f95f1586", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T01:29:43.095Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "521ed3bd28ae6ee76a781213f0890f76", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T01:32:42.642Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4e185fd01d6a4e98d22833f5026c2fc2", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T01:35:13.849Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "59a64c6965bf94dd9d033d828665c4c2", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T01:48:56.022Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "09cc0a2028b630d8a6f5ac0faaa502f9", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T01:52:50.896Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e21a3d76a55f67fd984594abf7b0af1f", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T01:58:39.688Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "12afb118dca629f0dfd13978588156ec", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T02:11:57.227Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bd06039fdf04b9b85eb3e00642ce0a98", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T02:15:04.269Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0992a01a50a0f12e28e5722a8f2972d1", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T02:18:30.346Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8af99989112864ddee2d2dcf9c79a699", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T02:21:59.400Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bd591ba907de56f7ee30e6db09f72a70", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T02:38:11.399Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5ace53086da70822c71e1d409afa6b28", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T02:46:52.895Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4384044f87c4a604917660ff9b6a65bd", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T02:53:01.113Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d776327caf00b7b0a955433572c28f7a", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T02:55:52.335Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "734853fa4f3b78433174ee8b780d119a", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T03:02:06.372Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "db4b501b46aaae1c73f21ccdc6814802", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T03:12:11.385Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "39a12b4413c76ca0d34a953f142249e1", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T03:17:48.797Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "128170852c202a9f5ac9d797f754f6b8", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T03:28:16.185Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8fb29aee65ee1ab5c79d78e227461d47", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T03:36:57.742Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3098d4d47f765c6dacbcc5532f029d7a", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T03:50:51.775Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c022eaf2244ad241be2a447991fc3241", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T04:04:47.437Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8a788d9c53895bc60af5ab6876b3dbf8", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T04:10:39.501Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e2b51b3bc42c802715859f0157f8e2c5", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T04:16:05.889Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2f80b46cd2f9d7ac7c255f7aab771f88", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T04:19:54.573Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "62b41506a68832a0ec2bb714b8cf40a3", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T04:26:07.953Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "42d8717df35ef6689428cc1986f09d13", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T04:34:51.943Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4053fa612b7932ac913e2d47dacd47aa", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T04:37:48.857Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9b13fdf8e98f7185b29dc765e50879de", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T04:47:03.116Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "71bb72a08fca6b1dc64845d9314b1e0b", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T05:00:35.717Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c3ed7cc4dcdece4ea8e592639278d6bb", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T05:07:27.496Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4495ee89cf6c533044a0514f5e7e718d", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T05:12:50.109Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "216820b6338505275f6af49797aa27ed", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T05:15:25.597Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "57013e33126409fbcd8ac8a4cb47da83", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T05:23:44.423Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2da7579ba993692fab222092a579a17c", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T05:27:39.930Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8b2927f3ec6da42843fef7d598fe63d1", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T05:34:47.973Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "deacf0b6452873fc15dfa8657c0b6f5c", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T06:00:36.924Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f71f5192986daf62aa4925cae2995b58", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T06:08:29.559Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bc9c84bbb20b836f1cc06930a7eddbe8", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T06:23:28.877Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d97a862816588abcef35d9855d22fe06", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T06:29:11.537Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f9f355029675e4bf69533f95dbdd4f07", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T06:43:23.685Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9e0a93760bd949f24b88a1f29cead97d", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T06:47:02.356Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "32a6c1a7bd3da77de229923ca3456077", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T06:50:36.466Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b8dddd34d3cde172a723bd990043a5a2", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T06:56:22.173Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c0852236d8f824ef46e286acaafa2ef8", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T07:00:15.322Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "320a2b3b8ef955db45aa6408a81af4de", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T07:09:09.785Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a90ade46bfe8c20ceed28712d89683b8", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T07:15:35.647Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dcc13e43dcd0c55a047dfc39ce3c8b05", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T07:27:20.449Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "943f2af2cae8d8c0a9035802acb2cde2", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T07:30:49.794Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7898435b159d25685de510f373a3e839", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T07:33:49.255Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ca14486ce2bd81c172eca3221721e4a9", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T07:37:23.801Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ffaceec88b9c3de3a741f356e0ec6c20", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T07:43:06.697Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "330431373d655dfd1a25052984742078", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T07:49:03.717Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "90a31fbec1a41365e0d392c8cda1a451", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T07:52:16.971Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ef84c32ab6aa50470ac3af88dacbbcbc", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T07:55:31.386Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4dbdec530fad6b3f7323cb941706606d", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T08:01:32.179Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e3d271ed092704d8ffc43ecc900348a1", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T08:10:03.220Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d234fb4fec30aad0bfa6190791ba4ccf", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T08:21:19.238Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6c1b9cc9013bc8cd25f5e90a4ed08555", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T08:25:05.460Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f2d66c3a3c336147f6caae79172137f5", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T08:37:35.026Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "533a7e6f340923be60d62e08c8f8d009", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T08:41:12.509Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e65a008294974c0852a203ef0355ccef", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T08:44:45.201Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fcd2f4117afdb040781b3449155826eb", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T08:51:03.605Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6dbb2474a91d618a65744dda3dff2a14", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T08:56:59.553Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "feda958a605bb815199724e69be1d1ac", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T09:12:17.973Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1c173367b0f93a5e865353affd18aff0", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T09:20:32.725Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "24ddb37152e0959c7a4c3a06c8f30b41", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T09:31:14.017Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "34d6409686dac329bd1e88eabbae0559", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T09:56:49.711Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9fc0fc3a0cd3298b8ee66107d21c675f", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T10:02:53.189Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "30f34c58d95f1ae14e2fa8b690bb38d5", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T10:07:20.974Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0d8bd35a628ca08a10dd718d50db0482", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T10:17:25.417Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "24a2c9a0491cfd72d7f4f88496245696", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T10:20:16.102Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b451862f4106af3992fde414251ec88e", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T10:32:36.241Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e534c4808da7b7878630f07509e9782b", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T10:41:40.069Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "071b45bb078e2c3ce5f5302251d0b681", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T10:44:49.934Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e8d5c1479afb68ff72f1cceb5a932545", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T10:50:23.189Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2f22b9835c070a8ac645a015418df49e", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T11:02:45.768Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4948f95f96e6e1afd3159e8506231dab", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T11:16:17.142Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8bd56e96b683b1722b50ae520a5afff4", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T11:24:47.133Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bc0f96a6c4cadc133726a37f7188f236", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T11:25:48.721Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ef9bcaa563ccf7f4f7ad3921336c4e70", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T11:25:49.474Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "58000ba176bde7613c362e286fdede1e", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T11:35:27.373Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f97399cc53de69b4414980817e1bbdc0", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T11:36:33.579Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1182bb46e89ec32e230dda0e43748ca2", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T11:42:08.013Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "aa11710dd337c920b477de3e2765c2da", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T11:47:10.444Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b4897fb16189e85a4cd8d42854db8323", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T11:56:31.727Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1f5a99a1043134dd7a414ca2579b7dcf", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T12:00:43.741Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f41b0e389501d85c2589d29deb5d1f08", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T12:25:23.879Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "23826773272c02a3667f50dca93b9e71", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T12:32:39.016Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b4ed49994708e9bc8bb359054222e52b", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T12:53:36.197Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e67392dae8db52264eca99d060abbdde", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T12:59:36.445Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "24f6e9ba3429d499f534a95d3c5df90e", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T13:31:04.545Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "166e3e0f8390c22909741dde04d34176", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T13:40:05.536Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "75afb46af777925de846324e566853d7", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T13:43:37.785Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "68ab2688b2abbe6bb1409c73a54e41fe", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T13:47:44.945Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d7f529d0db1f046a093916279cefd372", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T13:50:55.484Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dad9d2718a0ba01c62d41def7f50f9ae", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T13:56:52.837Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8c1ca05da0d696040baf498026f91eda", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T14:15:55.307Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "762279cd999e482120c5140cc0c1b048", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T14:18:51.010Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c9ec4ca3cdd77efacd056f6ad394276a", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T14:27:09.296Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "42b0664484abc2465d6d1984908d4f09", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T14:42:00.001Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9cb5c926e1e4e3c97329f635de08e0c7", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T15:16:56.789Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5a8d7c2b22911503c97969366d3a588b", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T15:22:07.278Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "48929e48b6798b05363dcc52af4e59c8", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T15:29:33.641Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f59318622d332371c95ab97e441f5065", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T15:37:43.113Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "72e3c369a3160f57dd4aea7b0d81f300", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T15:41:30.166Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0291619977d9069c4af12519af703ead", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T15:47:04.401Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "13797dd2e91918d9133e6d0373f796d3", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T15:53:11.301Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "461909ade3b7ad03331d3671f2f2497d", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T16:02:12.666Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b0e50c7e2434ae9fd2f4541e2b77200c", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T16:05:12.851Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a285e08de4cc8a8a64bc30f672dc17f1", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T16:08:33.405Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c3f1fb7bd0e51c10d67611ce8e6e3382", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T16:12:01.080Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d0f18d111e78cf63b0e19cdbb23726f2", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T16:19:45.276Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "43afd12be79e746039b01266b57bcac0", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T16:22:11.951Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bf3af65956d1a483e696f82a72e15a04", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T16:24:36.244Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "50db415807f75270434e6d48fc2d6142", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T16:28:45.447Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6796782ef889e23ad3900e82b7e099dd", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T16:31:53.399Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b0d37de3dc19de3acc333bcf57b97eb9", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T16:35:45.294Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a7a4cb9cb46d11392c28c2857ab91eb9", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T16:46:10.318Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9290b5e31ba8baf74411e3ac3676e376", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T16:51:12.217Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "58f9a65af7e69f524627fc90105c33b8", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:00:01.266Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f3eedda2fa46842825725e1473be6b92", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:02:28.607Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7adbe50dd4e3ea01175ffacaf8ba99c8", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:06:36.971Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "91da52bf118b3e4ecd419a7fa569f4aa", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:09:12.559Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3864784765efb02ef94abc9e16e80124", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:11:41.566Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "63776948615bece9f01a98bd23281b10", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:14:18.065Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "639af2141f9bde7c1f61f780c14a176b", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:17:46.140Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0e9b12385c025fe0e87839a3c83a3d0d", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:20:14.569Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0d49ea3207e8cfbd7d5efa38b07ff3f5", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:25:51.868Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8e17242a2d8a0682e12194fdddb07141", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:28:30.615Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e89096154cb17d80157420f903cfd8cb", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:37:07.777Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b3bd10dcff194b6e0e2a8314fde5f8b4", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:40:15.325Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c518625a620de2e54f35fb151bee43c0", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:43:35.393Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bae8d82cea09125420a85e973f0ed38b", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T17:48:49.785Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1a1ec9bd27bf8eafa0d8e79817d31a1f", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T18:11:34.045Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d1e244fb97e7985be3efac260c6eb68f", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T18:15:02.177Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2adb0d170a0c1a2cce298dcc0bd4e1c7", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T18:21:15.782Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ac16be564e1fe4ce1e8682a14901ca39", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T18:24:53.038Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3e138d44bfa984d9cedaec922f9e7ead", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T18:30:31.626Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "372be13a9d9c1b6be409294fcd268616", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T18:33:27.788Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "84aa4fb15e5cc2c80407c4fb0c5b3f2b", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T18:42:41.614Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2f4f41a4b5633a0daf3f3af24b52c5f6", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T18:48:37.942Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e3eb8c7078f2e13f8777dbd9e45474e4", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T18:54:20.678Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "db84673e1d46cdb41fa4f1f37ada2e66", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T18:59:45.059Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "550f4a42705fc6b0cf3c5ce495f9f51e", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T19:02:26.940Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2431d7810addd079a722867a1b10fa86", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T19:11:14.966Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fda21154e1ba305c749ba65f3d95f962", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T19:14:38.391Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "aec4227904cc60c6cf0291ed3bff637e", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T19:20:24.191Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0d25a7b91278d6da77d11743898c06d2", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T19:23:36.669Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a5e4b6acef85fc6f3fb6eef72b61cc76", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T19:29:09.077Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "65371b6712deb064686cef62cff1cb32", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T19:32:20.848Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "95c6590dcdcb6ea28b04077265e05f37", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T19:46:45.864Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c612caf5bf00238bde7ff5260d3865f4", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T19:49:51.781Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9f90793bcc4d828cfb4394dc5b00768c", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T20:06:23.069Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5c41874b2cbfaa5a532267f0dc1f3a80", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T20:15:22.310Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "59d2f77612113693cca411cd8829fb4f", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T20:18:19.766Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "82f99fad573dd6306fda7e7bc1cd3b75", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T20:28:20.493Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "83c17c80420309f7ee013cbf0373b1b9", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T20:31:26.873Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3885c6fb277816da564576605ed5e030", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T20:39:50.399Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "73ca0c6c84d4668e291d65e24506b97c", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T20:42:48.919Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a271df0b3b58fbe86a5ca08408e6bf77", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T20:54:28.335Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "92b62b52d3e2ccbcdc3847b29a955a9d", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T20:57:01.101Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "92f0d410eb8b952713e7b094b9dd101a", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T21:11:13.321Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a231182a5bf46680ad9e86a48619606e", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T21:16:32.945Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b97f081168560a3e9fbe209e75e576ab", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T21:19:03.510Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7490c66cd95b2b5e7c161663ca0f7a55", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T21:29:55.760Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0a381b301c6c228736946fc7c4de127a", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T21:32:55.149Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9e01a30fc174d1989df533f772befb11", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T21:40:34.117Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5c8093ecfedaf47af50f8674876abe0c", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T21:46:25.111Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "236d178571cea9751052cc4c7316bd15", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T21:54:12.179Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "11a854e23f7c0975e46c943a2c6b4a2e", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T21:58:02.785Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0a4ce2dd83d3fb6e2e5eb09af9327b3c", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T22:00:59.189Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "32045a631fba814c4f75156f79490512", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T22:08:53.753Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "da7d3ec7dec98001ed7654d934026451", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T22:14:38.566Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b074dc2000711ea41ea36dd199b0efae", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T22:19:42.813Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "be0089f4445d7de02df6840962642249", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T22:22:53.930Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "06bbf397656fe20ded7569a12c910269", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T22:23:43.022Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f5a0829844e6feacaa304522c333d7fc", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T22:31:14.605Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ab226dbcd858d9d8bb2a0ef5b10d9f41", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T22:34:33.145Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4b9f7d9a4193650dd11d83b88ab3e160", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T22:37:23.669Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "863b195e7ba0b058526abd69b13f3b63", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T22:39:56.600Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b813774d5357e3a8a01a6c77302911ee", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T22:42:23.167Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c5b631e04dc18895e842c5b74e0f086b", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T22:50:15.629Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "64f471cbd11137db41ab23df09e80f06", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T22:53:54.899Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "51985bb80aafc1903e440c6a33732546", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T23:00:38.774Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "686538904daae84e984ff8954f745ddd", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T23:07:15.213Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "51df6f84a5b52d14c6b05abc64ea01fa", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T23:12:30.105Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8c5919baf304a7ba5c4eae9007a454eb", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T23:12:39.498Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "87d9a495da23a6dce5f31dfc38c3e317", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T23:15:16.471Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e0fba00841fd425d34777ad2720c5c03", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T23:19:11.935Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3eafa159173dfb9989ef8fc2d41358b6", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T23:28:15.569Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3db46bcb22f2d0e425851422b66a45ea", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T23:32:05.046Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "052795a3140d5d7c5e0145365a52dce6", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T23:37:39.609Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5f9bb7e281affda2cab2deffffb32d8d", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T23:40:07.927Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1919748e8dcda68d31230ccc537b60e7", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T23:43:15.310Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a5623fcfa63827befa5e9b4761add204", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T23:49:14.785Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c343a9f8b7a23f4453ab7549260fc458", "model": "sessions.session", "fields": {"expire_date": "2013-03-24T23:52:32.225Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b6ca503ab3d7bd833aac6e939eaf52ec", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T00:17:02.171Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bf92f3796294b5367174028e4bc9ec47", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T00:23:25.823Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cadc3e747df234db73f46f5613abce9c", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T00:26:29.952Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "13a9c8ab2e6e2bcb426d41d0686e99ed", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T00:35:08.525Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9d14cdb02d26666fa0604d9895b46a75", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T00:40:19.691Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ddbf23f2df620fb2a7ba07fde501396a", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T00:42:48.140Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "77154d5daa5ea82149d1c0ac67695869", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T00:47:49.071Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "44a324246d58afe373fc526671c2a30b", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T00:51:37.917Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "158e38bc8700aa61fe07ebcb8682d591", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T00:55:32.573Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0cfe8d326a6eb5bf18eacbe7e72b6369", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T01:01:41.097Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "544f371f9d8ef1389e56ad7d69394dc3", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T01:04:49.669Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0adf96e483e6884cbea301035359bb7d", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T01:08:51.881Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "db7437e9439acb59873953ab6a0fd025", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T01:12:41.937Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "80d5e99f09a45d75066b43a3dafb196a", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T01:15:58.141Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ed79a96253c6cdebad905ce462ef40e1", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T01:21:19.023Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d1cc9361861bd10ec61e2bcc8858e109", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T01:27:52.625Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ac19e2e5d054ca42f470958e7adde916", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T01:31:29.594Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "885b51f7cf1ba8153e4e23d70ebf9be7", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T01:34:21.776Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "89f0561a75d194d49c81ead1a3f174aa", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T01:51:50.481Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4e3edd937aee48560d4b65ea2cd4b2b1", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T01:57:55.031Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "46dfb71e19c770a1d8f79589873fe775", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T02:00:36.306Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f74e7f5c25a040127b10b93ee58f9530", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T02:04:47.806Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c9fc509e6f26720b6ac11fd5855f8709", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T02:08:32.231Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "153976f7ac3d2f906bd5c2bb315ba066", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T02:17:04.153Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4314e4f89b7cef5f50ef6623b2158c4f", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T02:21:00.836Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a6996c98e205931d88eba84d3eed4021", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T02:24:32.053Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "aa85b9771891dc5df411643a38f84292", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T02:33:08.414Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ccf4b857caa7dc47e25f0c8f4809b06c", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T02:35:59.693Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7d122084b3bb1c1d8044d768079c7cce", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T02:39:56.315Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1b02f4cc904412700b48ce9f16464a8f", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T02:42:55.514Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6f80f90fad6ed95852062044cd13abb5", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T02:49:10.067Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1d46fd3f7425eb4b4bc1bd1fa05787dd", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T02:51:50.613Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2b6a323e02a1090e67efcfa5f0a80bfd", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T02:59:01.790Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "17b8bf38136be12e58aa77db597f5c8b", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T03:05:05.228Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c97521ca94cc4de2b17792d61df94b4d", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T03:12:50.745Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e7fbe98239d6d2c02e496de580b33d04", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T03:15:36.185Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9ab9e9570de0a0563338a5beb52f0e95", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T03:19:08.933Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "018a45fc2b3041912f2ab58df1ffd2ab", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T03:28:05.696Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d1b73ec8e54d4e0c788421fd45565f28", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T03:39:49.149Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5f14a8fd7e6eed2fcba60af1f3496cac", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T03:48:00.595Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c67a937aa5f61d7f8e7531ca06c7c163", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T03:50:25.481Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5859575c0b69db2e53b55eb98ac23245", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T03:56:20.201Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d1561a0812ed171d6d8ef5bd38107e41", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T04:25:00.455Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fe35bdac01b8be221d0a080f4cdd48e3", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T04:28:36.760Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "13e3afe70db912c6ca03e955052a9e4e", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T04:34:12.071Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "62fd5b2ee3a3cbb84d17100009185ddf", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T04:46:43.629Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bf9ea78dd673a14c0031e7b541e0eadc", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T05:00:51.837Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7de21eda1fcec70c07ad0d442c6566a6", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T05:04:40.826Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "42a93d147f3fbd2e9f80627e1a799789", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T05:13:08.129Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cf2129e6634ca9e5e7ce0dc75ea691fe", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T05:18:39.444Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6fd094b447214be7475fc669325d08c5", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T05:21:59.701Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f38239e2a95710bdb331c063d2596ba6", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T05:25:00.233Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0d46d93f0b8ef5bb7e0522a9c738637d", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T05:33:32.138Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "efe97eb83c9af81a3625632679a3f38c", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T05:50:31.785Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "04d8763a2c542dea86f9fb66ea78f423", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T05:55:54.463Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "81c5d01fb65be7c64ccbd8f075210aaa", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T05:59:30.801Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "862736cf38c3da2776bf576337a342bc", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T06:08:39.877Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9103a462dab348841234dbed3feb512f", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T06:11:46.729Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fe7cf8e252680ad13d36dfa6a215e45f", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T06:15:38.989Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5551d5fac3e9964cd2d03380c8f890ea", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T06:26:22.681Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "357614c7cf77143d79bb2b0e604a4dc9", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T06:32:12.393Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5445ddd5469f402c9d05bd750602c1c5", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T06:36:11.517Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f1abfef5d72d2efa5ce85ce33bea3f5b", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T06:44:22.987Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0b0b18f59d3d96e82f1931638bcafbc8", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T06:47:08.810Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4f992bbbdbfe9114c332870127f22db3", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T06:57:42.794Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "92ad13c0699d64db1e7e73c263be2614", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T07:02:04.862Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cbceb882d51a06a76d9f287100bd7fd2", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T07:23:09.745Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1b3568ee7a7a512d6e1ff70fbbf889a3", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T07:25:57.950Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fa207c7b0dd6a70f60b3b66535a1598f", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T07:29:15.481Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "485a71198bd287de30c2d525ebb3a2f3", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T07:34:52.593Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "76822659bac0b943b1d3de8c7c8ffe68", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T07:52:46.333Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5351fa611f9c63ae188975e4c2f8d12f", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T07:56:08.657Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b6762407fa4da05fa59d85b163bb21a6", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T08:02:48.463Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "da9f61bc273ce94306f6b8c651c29aeb", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T08:08:28.641Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "96abcf5edd49abf15a2649ef3461830d", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T08:17:39.401Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "55391c00bfd7ce39cc84a9339670e06d", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T08:31:39.670Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "57d367271de1f725279239d2dee8ad29", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T08:34:53.045Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5ea4fa4f43ec5c88efd487f8f2d41e08", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T08:41:15.498Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3c3d5f6f36441461946560a3eae09a11", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T08:46:57.205Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "446ecf7c5c7eab4db39c4e19b85213a6", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T08:49:58.669Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f10d244d7f22354ec35b1a14058b3fdb", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T08:52:39.453Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "51b81445d7019edf848c23dc67d64e27", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T08:56:22.377Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3caa0288e601334ac994baed61c1879c", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T09:10:05.088Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "701b170cb46939ae12673b94f045d658", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T09:13:12.395Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4410d2fc9b5afada96bf564deac6dfea", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T09:19:31.151Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a94bb93921987fd69c38cb6fc12900d7", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T09:23:30.725Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0e33bc069d28c2728173e790c9c49072", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T09:28:54.016Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cc6c393edbf785a77719b34cbcdb147d", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T09:32:01.753Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "52cba3deebef8392390c769d6a526b34", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T09:39:52.457Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9037b91234f250e445daa0052735eeb3", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T09:56:17.933Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cbc414f2bfff03b799476892e6d0a7e5", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T09:58:56.604Z", "session_data": "MTg2YzYyZjIxMmE2NjJiNzAwN2U0ZDU1YjE5Y2I2MzlkZTNiOTA0MTqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLBXUu\n"}}, {"pk": "064395b7abe98166a685ced44cf5c343", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T10:11:47.653Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dc4f4dcfdb5a84ac4d416846fc75335b", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T10:14:35.489Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5dc906b5b96ec777d2e55e6d9db2cb5f", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T10:19:15.861Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9f20cd720101a0c62de64b7a0c13089a", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T10:22:15.813Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8288aa5f8e79c18a9105b2c2c8116a61", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T10:26:38.181Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "65bc186b773f2b2ac45c7b63dbc97494", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T10:43:36.077Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "98b606d5e5808e52a49ad1555ae4ff05", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T10:46:36.454Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "976e41a7d99280e0c287846f746514ab", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T10:49:08.123Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "164538851035fce2fddd1406b439c14c", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T10:52:26.568Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9b68ea90a00756053b50a9c83e840ca8", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T10:55:46.567Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f7fddd1e26c38bfdc345d57c1b2c3453", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T11:03:53.297Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a8840bb7b6cf3a6d34bba5266ac7b428", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T11:06:26.383Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6ee6740b3510e174b1f1c1a4d9a2fdb0", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T11:13:28.373Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2b68175be4d89e9eb887111b0279039a", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T11:13:29.010Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "17b4d80b538ca93a9063a3d1658ce1d3", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T11:17:45.201Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7c19ce2b59a053a0da1fb3d3e5141474", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T11:20:28.451Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f4d6a768ae708078db1cf8ab1431b8a6", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T11:25:41.746Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0048442e1de2b49640c95c29bfd03e2c", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T11:31:50.161Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a3a64f2bf09e16010f6d793036416fdb", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T11:38:18.990Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6bf37ed4a431e005a99c59c22cda0a32", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T11:41:44.577Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9803be66e544f0ec7d4cf2816b6eb16d", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T11:45:20.342Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "20fb2d0bdedbf20712d05014a08b44ec", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T11:51:32.569Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "da9511c9b297cc3ccf4983eee018a255", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T12:00:15.737Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f95bca55d6f612a6322cd0c447f7ea2f", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T12:06:02.517Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "136a597a32d0ee2db0951b3d95f5c93b", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T12:22:13.942Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3af296f0e4fb4580f6e4745e1538689e", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T12:24:58.711Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5e0ba48f58239ba97ae151504b749862", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T12:28:21.024Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b9b4d1262de13fe2f13cdee16616c3e3", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T12:31:27.848Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b6015110c47cac44e983f9d5d7875adf", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T12:35:44.377Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0f1efb81bdcbbf2a3684db4cd63ae03a", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T12:41:54.477Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a69ebdad93ef66f4e63a4e5cffb72202", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T12:44:44.832Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "64feb8c9d88cb84930125289265c9623", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T12:50:03.070Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "be0edbffbc88afae2df4d6a58bcadb21", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T12:59:41.021Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "379420a16eabdab610d3aee99ea0a8e4", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T13:05:08.851Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0d8808d081cf0a2d95ee3de3f5e36cfe", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T13:09:28.317Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4aa92187a90d38ef9911115eb9028822", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T13:18:31.981Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f7e6e4baaf108bc071558d1bcfdcd902", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T13:21:28.806Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "147089609d30c4dad579cc7c944e424a", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T13:27:58.983Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a681627aec7ca74fa647c7069b4cf7ba", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T13:34:31.231Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ad31605ad78424c8909e2df0b12f34e7", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T13:41:55.336Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3f2e873dacdf67eee78df99f99a4531f", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T13:46:20.658Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "85370feaecefbb5c8f153266c8b6cedb", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T14:17:05.341Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "615c24ec929369691fbf2744e5834e27", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T14:20:42.235Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3125d8b0fd2a0964d74f3cfbd29b1073", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T14:27:03.160Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "56b5b3f5bf04b4fb07c6b0c9ed596f59", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T14:29:27.337Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2b32f2f146a0f6d5a41314f80067ef9d", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T14:35:16.263Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a78f8cb77c8148fdabb00a5e2c9a8eb5", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T14:43:57.640Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "96906784770f5c3f2b62dbf33b52481b", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T14:56:04.731Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a3f9f0d78249daefaca8f8e8f32836f3", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T14:59:11.557Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f862b6a84273065ae18c3a4d2db42d3a", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T15:01:59.721Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5d9ec8549e92659280f21b24d5331912", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T15:02:24.320Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "32470b55ca67c10b14b42d161aee2fb5", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T15:04:49.970Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "707b3dd4c8b4f14964eb19395e10747c", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T15:10:48.411Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d3b85254528adb1a0fc7af7450249191", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T15:14:02.617Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a30c6089cafe9ae9deaa05c4c6d84e21", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T15:23:39.589Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a6d855e5202403341e95c9af62ddc4e5", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T15:34:31.697Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1b73c2f3718d1366013acd7639efc757", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T15:39:55.669Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "07c2da55122ff04a4df3b5f98fdaefe8", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T15:42:32.934Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2f52ff5149b584a6e255cd3889887c6b", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T15:53:46.921Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d2cae88073d0d80d981293c5eb77b284", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T15:57:40.865Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "70b6501c662e82317705296ec8a0d27c", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T16:01:49.755Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ee1f0ef57b984914c540edd10b7da184", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T16:13:51.715Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "92a958bdb50739af07bc974ea66eac22", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T16:20:02.150Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c29009fe6f704fddff1ae3599173a63e", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T16:23:27.044Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2d94bc62ee88f5a8172173dfcdcdb5ab", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T16:29:03.661Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "31a6c490eae7207a950a0e4efb5f54b0", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T16:34:30.153Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ca6642960539996d2973fb34e369f531", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T16:40:48.706Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dfa06e26d62fa77c2d8b36bffa8782ba", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T16:48:32.558Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8de81d4e52d92e2339d73933ee0e9a1a", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T16:51:58.417Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "945f9f9cc683c04256b18369e35be755", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T16:55:15.301Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0d61eb80938243a5f38844f940afb66b", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T16:58:20.813Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "222b94b5af3f0021b4204b285b4ab41e", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T17:07:26.175Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cae452707c82793280946ee1073f10eb", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T17:12:38.859Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "880dd14849badff4bbe800b54e5247a5", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T17:16:37.343Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0489a0a015a49cd88170c01894323b1e", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T17:19:17.007Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "028359bef8f8f4decdbb8e220777da80", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T17:21:47.968Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3146fe58f18fc75e8011766ac94eec54", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T17:24:55.639Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fc174a4edbde71d0afd2a7d93cc97577", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T17:33:32.065Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ba141ed2ae139cafe1e425f2ae9eb69d", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T17:51:06.087Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bb963cbea1cf5ee16bad47d3d2bf55d9", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T17:58:51.918Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f3db5dab1b3713b5e712f4409861b1c4", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T18:01:24.460Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b9d56996019c75df357ec12ddd3e8450", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T18:08:02.898Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e8e6edacf1682a0870f45bac0d2544a4", "model": "sessions.session", "fields": {"expire_date": "2013-03-25T22:27:05.842Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "aec11a640e854374f67e8ac9e2effef5", "model": "sessions.session", "fields": {"expire_date": "2013-03-26T11:09:15.461Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7a5ceea90b0b8224eadab5f3af55d727", "model": "sessions.session", "fields": {"expire_date": "2013-03-26T13:12:26.568Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0d8daf45057a5253323a888360338c61", "model": "sessions.session", "fields": {"expire_date": "2013-03-26T13:48:09.673Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "315f6b01164b5aa5ac4d74145095cb57", "model": "sessions.session", "fields": {"expire_date": "2013-03-26T14:07:53.529Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b84c3171c4de88581edd292a1d66fc89", "model": "sessions.session", "fields": {"expire_date": "2013-03-26T17:22:00.243Z", "session_data": "ZWMwNWI1MDljNTNkMzE5MjliMTFiMzgzYmNmZGY3NTE1NDlkMzQzYzqAAn1xAShVDV9hdXRoX3Vz\nZXJfaWRLC1USX2F1dGhfdXNlcl9iYWNrZW5kVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRz\nLk1vZGVsQmFja2VuZHECdS4=\n"}}, {"pk": "7a018138bd1f601e2fc29a3d4961bf58", "model": "sessions.session", "fields": {"expire_date": "2013-03-27T02:04:42.570Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ece550770e3bdff0f60c38fff4ca1c96", "model": "sessions.session", "fields": {"expire_date": "2013-03-27T06:55:10.481Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fd4e77c7b8ffe102fa4d37346f290cb1", "model": "sessions.session", "fields": {"expire_date": "2013-03-27T06:55:10.721Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "264b5dfa3a2c30e6487e0886b8da6e4f", "model": "sessions.session", "fields": {"expire_date": "2013-03-27T06:56:28.493Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "aa9684f9523add7b5fddbaecee6b9dd9", "model": "sessions.session", "fields": {"expire_date": "2013-03-27T07:13:30.588Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5e17517912d0e1b2135a802873531a2c", "model": "sessions.session", "fields": {"expire_date": "2013-03-27T07:29:44.217Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f5fa0dc15df19e53013cabdac805ba46", "model": "sessions.session", "fields": {"expire_date": "2013-03-27T07:35:04.799Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5345093a6a90142ceb98fc7cb8ed09d9", "model": "sessions.session", "fields": {"expire_date": "2013-03-27T11:08:35.117Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a3da865460df23bcb0cc88687ec9fc32", "model": "sessions.session", "fields": {"expire_date": "2013-03-27T14:09:59.259Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "de94b075a14f9b755ee4d46e15b1a666", "model": "sessions.session", "fields": {"expire_date": "2013-03-27T15:52:06.518Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7cf0f9002431fd9ecc90b48c63fdf789", "model": "sessions.session", "fields": {"expire_date": "2013-03-28T02:05:51.717Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c8921e2c26291a77be63ec13222c5f35", "model": "sessions.session", "fields": {"expire_date": "2013-03-28T06:41:28.593Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "982c2c72f2a13bbdf2a40eeae7d1c7ef", "model": "sessions.session", "fields": {"expire_date": "2013-03-28T06:47:17.178Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b32c776580000c1af2544d9c1425ae76", "model": "sessions.session", "fields": {"expire_date": "2013-03-28T08:14:21.660Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f8e4b84882bf01479ad73003b3117dd2", "model": "sessions.session", "fields": {"expire_date": "2013-03-28T09:34:21.026Z", "session_data": "MTg2YzYyZjIxMmE2NjJiNzAwN2U0ZDU1YjE5Y2I2MzlkZTNiOTA0MTqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLBXUu\n"}}, {"pk": "06d8d577d0ce9e30bf425a24718b642e", "model": "sessions.session", "fields": {"expire_date": "2013-03-28T11:06:22.762Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f2660f028253767acf243394ffc98f52", "model": "sessions.session", "fields": {"expire_date": "2013-03-28T13:02:15.121Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "a8e6a41edfb11993034c68338ed8076b", "model": "sessions.session", "fields": {"expire_date": "2013-03-28T14:02:34.385Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9dc5e70a17f0e9810fc3251465641c86", "model": "sessions.session", "fields": {"expire_date": "2013-03-28T14:08:54.034Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "68e16c203b316ba795a87426a1c772a8", "model": "sessions.session", "fields": {"expire_date": "2013-03-29T00:00:08.211Z", "session_data": "MTM3ZGEzZTljYWM2MTEzNTE4OTU1MWI4YjAxYTI3MTczY2MyNTA5YzqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLCXUu\n"}}, {"pk": "d4df461ff94801e9ce152fa6d34ea524", "model": "sessions.session", "fields": {"expire_date": "2013-03-29T00:05:21.630Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e2383a37ddca7cc91320a85f7de5330f", "model": "sessions.session", "fields": {"expire_date": "2013-03-29T02:41:55.964Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "65bb6cf5d3604a21c481756511d0aa3f", "model": "sessions.session", "fields": {"expire_date": "2013-03-29T07:25:02.412Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3acd5086115f01101514e7180e26566e", "model": "sessions.session", "fields": {"expire_date": "2013-03-29T13:44:33.954Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "96ed55b8a45c4d4759ee462f3986123d", "model": "sessions.session", "fields": {"expire_date": "2013-03-29T13:49:11.394Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "26102fc4278f7873fe6f5741e5fea911", "model": "sessions.session", "fields": {"expire_date": "2013-03-30T02:00:17.221Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6b91c1f6392ee6018e164040c2e0c0eb", "model": "sessions.session", "fields": {"expire_date": "2013-03-30T09:03:29.337Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "38fcf441a2e2626b240b8259e562ac4d", "model": "sessions.session", "fields": {"expire_date": "2013-03-30T13:04:15.598Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "23d7dfa6adf61de4fbf530d249d525ab", "model": "sessions.session", "fields": {"expire_date": "2013-03-30T13:54:03.075Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f86655a2f302c06008f55d17ede47087", "model": "sessions.session", "fields": {"expire_date": "2013-03-31T01:54:07.074Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "abfbabdc2b563df2da79f9d5eea9f968", "model": "sessions.session", "fields": {"expire_date": "2013-03-31T10:55:10.309Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "28fcaafc5cd1e624610045d6fc1e9894", "model": "sessions.session", "fields": {"expire_date": "2013-03-31T13:14:43.423Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f64e3347b127dcf09b8899af932cf049", "model": "sessions.session", "fields": {"expire_date": "2013-03-31T13:44:14.092Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "54726d131dc790cd29af5146163369c3", "model": "sessions.session", "fields": {"expire_date": "2013-03-31T16:28:12.385Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8ed1de313e77b5e1f23096e939da0f3d", "model": "sessions.session", "fields": {"expire_date": "2013-03-31T19:18:52.110Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0df90105914b68d83cc6abac683368ac", "model": "sessions.session", "fields": {"expire_date": "2013-03-31T19:19:29.354Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "67920975fdb0131944239713921d7e6e", "model": "sessions.session", "fields": {"expire_date": "2013-03-31T20:59:33.143Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "6dd8a5d751207bf8f257775fc36682f1", "model": "sessions.session", "fields": {"expire_date": "2013-04-01T01:55:32.812Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6f376d5332d608824e558694b70d0244", "model": "sessions.session", "fields": {"expire_date": "2013-04-01T05:21:40.206Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6f63d245dabfd4eaef5d25172278e0c7", "model": "sessions.session", "fields": {"expire_date": "2013-04-01T06:28:07.946Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "49a0e33d0eaf43f6192cbb89ed24aad0", "model": "sessions.session", "fields": {"expire_date": "2013-04-01T08:12:01.482Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f8fee56df74b52af0485a30b10eb519e", "model": "sessions.session", "fields": {"expire_date": "2013-04-01T09:38:39.369Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "678c7d741608beddd86c02e1752e5b31", "model": "sessions.session", "fields": {"expire_date": "2013-04-01T09:46:14.321Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "087d5d60992d688881d8ff086dcbf2cc", "model": "sessions.session", "fields": {"expire_date": "2013-04-01T13:49:57.995Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4b7d4280fd7d51d234189561037c173e", "model": "sessions.session", "fields": {"expire_date": "2013-04-01T14:00:16.280Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b2af0f2befe4fb41e2ae62e0bcda149b", "model": "sessions.session", "fields": {"expire_date": "2013-04-01T16:27:56.021Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9e6e9cb01182b4c54a331e2f35e6e6f0", "model": "sessions.session", "fields": {"expire_date": "2013-04-01T22:42:31.486Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "8f86c643ecd3b365bcb8eee8247e7696", "model": "sessions.session", "fields": {"expire_date": "2013-04-02T01:06:49.378Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a360897e00cb02addb5812027863f05f", "model": "sessions.session", "fields": {"expire_date": "2013-04-02T14:25:51.230Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3bc00bc6dfb7118b1ee0a25b419d976b", "model": "sessions.session", "fields": {"expire_date": "2013-04-02T16:07:01.780Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fff6edc50cf64997f413117c2ef9c735", "model": "sessions.session", "fields": {"expire_date": "2013-04-02T19:57:29.843Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1a146d863fb74b901275beea178c204f", "model": "sessions.session", "fields": {"expire_date": "2013-04-02T22:35:13.052Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6d424f952163e560c68dedc76edcf9fd", "model": "sessions.session", "fields": {"expire_date": "2013-04-03T08:05:26.818Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "532c7b036c0dce7eebb9ac11f8acc38f", "model": "sessions.session", "fields": {"expire_date": "2013-04-03T16:24:20.234Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a5b38d436a0f20dfaf482e32d84358c9", "model": "sessions.session", "fields": {"expire_date": "2013-04-03T20:02:28.192Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3bf10d53c3e0615a2f18cf92d58c4d88", "model": "sessions.session", "fields": {"expire_date": "2013-04-04T07:28:09.363Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "57e6f5112932f1b58cf53078aba17d3a", "model": "sessions.session", "fields": {"expire_date": "2013-04-04T08:27:44.641Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "30865420323fb64e47fc4857a0162bbc", "model": "sessions.session", "fields": {"expire_date": "2013-04-04T09:13:14.611Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8c19c09cb24c42a5fb2e520022b52af6", "model": "sessions.session", "fields": {"expire_date": "2013-04-04T09:13:15.166Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dfea5429799d31956871ed327dc46efd", "model": "sessions.session", "fields": {"expire_date": "2013-04-04T10:37:30.803Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c43464f115fa35090a410b547920a34b", "model": "sessions.session", "fields": {"expire_date": "2013-04-04T15:48:39.418Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "89493350c9f67969306d70ee3c7985a8", "model": "sessions.session", "fields": {"expire_date": "2013-04-04T18:22:29.445Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "97f6c2aeb210789051c724eb29e58e79", "model": "sessions.session", "fields": {"expire_date": "2013-04-04T20:12:02.261Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0291e4d8634e4727db2588796fcc5f78", "model": "sessions.session", "fields": {"expire_date": "2013-04-05T07:27:36.029Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1d2a2223e40674bdbc04af61cbb6762c", "model": "sessions.session", "fields": {"expire_date": "2013-04-05T13:20:31.308Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1b9d490a45e2e1b3f70478d29483a022", "model": "sessions.session", "fields": {"expire_date": "2013-04-05T21:13:56.248Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cbcecbb0751fedb2082f0ab5c809d936", "model": "sessions.session", "fields": {"expire_date": "2013-04-05T22:17:31.149Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7e198d3b2b130228f205ae7ea7101e55", "model": "sessions.session", "fields": {"expire_date": "2013-04-06T06:44:21.705Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a2b4125678a34e00c0d6d05a5542dc3b", "model": "sessions.session", "fields": {"expire_date": "2013-04-06T09:13:27.608Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2d19ed398ff7655a36d7f810b471d40f", "model": "sessions.session", "fields": {"expire_date": "2013-04-06T11:45:00.441Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e2d070771473d866943757852c6d03a6", "model": "sessions.session", "fields": {"expire_date": "2013-04-06T18:27:31.189Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e79d97144b264849409fd5794a482a3f", "model": "sessions.session", "fields": {"expire_date": "2013-04-06T21:23:26.501Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "02619ed589f5e02c67edc5ec566cbfae", "model": "sessions.session", "fields": {"expire_date": "2013-04-07T03:02:39.537Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "37867d4904639ce67ed5b9358047dd5c", "model": "sessions.session", "fields": {"expire_date": "2013-04-07T04:08:30.033Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0fe69123108a7e642963f5c24c876758", "model": "sessions.session", "fields": {"expire_date": "2013-04-07T04:10:30.067Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ed404777faa5ea44616c062fc023450d", "model": "sessions.session", "fields": {"expire_date": "2013-04-07T04:20:25.628Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2fa820bac13d28bc41386166801f1d26", "model": "sessions.session", "fields": {"expire_date": "2013-04-07T04:21:12.821Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2982b29e9d2647c6cefadf3a6cc7aedb", "model": "sessions.session", "fields": {"expire_date": "2013-04-07T04:22:26.597Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c7289547599025d27f3eae9181995086", "model": "sessions.session", "fields": {"expire_date": "2013-04-07T09:18:15.721Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "66ae5fe9f8587162b5acabf45f87d5f7", "model": "sessions.session", "fields": {"expire_date": "2013-04-07T12:49:13.405Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "40ea363e8e4c256edcf801a92a4098b4", "model": "sessions.session", "fields": {"expire_date": "2013-04-07T18:31:28.033Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "87df56c0f2c292ad316b3d43a790bbe2", "model": "sessions.session", "fields": {"expire_date": "2013-04-07T21:21:19.904Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6bc28c729673edadfb479c6f9357a51b", "model": "sessions.session", "fields": {"expire_date": "2013-04-08T03:11:20.957Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5aed97e5d835e98f5822bbf5f93e99f0", "model": "sessions.session", "fields": {"expire_date": "2013-04-08T05:15:13.729Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "522a75ebd6199b1912e8dfffbd9421dd", "model": "sessions.session", "fields": {"expire_date": "2013-04-08T09:28:44.774Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c4f45e6d125be34cc502d38320fb0038", "model": "sessions.session", "fields": {"expire_date": "2013-04-08T15:33:34.370Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cdfa682c47fb628de315a2f6c5341100", "model": "sessions.session", "fields": {"expire_date": "2013-04-08T21:11:04.021Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "efab615fbcd08c2c1be07bbc79818b49", "model": "sessions.session", "fields": {"expire_date": "2013-04-08T21:12:00.893Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d2becadd88f5474616d0924ea29cf25d", "model": "sessions.session", "fields": {"expire_date": "2013-04-09T02:56:28.113Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "361e090e2144a55f5e043058ccd6768c", "model": "sessions.session", "fields": {"expire_date": "2013-04-09T06:58:28.482Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ca01891c834a0b8fb4950e7c371b4c80", "model": "sessions.session", "fields": {"expire_date": "2013-04-09T07:54:18.183Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e2cf500b66bd62b4bc97333ab4c3394f", "model": "sessions.session", "fields": {"expire_date": "2013-04-09T12:10:54.223Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c4c4c2ca41d5abf006ebe1fc2fcc1b7a", "model": "sessions.session", "fields": {"expire_date": "2013-04-09T13:17:48.465Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "af328eefc4a7a5efe7f9e23e9e531273", "model": "sessions.session", "fields": {"expire_date": "2013-04-09T20:24:59.021Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bffe56ddc48e815e83f882b2fed18db6", "model": "sessions.session", "fields": {"expire_date": "2013-04-09T21:09:42.185Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a5de8f2c499d6db1ed89385c894785dc", "model": "sessions.session", "fields": {"expire_date": "2013-04-10T00:10:56.758Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "839f90bbc4ee9d8333848e038a31ff7b", "model": "sessions.session", "fields": {"expire_date": "2013-04-10T10:01:21.468Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f004336b7739edd354ac63d2c4648eab", "model": "sessions.session", "fields": {"expire_date": "2013-04-10T12:01:08.330Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9474f2678c1a21879b09f7f320b88316", "model": "sessions.session", "fields": {"expire_date": "2013-04-10T12:06:31.402Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dda01a68e58b9e56f520e1318316f23e", "model": "sessions.session", "fields": {"expire_date": "2013-04-10T12:28:48.738Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6b6d041201c8febda9b4bc6eeeeecc0e", "model": "sessions.session", "fields": {"expire_date": "2013-04-10T18:47:44.465Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3cf3ead60797cbb4f7a8c726b338a5f0", "model": "sessions.session", "fields": {"expire_date": "2013-04-10T18:49:25.511Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e9a43d0a75a06c25c22e69823944e012", "model": "sessions.session", "fields": {"expire_date": "2013-04-10T20:16:24.421Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "92f70edab7f227cc5d87d815d36bc247", "model": "sessions.session", "fields": {"expire_date": "2013-04-10T23:32:16.849Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3f13a3da96af8b609349b4d58cc38b54", "model": "sessions.session", "fields": {"expire_date": "2013-04-11T03:35:54.130Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "41ec0d784ed6633c1cd14fa4576db81c", "model": "sessions.session", "fields": {"expire_date": "2013-04-11T09:49:09.609Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d27cd5bb318a4b0051f936100d8393de", "model": "sessions.session", "fields": {"expire_date": "2013-04-11T12:26:18.908Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c76003bcdfe55fc8c4f915d1de30269f", "model": "sessions.session", "fields": {"expire_date": "2013-04-12T08:30:01.481Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "85d262836b19e38416e8e530efa700a0", "model": "sessions.session", "fields": {"expire_date": "2013-04-12T09:07:23.709Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d99930c536854992f0a977c8604b6dd8", "model": "sessions.session", "fields": {"expire_date": "2013-04-12T23:33:58.309Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ccc3b91c03b8b5aeba5977ad1b40c5f9", "model": "sessions.session", "fields": {"expire_date": "2013-04-13T05:36:55.730Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "04291d85ada45a94dad0321aee0fc2a6", "model": "sessions.session", "fields": {"expire_date": "2013-04-13T09:12:28.008Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f38cb1b08429c2491fce878b39d956e5", "model": "sessions.session", "fields": {"expire_date": "2013-04-13T09:15:02.786Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c841b8e939b43cbf0231b4b272901c79", "model": "sessions.session", "fields": {"expire_date": "2013-04-13T11:05:08.743Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "befce5cff7792a5e4a8dc392ec62efcb", "model": "sessions.session", "fields": {"expire_date": "2013-04-13T12:41:51.866Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "48699145a6fe25dd056bf38077cdf0b5", "model": "sessions.session", "fields": {"expire_date": "2013-04-13T15:30:48.627Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d62a77beaa3c207a93caa76625f91720", "model": "sessions.session", "fields": {"expire_date": "2013-04-13T17:32:00.132Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ac7c45e82508791c81ae159a2faf7272", "model": "sessions.session", "fields": {"expire_date": "2013-04-13T18:12:03.873Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0ae09f32e91966fac4dd39997db92f43", "model": "sessions.session", "fields": {"expire_date": "2013-04-13T20:31:48.301Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "db8eeb03ae1f457d9e83c7d9cb103e30", "model": "sessions.session", "fields": {"expire_date": "2013-04-14T06:07:35.979Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0d39f91e44eb7dc1bdcffbe0b8e39095", "model": "sessions.session", "fields": {"expire_date": "2013-04-14T06:50:54.509Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "32819d987192b44d45aa764583826db3", "model": "sessions.session", "fields": {"expire_date": "2013-04-14T08:33:56.892Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5b4bedec235f116562fd0af496ab2e40", "model": "sessions.session", "fields": {"expire_date": "2013-04-14T13:28:01.670Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "97988ff06e954860534180ca7e6e0f86", "model": "sessions.session", "fields": {"expire_date": "2013-04-14T18:08:19.625Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d69fa885e514dfe700b8a51ca07eb9d1", "model": "sessions.session", "fields": {"expire_date": "2013-04-14T19:09:16.365Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7418d32488870a908aed11d299555ce0", "model": "sessions.session", "fields": {"expire_date": "2013-04-15T01:09:36.545Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "efc3f17e0a881e8096b6efd3a47d3666", "model": "sessions.session", "fields": {"expire_date": "2013-04-15T01:09:37.605Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4a5249e4add32cc3f88c8221d7a5ed9c", "model": "sessions.session", "fields": {"expire_date": "2013-04-15T06:23:04.216Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "88d14ba0e7bb64852643f186553c91a9", "model": "sessions.session", "fields": {"expire_date": "2013-04-15T06:50:50.468Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "460c77a8f63d2cdd851a885e8cc25b33", "model": "sessions.session", "fields": {"expire_date": "2013-04-15T10:08:00.783Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1d15ae3829087ebf3519a464ef10b29e", "model": "sessions.session", "fields": {"expire_date": "2013-04-15T16:57:18.647Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ce99098f03b6ff2b08aa67a2547dac7b", "model": "sessions.session", "fields": {"expire_date": "2013-04-15T18:19:55.175Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b15f8fc31419e07ee93409572490af00", "model": "sessions.session", "fields": {"expire_date": "2013-04-16T07:48:08.004Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e0735053b1e2826b0c8dc90a8f2c3c09", "model": "sessions.session", "fields": {"expire_date": "2013-04-16T09:36:03.966Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "98b7b4cbca018a9332a22aa6b6d2f5ae", "model": "sessions.session", "fields": {"expire_date": "2013-04-16T10:23:18.388Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "65086fd652cb286dde20107fc6d9f4ef", "model": "sessions.session", "fields": {"expire_date": "2013-04-16T10:39:20.351Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0198094ffcd378c7aa18a7c8e2193026", "model": "sessions.session", "fields": {"expire_date": "2013-04-16T11:47:09.117Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3a1dc7aaf00eb75e8d986122008c8771", "model": "sessions.session", "fields": {"expire_date": "2013-04-16T18:14:28.436Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "487f4b45146f1851017fe92632fa0dec", "model": "sessions.session", "fields": {"expire_date": "2013-04-16T19:54:27.831Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c9fe01aae32e15c3e268eee5e9be40ab", "model": "sessions.session", "fields": {"expire_date": "2013-04-17T03:12:38.780Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "df79237029e0ec3468981ccffca177f1", "model": "sessions.session", "fields": {"expire_date": "2013-04-17T09:07:03.034Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "db17d20cc34f0354e66d9b5707dfca40", "model": "sessions.session", "fields": {"expire_date": "2013-04-17T11:48:00.613Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c4dd32b65de7bf5e93b7f3d7f0a4c496", "model": "sessions.session", "fields": {"expire_date": "2013-04-17T16:18:41.873Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "9daf96e4b333d3d792ded233166634b0", "model": "sessions.session", "fields": {"expire_date": "2013-04-17T20:49:30.428Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "607524a15380c12d1f69b41a38f7d316", "model": "sessions.session", "fields": {"expire_date": "2013-04-17T22:28:08.388Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9a6b9d58d8b9943f65f2d6ea40e8db9b", "model": "sessions.session", "fields": {"expire_date": "2013-04-18T08:50:53.697Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0843917e661d1a695e8f8767ec300bd2", "model": "sessions.session", "fields": {"expire_date": "2013-04-18T08:51:01.770Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9c9d69e7d8e4a01f8a237e0e1be05706", "model": "sessions.session", "fields": {"expire_date": "2013-04-18T08:55:25.785Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "96371dea9a096d0db6ad88103227f2da", "model": "sessions.session", "fields": {"expire_date": "2013-04-18T09:01:47.506Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "51cdbfbc70f6e4f80a2873aadcd4f4b3", "model": "sessions.session", "fields": {"expire_date": "2013-04-18T09:38:27.949Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "65314521a41fc297f44908cbaf62e792", "model": "sessions.session", "fields": {"expire_date": "2013-04-18T10:19:47.729Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ea8f7ff4f04ee2b88e0bc5eee66d4748", "model": "sessions.session", "fields": {"expire_date": "2013-04-18T10:24:10.953Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c13f9ee661e62c5e94cd0a77427208a9", "model": "sessions.session", "fields": {"expire_date": "2013-04-18T14:20:17.538Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "aa499691df0b0e55dc1715f7f91b7eef", "model": "sessions.session", "fields": {"expire_date": "2013-04-18T14:26:25.259Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7ad7ae7ed740242cbe674d80d9f27f0e", "model": "sessions.session", "fields": {"expire_date": "2013-04-18T22:36:13.211Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "432620165cd8e23aa57162de89b3be56", "model": "sessions.session", "fields": {"expire_date": "2013-04-19T10:39:21.206Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b8ca8baaf6b3d94a2280a58a46040700", "model": "sessions.session", "fields": {"expire_date": "2013-04-19T12:55:25.234Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "beb17a583da08ffceead4697d7d08fb3", "model": "sessions.session", "fields": {"expire_date": "2013-04-19T13:18:10.503Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "136528b26bf61157f03450dc7a3584cf", "model": "sessions.session", "fields": {"expire_date": "2013-04-19T13:32:03.499Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3c3c1eecf81b154eab35f1ce83f00dc9", "model": "sessions.session", "fields": {"expire_date": "2013-04-19T14:18:44.348Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "182b9d7f044d2134e54409dd51bda6e9", "model": "sessions.session", "fields": {"expire_date": "2013-04-19T19:55:58.268Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cbee51f9d9627ea08d7cff25337ebeaa", "model": "sessions.session", "fields": {"expire_date": "2013-04-20T09:32:31.229Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0c7177439493823c225bb3d72239d94d", "model": "sessions.session", "fields": {"expire_date": "2013-04-20T12:07:58.699Z", "session_data": "OGY2ODNmZTdkOGFlOWQ2YWNiOTQ0NWMxZTc1MmFiNjc1NDJhOTUzNjqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "63eb693ee6576c43d98ee99a1ab2fbe0", "model": "sessions.session", "fields": {"expire_date": "2013-04-20T13:45:01.439Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6ac8321e0ad4e8541994c070841e8ac7", "model": "sessions.session", "fields": {"expire_date": "2013-04-20T21:18:25.228Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "736c2aed93352467282ff0d3484f86ac", "model": "sessions.session", "fields": {"expire_date": "2013-04-21T01:15:34.968Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7de6e8dbd2ccc90ecd0fd9593f543c4d", "model": "sessions.session", "fields": {"expire_date": "2013-04-21T01:26:02.297Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f79f3b12134c9e6f8a35670154e8c980", "model": "sessions.session", "fields": {"expire_date": "2013-04-21T02:27:13.272Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5258e2cf09747e8462cb16e6d7afc7a2", "model": "sessions.session", "fields": {"expire_date": "2013-04-21T07:29:49.249Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e2cbd43ad310323a97fd80cdf394f960", "model": "sessions.session", "fields": {"expire_date": "2013-04-21T10:02:14.037Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "96a844c6a20ce595f4df91ef24f501c1", "model": "sessions.session", "fields": {"expire_date": "2013-04-21T15:37:53.373Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "eca169478eb39c8e12129d2670088b00", "model": "sessions.session", "fields": {"expire_date": "2013-04-21T19:53:13.535Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "208673f378c986c18598d8004c8a2cd2", "model": "sessions.session", "fields": {"expire_date": "2013-04-21T21:31:23.833Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f5c192a9f03010b9f0719f80de967acd", "model": "sessions.session", "fields": {"expire_date": "2013-04-22T01:54:00.830Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "89571da74db408133a0b1fc69b73a784", "model": "sessions.session", "fields": {"expire_date": "2013-04-22T09:41:58.963Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1c7a20f0cb10b20ce090cdabdcee423a", "model": "sessions.session", "fields": {"expire_date": "2013-04-22T14:37:43.479Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5430bac8034134c75d17deb14612ccf7", "model": "sessions.session", "fields": {"expire_date": "2013-04-22T16:55:08.274Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8fea8ea830a60b16100b7333cee842ca", "model": "sessions.session", "fields": {"expire_date": "2013-04-22T21:44:46.051Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a31f58aa879375b3e22d8f0567f3da64", "model": "sessions.session", "fields": {"expire_date": "2013-04-22T23:24:50.637Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b0c2443f643553e4fb2b7d8d6e574d59", "model": "sessions.session", "fields": {"expire_date": "2013-04-22T23:34:20.646Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4afba2470d8c0ffb3e8465a0cdc9e0f5", "model": "sessions.session", "fields": {"expire_date": "2013-04-23T03:10:09.501Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "368fc0d03c1fe6d33d755768e0cc6c0f", "model": "sessions.session", "fields": {"expire_date": "2013-04-23T06:26:27.167Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "26cb7468d0fe6235ec9d0116f128064c", "model": "sessions.session", "fields": {"expire_date": "2013-04-23T10:53:27.345Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "73bcf23b5311b2da7b1464f238502287", "model": "sessions.session", "fields": {"expire_date": "2013-04-23T15:49:35.600Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "475c8cb5c9830cd4b34cade37012ce91", "model": "sessions.session", "fields": {"expire_date": "2013-04-23T15:49:36.153Z", "session_data": "Zjg1YWYyYmFlOWZkNjg1NjJiMTY3YzM2OWMyZjhjNmIzMDdlMWU3NjqAAn1xAS4=\n"}}, {"pk": "5d882a7d2309f1e70ea037a4982cd9d1", "model": "sessions.session", "fields": {"expire_date": "2013-04-23T16:01:42.474Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "837759f4cae6a50c2a2159b888afc62a", "model": "sessions.session", "fields": {"expire_date": "2013-04-23T16:53:34.433Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a710dd61270aa1805fdf31ac8b45c876", "model": "sessions.session", "fields": {"expire_date": "2013-04-23T17:46:57.468Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5bcc3529ef34243e7dd87caaed1effb6", "model": "sessions.session", "fields": {"expire_date": "2013-04-23T20:36:45.891Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2cdcdd4a439da2aa04f310d6dddeaf03", "model": "sessions.session", "fields": {"expire_date": "2013-04-23T20:36:46.076Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6a4ec3e192ee9099d70926faf21f5a86", "model": "sessions.session", "fields": {"expire_date": "2013-04-23T22:46:57.989Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "98e600cfd70372db1d34db2e9c80ac3f", "model": "sessions.session", "fields": {"expire_date": "2013-04-23T22:49:18.885Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3a7d1ce2efbdc290fac680738d8ac4d1", "model": "sessions.session", "fields": {"expire_date": "2013-04-23T23:50:44.619Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "23d82c8505a01003ce7dd06f284c0454", "model": "sessions.session", "fields": {"expire_date": "2013-04-24T04:17:34.093Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b54c0af2b4fc1b11bbc20d3de6c739cd", "model": "sessions.session", "fields": {"expire_date": "2013-04-24T16:54:03.126Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2a2bc2a2c7e357732221bc2437582dd8", "model": "sessions.session", "fields": {"expire_date": "2013-04-24T19:41:15.410Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9045dd31a27bc5a434368b39f0fd84c5", "model": "sessions.session", "fields": {"expire_date": "2013-04-25T05:02:17.237Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7d6c3353020aff6e2e944a01a801cbc5", "model": "sessions.session", "fields": {"expire_date": "2013-04-25T07:50:48.524Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "39f6a43d6af350a93f5a71e4f2d8880c", "model": "sessions.session", "fields": {"expire_date": "2013-04-25T17:08:17.842Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "50ab06a1fca25fe2dcbf4187ffa500cf", "model": "sessions.session", "fields": {"expire_date": "2013-04-25T18:58:38.545Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9d29630cce174116e18e097942b09bfe", "model": "sessions.session", "fields": {"expire_date": "2013-04-25T20:13:34.874Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "369e0c9ee42e8365537c054528ca13e0", "model": "sessions.session", "fields": {"expire_date": "2013-04-25T21:26:20.032Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4bcd430d3fd03da31dd9aff388fb3342", "model": "sessions.session", "fields": {"expire_date": "2013-04-25T21:26:20.877Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "34557ce65b0b267ecd050f0492dbfcc1", "model": "sessions.session", "fields": {"expire_date": "2013-04-26T12:22:13.015Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "15a68ef033cc4664754432808c94de54", "model": "sessions.session", "fields": {"expire_date": "2013-04-26T12:35:32.249Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d5f87a5727d131bc0d46d07f6a55012b", "model": "sessions.session", "fields": {"expire_date": "2013-04-26T19:56:33.260Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d424580b03f1cea7cd435334f13a25e7", "model": "sessions.session", "fields": {"expire_date": "2013-04-26T20:56:13.849Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "693f0e99a6fd1d2731e3ec8b29ee9904", "model": "sessions.session", "fields": {"expire_date": "2013-04-27T01:39:57.056Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9dd9f10bae9d9954080d012d73e9cbc5", "model": "sessions.session", "fields": {"expire_date": "2013-04-27T01:59:48.673Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3295b85df6286488db058dfa6d645929", "model": "sessions.session", "fields": {"expire_date": "2013-04-27T11:29:54.749Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "20cf0349ce4ad70f73e1a1db15122893", "model": "sessions.session", "fields": {"expire_date": "2013-04-27T12:40:43.800Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6b60338bf68fd57e6b8ab615f4556665", "model": "sessions.session", "fields": {"expire_date": "2013-04-27T16:50:35.355Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "394e6b60d75820120f7fb12c20c661cd", "model": "sessions.session", "fields": {"expire_date": "2013-04-27T20:04:42.157Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bb5ddf26a33374368df7324cf46f137c", "model": "sessions.session", "fields": {"expire_date": "2013-04-27T20:06:24.055Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "127b250e3ac7d6cd42803786d4d0092a", "model": "sessions.session", "fields": {"expire_date": "2013-04-28T12:25:20.201Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e8613d509cbcfaa8d5ce21ed460d9a31", "model": "sessions.session", "fields": {"expire_date": "2013-04-28T17:46:38.477Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9ef1e97df6a2cb3451810e019a83d5a6", "model": "sessions.session", "fields": {"expire_date": "2013-04-28T21:49:21.501Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "87b5178f9e998f512fa9d5109c5aacd6", "model": "sessions.session", "fields": {"expire_date": "2013-04-29T01:45:07.042Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2a3d7326f2369c3122738d45438552e3", "model": "sessions.session", "fields": {"expire_date": "2013-04-29T06:19:39.157Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d8e05e2692dea0e44d6e686e78034f9c", "model": "sessions.session", "fields": {"expire_date": "2013-04-29T09:18:05.875Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "384beb58477d37fe1e4a5b6516ea0e94", "model": "sessions.session", "fields": {"expire_date": "2013-04-29T17:09:58.340Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c94c96731381dd61e8856c9aeccd42ca", "model": "sessions.session", "fields": {"expire_date": "2013-04-30T16:55:42.569Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "42261801db697271bf1fabe65c116c7c", "model": "sessions.session", "fields": {"expire_date": "2013-04-30T21:58:07.483Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1c102a51b23055e24ad2a69906d0cdcd", "model": "sessions.session", "fields": {"expire_date": "2013-04-30T22:02:35.596Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d13a6fdaa9bd9c334c5d0fc7c63fc57e", "model": "sessions.session", "fields": {"expire_date": "2013-05-01T00:57:35.799Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e0c2676c2dfe9c57c31433b82e182889", "model": "sessions.session", "fields": {"expire_date": "2013-05-01T08:06:39.528Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c2d0585d6a22d8fada7f053854f3adae", "model": "sessions.session", "fields": {"expire_date": "2013-05-01T13:31:43.198Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d7f9375ad2069218b7d0bc042f8284cb", "model": "sessions.session", "fields": {"expire_date": "2013-05-01T14:34:15.716Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1f69c55517d500d0a74285d9026836e3", "model": "sessions.session", "fields": {"expire_date": "2013-05-01T19:23:10.528Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "dfe05533d71fc0d0f4f669bd51a127bd", "model": "sessions.session", "fields": {"expire_date": "2013-05-02T04:55:03.036Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "513283ead107fc3cf536bf28f9f36194", "model": "sessions.session", "fields": {"expire_date": "2013-05-02T08:30:11.344Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "076df271e0bb969cb5326a89e84c99d5", "model": "sessions.session", "fields": {"expire_date": "2013-05-02T09:36:38.481Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3d68fb62e5dc44d2495d7abd7a2a6898", "model": "sessions.session", "fields": {"expire_date": "2013-05-02T17:57:53.724Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7a70c9a4b00e59f718b7bdff396d8a4c", "model": "sessions.session", "fields": {"expire_date": "2013-05-02T19:42:10.381Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0c79fa4d786b24a1e94536df3625954c", "model": "sessions.session", "fields": {"expire_date": "2013-05-03T10:50:29.984Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0ef084088f29a1249f49a88762f5264f", "model": "sessions.session", "fields": {"expire_date": "2013-05-03T10:51:45.133Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a456ac46740e9df9746f56b67d7c2ffe", "model": "sessions.session", "fields": {"expire_date": "2013-05-03T11:16:16.693Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a599f8fca80fba30668e6ecbc7527961", "model": "sessions.session", "fields": {"expire_date": "2013-05-03T11:35:39.069Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "9ec5652dc9895bf19fb2927d6c8cca06", "model": "sessions.session", "fields": {"expire_date": "2013-05-03T13:15:43.738Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "64ef2a3266b1c854c39e154806120432", "model": "sessions.session", "fields": {"expire_date": "2013-05-03T13:55:46.543Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "12a29ac8da81e1a35bde97492b419d1c", "model": "sessions.session", "fields": {"expire_date": "2013-05-04T02:00:42.142Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "524d0d0f63c3d18990c9fa5db41a83d0", "model": "sessions.session", "fields": {"expire_date": "2013-05-04T02:39:02.268Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bd5e824fff9b7bc71b4e360a6d520e86", "model": "sessions.session", "fields": {"expire_date": "2013-05-04T06:51:35.801Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e916542a9bc81c8d32d116f369e4d5f6", "model": "sessions.session", "fields": {"expire_date": "2013-05-04T14:26:22.305Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d4d83629e40e98d7db1d75ca9f4f51a5", "model": "sessions.session", "fields": {"expire_date": "2013-05-04T18:53:43.347Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fb2a673257e26de0a5e374e1bba63b7e", "model": "sessions.session", "fields": {"expire_date": "2013-05-04T19:16:17.461Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7a21c224b7bf6c58320367705b18d47a", "model": "sessions.session", "fields": {"expire_date": "2013-05-04T22:36:00.484Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "70eba5158adc21c70258ac604126ff6f", "model": "sessions.session", "fields": {"expire_date": "2013-05-05T01:30:32.687Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f8eab83e22bddb217ef54a6c69411644", "model": "sessions.session", "fields": {"expire_date": "2013-05-05T13:40:19.038Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3343759249358b4a4d7baa8f2da623ca", "model": "sessions.session", "fields": {"expire_date": "2013-05-05T16:52:01.422Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "27f633a51070092acf6d605d1932f62c", "model": "sessions.session", "fields": {"expire_date": "2013-05-05T19:34:57.323Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "49aa6b1319a9d6003d5835cca04d0ccd", "model": "sessions.session", "fields": {"expire_date": "2013-05-06T03:43:59.748Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2e923682bb089f917e65fd6c18a4566c", "model": "sessions.session", "fields": {"expire_date": "2013-05-06T05:22:15.881Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "41bd7319f5887243f52550009290092c", "model": "sessions.session", "fields": {"expire_date": "2013-05-06T06:26:17.165Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "853cecc37ff6378922e420fea5bae4b8", "model": "sessions.session", "fields": {"expire_date": "2013-05-06T14:15:57.415Z", "session_data": "MTM3ZGEzZTljYWM2MTEzNTE4OTU1MWI4YjAxYTI3MTczY2MyNTA5YzqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLCXUu\n"}}, {"pk": "35d579cc904cd1ae0b6232b9be91b79c", "model": "sessions.session", "fields": {"expire_date": "2013-05-06T16:22:20.391Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "df1873ba9fbe024c0854431548a24163", "model": "sessions.session", "fields": {"expire_date": "2013-05-06T16:26:52.365Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "836e3d4d8fc8a41c925c8f3e1d5c52b9", "model": "sessions.session", "fields": {"expire_date": "2013-05-06T19:13:26.406Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9b83e68ce4f9fe1fe850a05b9a892ff4", "model": "sessions.session", "fields": {"expire_date": "2013-05-07T03:37:26.005Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a27097cd875eb4f7f26d58df6e5439e3", "model": "sessions.session", "fields": {"expire_date": "2013-05-07T05:09:10.891Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cbe31792d50ff817b96362c726456bc0", "model": "sessions.session", "fields": {"expire_date": "2013-05-07T05:09:11.485Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7a3ffedd7ac1a70a6e19f1bb83eab4fd", "model": "sessions.session", "fields": {"expire_date": "2013-05-07T06:31:17.559Z", "session_data": "NzkyMDMxNTgzOWJiZmI4ZjcxMzc0MWE2OWE1MDhmZWU5NjUwY2Q2YjqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsMdS4=\n"}}, {"pk": "1adb06efc2da610a075d587e40f971fd", "model": "sessions.session", "fields": {"expire_date": "2013-05-07T09:14:32.145Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "67fa291b94343b6048bfc59ff62cc12e", "model": "sessions.session", "fields": {"expire_date": "2013-05-07T15:13:15.871Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "00590f7eb85051405de8bf7c5ef319bf", "model": "sessions.session", "fields": {"expire_date": "2013-05-08T02:29:41.495Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "13855fc8df78834e5882b96ee91a857e", "model": "sessions.session", "fields": {"expire_date": "2013-05-08T03:12:40.248Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "52a07119f33a086926cb086445f2bec3", "model": "sessions.session", "fields": {"expire_date": "2013-05-08T14:16:58.416Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b7ed364743403befbb9f3fccce95c204", "model": "sessions.session", "fields": {"expire_date": "2013-05-08T18:42:17.549Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e03ae57354cf90879f3a0579bb4ecd47", "model": "sessions.session", "fields": {"expire_date": "2013-05-08T18:58:05.664Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0e3a9bd5caf5db3ad2de7203660389c7", "model": "sessions.session", "fields": {"expire_date": "2013-05-08T22:34:34.370Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "beb00c74354766b8e10074f219437088", "model": "sessions.session", "fields": {"expire_date": "2013-05-09T02:38:20.796Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1f79fcb160dbbc1222e30780fa7c315c", "model": "sessions.session", "fields": {"expire_date": "2013-05-09T08:51:07.586Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e31c647d9bbf8d173ab55648b472a36e", "model": "sessions.session", "fields": {"expire_date": "2013-05-09T14:17:38.065Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1a7258c78a65c3aa8605e64224bf1f78", "model": "sessions.session", "fields": {"expire_date": "2013-05-09T19:21:28.556Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "145ca7c28bdb52b78f1678f494abd385", "model": "sessions.session", "fields": {"expire_date": "2013-05-09T23:58:07.377Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "73f99362d4e55cc75af7883eba42725c", "model": "sessions.session", "fields": {"expire_date": "2013-05-10T01:54:22.635Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "be5cf57ae8651729234af3f7007a9380", "model": "sessions.session", "fields": {"expire_date": "2013-05-10T14:35:50.551Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "464c3d32ef6bac5eae18d4cd18c9fcc4", "model": "sessions.session", "fields": {"expire_date": "2013-05-10T15:49:26.921Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4e4d69dcaa687d9972de4d583b190e4a", "model": "sessions.session", "fields": {"expire_date": "2013-05-10T19:58:45.857Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "83d97b3774b21230f14507600125eeb6", "model": "sessions.session", "fields": {"expire_date": "2013-05-10T20:08:11.442Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ab37aa39871150af36ec4704e14b57b2", "model": "sessions.session", "fields": {"expire_date": "2013-05-11T00:54:29.255Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ebae9b6cca60298da91d16b64ed8e9dc", "model": "sessions.session", "fields": {"expire_date": "2013-05-11T06:25:35.131Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fe37f7fad38f5c443497f7e04851f5d6", "model": "sessions.session", "fields": {"expire_date": "2013-05-11T06:28:11.972Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a40989e84ec06d05b50beaeae271ebf7", "model": "sessions.session", "fields": {"expire_date": "2013-05-11T08:09:17.119Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dc52178051788c51834bdd929e8562d4", "model": "sessions.session", "fields": {"expire_date": "2013-05-11T12:20:05.492Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "928811c7a938d51b50eace54d6d75028", "model": "sessions.session", "fields": {"expire_date": "2013-05-11T13:29:57.818Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "69abc5fbd0137a767700ac5c008523ad", "model": "sessions.session", "fields": {"expire_date": "2013-05-11T13:33:13.177Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "03667304ef842a0f89d9e89cb84b295f", "model": "sessions.session", "fields": {"expire_date": "2013-05-11T13:33:36.867Z", "session_data": "OTA4MDE4Njk0OWQwYTg3YzY4MmM3ODFhMTcwNjMwMjE2NTk5YWNhMjqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsNdS4=\n"}}, {"pk": "dc8be13cdcb42aaf6aab3105cc5b11ef", "model": "sessions.session", "fields": {"expire_date": "2013-05-11T19:12:42.407Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "724f60ddee6a6d5b24e7837e65e8ce2e", "model": "sessions.session", "fields": {"expire_date": "2013-05-11T23:25:05.856Z", "session_data": "OGY2ODNmZTdkOGFlOWQ2YWNiOTQ0NWMxZTc1MmFiNjc1NDJhOTUzNjqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "3f02d057ed7e4fdd3d17731f019311f5", "model": "sessions.session", "fields": {"expire_date": "2013-05-12T07:05:57.294Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6295069caf7d4599a147607dcf857bff", "model": "sessions.session", "fields": {"expire_date": "2013-05-12T13:17:25.592Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "60f8f6839e00ee161d81530c51c7ecef", "model": "sessions.session", "fields": {"expire_date": "2013-05-12T19:45:54.414Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6d48003667733d06cfc515b9503b7d20", "model": "sessions.session", "fields": {"expire_date": "2013-05-13T08:20:45.569Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "45566fe2aa430aecd0ab967c35ccf442", "model": "sessions.session", "fields": {"expire_date": "2013-05-13T13:16:20.598Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e04d864bf38ce4134d3d707bfd995835", "model": "sessions.session", "fields": {"expire_date": "2013-05-13T20:09:04.092Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c237cdb354b1f68e060e36a575c376db", "model": "sessions.session", "fields": {"expire_date": "2013-05-14T01:15:31.153Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "dc2294b46288165d4debf43217b83220", "model": "sessions.session", "fields": {"expire_date": "2013-05-14T07:36:12.691Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3c2367db32ea0a344cee640af358d6bc", "model": "sessions.session", "fields": {"expire_date": "2013-05-14T19:14:59.985Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c016d155d8896baae57600342171a974", "model": "sessions.session", "fields": {"expire_date": "2013-05-14T20:24:52.301Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8aad3afbb2f6c73133aecb934c99fb5b", "model": "sessions.session", "fields": {"expire_date": "2013-05-14T20:26:52.468Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fce2e1f4fd28c6655e64117f7139c5d4", "model": "sessions.session", "fields": {"expire_date": "2013-05-15T02:50:32.872Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8f0929be07da461a1f448b6909d41f3b", "model": "sessions.session", "fields": {"expire_date": "2013-05-15T14:56:27.134Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2522b00cbeb9bae158db0f2439570491", "model": "sessions.session", "fields": {"expire_date": "2013-05-15T18:57:05.418Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3a6ff52aadf834368c36e9cb07778f85", "model": "sessions.session", "fields": {"expire_date": "2013-05-16T03:20:11.021Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2dcbec827e659ca31705136d050726f7", "model": "sessions.session", "fields": {"expire_date": "2013-05-16T07:31:18.820Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5ce8c7e2ee49ab76aa23af994037dff3", "model": "sessions.session", "fields": {"expire_date": "2013-05-16T15:27:18.801Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "50ec7557c42d0efd28d0be62dd1d4c9e", "model": "sessions.session", "fields": {"expire_date": "2013-05-16T17:08:29.561Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e8028c0b089dafe51be325d32eaa2097", "model": "sessions.session", "fields": {"expire_date": "2013-05-16T18:15:40.491Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ff1fcac2606efddaff87e1033e64a3e4", "model": "sessions.session", "fields": {"expire_date": "2013-05-16T20:14:43.647Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "990ef7fe4f0536996a89787b74513c9e", "model": "sessions.session", "fields": {"expire_date": "2013-05-17T08:39:24.278Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "38d90530919f1af8c72ca078d6e58d2a", "model": "sessions.session", "fields": {"expire_date": "2013-05-17T08:39:24.885Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6bce71637e8671d3a2b4757acaa9cb7d", "model": "sessions.session", "fields": {"expire_date": "2013-05-17T10:16:44.202Z", "session_data": "OGY2ODNmZTdkOGFlOWQ2YWNiOTQ0NWMxZTc1MmFiNjc1NDJhOTUzNjqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "d0640cd6e30f3ac5e02f0251b05b9bea", "model": "sessions.session", "fields": {"expire_date": "2013-05-17T10:29:23.427Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9ba3a7d2213624acef7049f3b33bb8cb", "model": "sessions.session", "fields": {"expire_date": "2013-05-17T13:47:53.838Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7467d29fb06dd8900a7f6339f443ba90", "model": "sessions.session", "fields": {"expire_date": "2013-05-17T16:11:29.736Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a5691a343a6a8d581771968486bfa089", "model": "sessions.session", "fields": {"expire_date": "2013-05-18T04:28:49.037Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "845788030f0db6b4606c93c6d93297c9", "model": "sessions.session", "fields": {"expire_date": "2013-05-18T16:11:46.523Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ea9ec6e4de6adb7b4ff568f1ebcc43de", "model": "sessions.session", "fields": {"expire_date": "2013-05-18T18:07:22.831Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0a0833ddca7ecf77525959d390b0f281", "model": "sessions.session", "fields": {"expire_date": "2013-05-19T04:25:02.321Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b2519bdf229c174670e7748ca6d209d4", "model": "sessions.session", "fields": {"expire_date": "2013-05-19T04:34:45.661Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "1ef126e8e311a78bd2c704aef2489ee1", "model": "sessions.session", "fields": {"expire_date": "2013-05-19T15:50:15.143Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "7b389edf0e3a30d75f77de3aa7a66b5a", "model": "sessions.session", "fields": {"expire_date": "2013-05-20T04:36:22.764Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d5781c34a8aa78d552896f48d30cb5df", "model": "sessions.session", "fields": {"expire_date": "2013-05-20T16:51:37.929Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b2ce8fdd4da204e6701b07ce71b8e01d", "model": "sessions.session", "fields": {"expire_date": "2013-05-20T21:25:39.062Z", "session_data": "ZTJhMTI5ZGQxOGVmZWI2M2I4Y2NhNmE3MWQ1MTdkODUzMjIzYTA3NjqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsOdS4=\n"}}, {"pk": "0aa98af73e12498fbad14637b2e9dfa2", "model": "sessions.session", "fields": {"expire_date": "2013-05-21T03:45:46.459Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4281dc878edb078f71220889948b0a91", "model": "sessions.session", "fields": {"expire_date": "2013-05-21T04:52:15.678Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "dcf3ba7ff236cf7d25282c3c1753c7e0", "model": "sessions.session", "fields": {"expire_date": "2013-05-21T08:32:01.374Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b21d0fae3f9036def5161a51620b1c9e", "model": "sessions.session", "fields": {"expire_date": "2013-05-21T09:12:13.923Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b6b3fc20a89108bf3dc42e1e904c118a", "model": "sessions.session", "fields": {"expire_date": "2013-05-21T09:35:10.374Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6a3ad17383407667b5b7333e7c95cbf4", "model": "sessions.session", "fields": {"expire_date": "2013-05-21T10:23:33.851Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "93ce49a590fb362db1f91b9a37ff85e3", "model": "sessions.session", "fields": {"expire_date": "2013-05-21T10:33:54.035Z", "session_data": "MTM3ZGEzZTljYWM2MTEzNTE4OTU1MWI4YjAxYTI3MTczY2MyNTA5YzqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLCXUu\n"}}, {"pk": "2bfad05cdbc1420742f1e2506796c6b0", "model": "sessions.session", "fields": {"expire_date": "2013-05-21T12:51:46.982Z", "session_data": "OTM0NzdkYmQ5YjIzM2FkODhkNmJhZWI2MDEzYTM5OTVmZGIxMzU1ZjqAAn1xAShVCnRlc3Rjb29r\naWVxAlUGd29ya2VkcQNVDV9hdXRoX3VzZXJfaWRLCVUSX2F1dGhfdXNlcl9iYWNrZW5kVSlkamFu\nZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHUu\n"}}, {"pk": "68216a1b33ac5f9ae7efc62ee3fdc47b", "model": "sessions.session", "fields": {"expire_date": "2013-05-21T12:52:01.330Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ed344dd559a18a691f3828c938e0e606", "model": "sessions.session", "fields": {"expire_date": "2013-05-21T17:06:21.932Z", "session_data": "MDcyNTM3MzcxZDUxZjUxMzc2MGVkOWI3YzkzNDcwNzEzYzlhMjZjNDqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLAnUu\n"}}, {"pk": "2888e5957cf78c09d9d3c912106a22e8", "model": "sessions.session", "fields": {"expire_date": "2013-05-21T17:32:28.458Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d44f9e9dbe9c8ef97b27412dc81b7236", "model": "sessions.session", "fields": {"expire_date": "2013-05-21T18:40:21.061Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c53053ce21fc775a381670ca84ead204", "model": "sessions.session", "fields": {"expire_date": "2013-05-22T03:49:55.598Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "193fea473bae5c19b4443b971f208bfa", "model": "sessions.session", "fields": {"expire_date": "2013-05-22T07:19:13.295Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6b0970c560dc00179cae3afe89caa03b", "model": "sessions.session", "fields": {"expire_date": "2013-05-22T07:58:23.176Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ab5a476e0d6dc5eec1710f5bdbbf5bff", "model": "sessions.session", "fields": {"expire_date": "2013-05-22T10:56:36.750Z", "session_data": "YzQ5NDdhZTdmZTk1MTdhZmEyNGRlZjcxYjAyOTI4ZjE3M2RkMWJjMzqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsPdS4=\n"}}, {"pk": "804f9196bcf76634182f09e314a179f4", "model": "sessions.session", "fields": {"expire_date": "2013-05-22T14:31:20.336Z", "session_data": "OGY2ODNmZTdkOGFlOWQ2YWNiOTQ0NWMxZTc1MmFiNjc1NDJhOTUzNjqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "a1db7506ee0cfd4b741daea2d16a1136", "model": "sessions.session", "fields": {"expire_date": "2013-05-22T14:43:25.280Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "cb0f854bcf15065758752ea58d395870", "model": "sessions.session", "fields": {"expire_date": "2013-05-22T14:45:00.281Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0dc137b5fd1cf2b8f7a13f2e9e688b76", "model": "sessions.session", "fields": {"expire_date": "2013-05-22T15:40:55.882Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "14777887c2df28e88fd5cb5833fa116c", "model": "sessions.session", "fields": {"expire_date": "2013-05-22T18:00:13.696Z", "session_data": "MTg2YzYyZjIxMmE2NjJiNzAwN2U0ZDU1YjE5Y2I2MzlkZTNiOTA0MTqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLBXUu\n"}}, {"pk": "235a8ec20c4b3d17e7b40239c5d22ef2", "model": "sessions.session", "fields": {"expire_date": "2013-05-23T01:25:33.783Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e44de6d97d5324207dcfd77536c53849", "model": "sessions.session", "fields": {"expire_date": "2013-05-23T02:58:21.790Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9b2865a82e55d74c29a1481446ba72c7", "model": "sessions.session", "fields": {"expire_date": "2013-05-23T04:19:24.379Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f93cdb1ecfcc207918aa4f470d06ed18", "model": "sessions.session", "fields": {"expire_date": "2013-05-23T09:19:44.215Z", "session_data": "NWIyYjZmOWFjMWQ3MjE5YWQ4ZGM4ZTM2MjZmOTdjOTg1MTM0ZmRhNDqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsQdS4=\n"}}, {"pk": "cca526f7a1284ecb22a2afecdfba5550", "model": "sessions.session", "fields": {"expire_date": "2013-05-23T15:13:04.374Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6b5ee7e08e41505a5fe48c7ae13d53d3", "model": "sessions.session", "fields": {"expire_date": "2013-05-23T16:01:31.564Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5be73c62151136ebd260fe2792f5afa9", "model": "sessions.session", "fields": {"expire_date": "2013-05-23T16:54:23.090Z", "session_data": "YjJmOWNmN2ZjMWVjMzI1N2Q3YzFmZjc4YjJkNTFiYzllMGI5MjQzOTqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLBHUu\n"}}, {"pk": "d0ac260cf5b7fed9f32cca3d632261a9", "model": "sessions.session", "fields": {"expire_date": "2013-05-23T19:58:25.709Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d5f0bfd6c6a3c558be53623e056255a1", "model": "sessions.session", "fields": {"expire_date": "2013-05-24T01:17:17.001Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a9da6c0b0b9351a61271aa6c805c830b", "model": "sessions.session", "fields": {"expire_date": "2013-05-24T01:19:27.164Z", "session_data": "MGY0NzY4OTJiNDI2ZGM1MzVjZjk4MzA3OGNmMWQ2YzY4ZTQ4NjhhMTqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZHECVRJfYXV0aF91c2VyX2JhY2tlbmRVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kVQ1fYXV0aF91c2VyX2lkSxF1Lg==\n"}}, {"pk": "e15d88e43e4e0a9075ee25f98a6b3544", "model": "sessions.session", "fields": {"expire_date": "2013-05-24T05:13:04.528Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "495042a05ecc0918f4528e507b1e8453", "model": "sessions.session", "fields": {"expire_date": "2013-05-24T06:45:41.432Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "bab2321a9e7b20efcfeb5ac72083f194", "model": "sessions.session", "fields": {"expire_date": "2013-05-24T07:32:36.779Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d9c9d8ce8952c36ff43dc5329f7a2736", "model": "sessions.session", "fields": {"expire_date": "2013-05-24T10:55:34.482Z", "session_data": "NDMzYzI1ODlhZmQwYWI2N2NlZDYwMjQ5ODQwYTliMzkzMDViYjZkYjqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsSdS4=\n"}}, {"pk": "b0b9a5622e4c9e1b5d255d7161716d15", "model": "sessions.session", "fields": {"expire_date": "2013-05-24T11:44:27.012Z", "session_data": "MTM3ZGEzZTljYWM2MTEzNTE4OTU1MWI4YjAxYTI3MTczY2MyNTA5YzqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLCXUu\n"}}, {"pk": "a74b8d8c6a7fc32aebb32bdfac822d1d", "model": "sessions.session", "fields": {"expire_date": "2013-05-24T15:20:59.945Z", "session_data": "MTM3ZGEzZTljYWM2MTEzNTE4OTU1MWI4YjAxYTI3MTczY2MyNTA5YzqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLCXUu\n"}}, {"pk": "b469d44227e8c43aa3e48f196e55e18b", "model": "sessions.session", "fields": {"expire_date": "2013-05-24T16:49:28.985Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "5c4b29bcd4483320fd8c5bd86f52da85", "model": "sessions.session", "fields": {"expire_date": "2013-05-24T16:56:44.168Z", "session_data": "NWRiNzU2ZDQ2ZGZkNWRiMjJjMjMzNThhNmYyMzZiNzBiMGE3Y2EwNzqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsTdS4=\n"}}, {"pk": "802839ebef191b84d296929d3e10fc6d", "model": "sessions.session", "fields": {"expire_date": "2013-05-24T19:03:17.756Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9ae838fb7265ca1910b794fd4fd734f9", "model": "sessions.session", "fields": {"expire_date": "2013-05-24T19:58:33.956Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a0e9ca81ad444c16eee899ed298a0729", "model": "sessions.session", "fields": {"expire_date": "2013-05-24T20:22:00.928Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5675d27479414f50e2d398a9bf219301", "model": "sessions.session", "fields": {"expire_date": "2013-05-25T08:17:59.823Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "50f918064f61fb00c6591b4711116ff6", "model": "sessions.session", "fields": {"expire_date": "2013-05-25T09:55:00.059Z", "session_data": "OTEzNWZhMzc5ZWY4NDM5MDQzZDI3YTUwMjg3YTMwNGNjYjUyOTFiYTqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsUdS4=\n"}}, {"pk": "e428bfbccc640c5656f6179283067d0e", "model": "sessions.session", "fields": {"expire_date": "2013-05-25T11:34:02.339Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f134a0a33b36f12c486e52115b81ce81", "model": "sessions.session", "fields": {"expire_date": "2013-05-25T12:09:36.452Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c006b9790348f00843a08b9d82730a22", "model": "sessions.session", "fields": {"expire_date": "2013-05-25T13:53:21.201Z", "session_data": "OWEwYzZkZGFhNTliZTA1ZTEzZjNmNmZhYzE4YTYyYjQ0MzljZWJlZTqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsVdS4=\n"}}, {"pk": "e060ff7dd60d14e4ba1b69c5fb8f97bc", "model": "sessions.session", "fields": {"expire_date": "2013-05-25T14:59:03.370Z", "session_data": "OGY2ODNmZTdkOGFlOWQ2YWNiOTQ0NWMxZTc1MmFiNjc1NDJhOTUzNjqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "7d03507d3166df84cc5b736de27c8115", "model": "sessions.session", "fields": {"expire_date": "2013-05-25T18:52:56.425Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c383e495c2d17715f21d64e96c04162c", "model": "sessions.session", "fields": {"expire_date": "2013-05-25T20:32:26.274Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "c375b6be871a8dd7f008c789732f5c25", "model": "sessions.session", "fields": {"expire_date": "2013-05-25T20:51:30.395Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "97e8ef475efd5f34162848df4c395fe2", "model": "sessions.session", "fields": {"expire_date": "2013-05-26T05:09:34.914Z", "session_data": "ZTg5MmIzMzBmNWFlNzg5ZDk0NTMzZTg2N2VkNGE3ZWRhOGRhZmVjNjqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLFHUu\n"}}, {"pk": "acef9c9a5e9aff575023927495e76bcc", "model": "sessions.session", "fields": {"expire_date": "2013-05-26T05:14:50.192Z", "session_data": "ZTg5MmIzMzBmNWFlNzg5ZDk0NTMzZTg2N2VkNGE3ZWRhOGRhZmVjNjqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLFHUu\n"}}, {"pk": "64045969b7225d396f2e8fa463ab3213", "model": "sessions.session", "fields": {"expire_date": "2013-05-26T06:57:29.377Z", "session_data": "OGY2ODNmZTdkOGFlOWQ2YWNiOTQ0NWMxZTc1MmFiNjc1NDJhOTUzNjqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLA3Uu\n"}}, {"pk": "cc6654a692654fa1afdf945e53e90302", "model": "sessions.session", "fields": {"expire_date": "2013-05-26T13:51:53.860Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0ea7e617e4fd999ea25063e6bec88f4f", "model": "sessions.session", "fields": {"expire_date": "2013-05-26T13:57:49.870Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "6a1d1e68274591deb618dc48b2ed2cb1", "model": "sessions.session", "fields": {"expire_date": "2013-05-26T15:14:56.622Z", "session_data": "OTgzNjYyZDc4YjU5MzM4OTg4Y2ZjZDk3ZTgyNzc2ZDkyOTBmMWFjNDqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsWdS4=\n"}}, {"pk": "d6955516fc331f8ac5a60243aeff2991", "model": "sessions.session", "fields": {"expire_date": "2013-05-26T17:24:14.244Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4a60a7d37e48c9d50860b263c868f1e5", "model": "sessions.session", "fields": {"expire_date": "2013-05-26T19:32:10.865Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "f311a486c718d7f90e098fa589a92a85", "model": "sessions.session", "fields": {"expire_date": "2013-05-26T23:26:01.732Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "d13c19d986b901f4840fa0c71c112120", "model": "sessions.session", "fields": {"expire_date": "2013-05-26T23:26:02.359Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4e4176ef64dd6b0af0c9153a9be3434c", "model": "sessions.session", "fields": {"expire_date": "2013-05-27T04:08:13.307Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4472e2fea145babe2a748b117f38bdb9", "model": "sessions.session", "fields": {"expire_date": "2013-05-27T08:47:53.236Z", "session_data": "ZjFmZDQ0MDhmZTUzNjgwZDBkMGRiYjUxYTIzNDA1NWI5YThiM2Y2NzqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsXdS4=\n"}}, {"pk": "c3e3fe1108c7e2fe5c34c6170de10f3f", "model": "sessions.session", "fields": {"expire_date": "2013-05-27T09:15:32.691Z", "session_data": "YzI5ZjE3NzcyOTQzMjU2OTkzNTdiNTY3ZTRjNzk5ZWY5MzViNWIxNTqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsYdS4=\n"}}, {"pk": "635ca42aea1c33d389aec214cfe9c911", "model": "sessions.session", "fields": {"expire_date": "2013-05-27T10:48:37.347Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "ecf35afd4a2a1228c710e57894050ce4", "model": "sessions.session", "fields": {"expire_date": "2013-05-27T11:19:07.887Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "797c3d078b9583a9152e1eb2170b1157", "model": "sessions.session", "fields": {"expire_date": "2013-05-27T11:47:41.635Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "b2ea4a9c6b256a52b035d35cb3dc9f88", "model": "sessions.session", "fields": {"expire_date": "2013-05-27T13:03:16.151Z", "session_data": "YTljZWM4YjYzMjU3N2ZkNjI5YmQ2NDM3MjY4NWM2N2ZkNjEwYTZmOTqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsZdS4=\n"}}, {"pk": "9187f8902f0673c25ba2331e774e4ddf", "model": "sessions.session", "fields": {"expire_date": "2013-05-27T14:02:13.176Z", "session_data": "OTM0NzdkYmQ5YjIzM2FkODhkNmJhZWI2MDEzYTM5OTVmZGIxMzU1ZjqAAn1xAShVCnRlc3Rjb29r\naWVxAlUGd29ya2VkcQNVDV9hdXRoX3VzZXJfaWRLCVUSX2F1dGhfdXNlcl9iYWNrZW5kVSlkamFu\nZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHUu\n"}}, {"pk": "9d60d00fbd0caed9179a187636db58c5", "model": "sessions.session", "fields": {"expire_date": "2013-05-27T14:44:46.650Z", "session_data": "NzZhYzk3YTAwNTFjMmNiYzkwYzViMWFjZDY3YzRlYzJmZTE4NzI3OTqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLB3Uu\n"}}, {"pk": "308038a3e904cc7964ba0c75a307a732", "model": "sessions.session", "fields": {"expire_date": "2013-05-27T17:12:01.125Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5c3ab266f6316a655a6b6607b296e1d5", "model": "sessions.session", "fields": {"expire_date": "2013-05-27T17:12:02.570Z", "session_data": "Zjg1YWYyYmFlOWZkNjg1NjJiMTY3YzM2OWMyZjhjNmIzMDdlMWU3NjqAAn1xAS4=\n"}}, {"pk": "7c0da487cd8825b852aa444c64753f04", "model": "sessions.session", "fields": {"expire_date": "2013-05-27T19:58:51.494Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e840773f7553f2b39613c1780b0924c3", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T07:14:49.862Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "fc320beb57324aed32029bfa9f8da76b", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T07:16:44.370Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0b0bd20888fc0991915277d44b7b4723", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T08:32:23.157Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "45e7b90ecb69a1936cc3497c69c385ec", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T10:26:52.248Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "3d9acbc959eddb4f5745a283e5ef04f0", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T11:12:23.825Z", "session_data": "MzA1ZWQ0MzBkNWFjM2I4YzBmYjNmNTQzYzc0NDcxZGVjMmE2ZTUwNzqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLF3Uu\n"}}, {"pk": "d99ee26d4a18ff814805dbc513fbd5d7", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T12:24:30.587Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5766645e7ed437a6027f922c1c08dfea", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T16:11:32.875Z", "session_data": "YjdmMDgyYTQ0OTljNzZkZjNmMzYxNGVjMzA2NzIxMzE1ZGUyNWFhNDqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsadS4=\n"}}, {"pk": "2e32ea4f7239e0fc853ce226eef461b0", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T17:17:10.544Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "e13315eaf776af70b2f6ba2c5cf6d655", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T17:17:11.281Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "eef2f728e06dab1e030424ede69594a6", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T19:33:06.855Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "afe7080a255c3c21ca15e830f1834740", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T21:29:27.976Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "21e7800a3215a9b1e04e4b6e6dcf4b2c", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T21:29:34.908Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "4cfdcb2baf04df7b7b184fdddb1ae6e1", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T21:30:47.353Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "a3f737bf23ee7d1f93a86a1e69c0ff62", "model": "sessions.session", "fields": {"expire_date": "2013-05-28T23:34:34.364Z", "session_data": "NDg3NTNkMWY3ZTk3NWRjMWQ4Yjc4YTRmMTc2MzdhOGJkYjQ0YjJiMjqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZHECVRJfYXV0aF91c2VyX2JhY2tlbmRVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kVQ1fYXV0aF91c2VyX2lkSwJ1Lg==\n"}}, {"pk": "ef7ba72daebf7e3295fe1ed3162072b5", "model": "sessions.session", "fields": {"expire_date": "2013-05-29T00:56:59.571Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0298953e49e94430f3d300518025b4c6", "model": "sessions.session", "fields": {"expire_date": "2013-05-29T10:39:54.050Z", "session_data": "YjJmOWNmN2ZjMWVjMzI1N2Q3YzFmZjc4YjJkNTFiYzllMGI5MjQzOTqAAn1xAShVEl9hdXRoX3Vz\nZXJfYmFja2VuZHECVSlkamFuZ28uY29udHJpYi5hdXRoLmJhY2tlbmRzLk1vZGVsQmFja2VuZHED\nVQ1fYXV0aF91c2VyX2lkcQRLBHUu\n"}}, {"pk": "60290869dc8c8e5b2ca84b322264bb1f", "model": "sessions.session", "fields": {"expire_date": "2013-05-29T11:23:02.353Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "9b2e6f7892a1343ce31da21de1cb99b7", "model": "sessions.session", "fields": {"expire_date": "2013-05-29T14:20:10.535Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "8e0d48c08c5829a5bee556f031ea27ff", "model": "sessions.session", "fields": {"expire_date": "2013-05-29T17:36:24.003Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": "0ecc7d71e1c2056be3ea7371e6dc385c", "model": "sessions.session", "fields": {"expire_date": "2013-05-29T20:23:45.172Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "0267d982e609a54fef560eab9ad9c918", "model": "sessions.session", "fields": {"expire_date": "2013-05-29T20:41:00.714Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5c2a8e56ab494df57dfb4f2b6e05b2cd", "model": "sessions.session", "fields": {"expire_date": "2013-05-29T20:41:01.644Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "2f22f0549b7f8b24ca8d8f7c684d14f2", "model": "sessions.session", "fields": {"expire_date": "2013-05-29T22:12:56.992Z", "session_data": "Nzk0YWUwODc1YjM2NjNhMTZhNzFiMWY4YmUxMDc0YmYyNmNjZWNkYjqAAn1xAShVCnRlc3Rjb29r\naWVVBndvcmtlZFUSX2F1dGhfdXNlcl9iYWNrZW5kcQJVKWRqYW5nby5jb250cmliLmF1dGguYmFj\na2VuZHMuTW9kZWxCYWNrZW5kcQNVDV9hdXRoX3VzZXJfaWRxBEsbdS4=\n"}}, {"pk": "07dd18f6332becba1c93d3b93844b035", "model": "sessions.session", "fields": {"expire_date": "2013-05-29T23:57:26.437Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "901e99b16f343af2fe46142cc97a47a5", "model": "sessions.session", "fields": {"expire_date": "2013-05-30T10:29:59.264Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5cda302d2de40db7aea61e7d003cebb9", "model": "sessions.session", "fields": {"expire_date": "2013-05-30T10:29:59.887Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5fa4227d3aeba3529f6cc1669593368a", "model": "sessions.session", "fields": {"expire_date": "2013-05-30T12:23:43.429Z", "session_data": "OTM0Y2M4Mzg3NjllN2JjMzRhNTBiNTQ0NjhjZjNjOWIxZTMxZDMwYzqAAn1xAVUKdGVzdGNvb2tp\nZXECVQZ3b3JrZWRxA3Mu\n"}}, {"pk": "5cbe9a77c804ab7a2dc26be0eb16e77e", "model": "sessions.session", "fields": {"expire_date": "2013-05-30T13:45:47.306Z", "session_data": "MjkwNmJmMDBhODA2ODc0ZjM5OWViNjc3ZDhlYzE1MTg4YjYzNGRiMTqAAn1xAVUKdGVzdGNvb2tp\nZVUGd29ya2VkcQJzLg==\n"}}, {"pk": 1, "model": "sites.site", "fields": {"domain": "wikipendium.no", "name": "wikipendium.no"}}, {"pk": 1, "model": "south.migrationhistory", "fields": {"applied": "2013-02-03T03:33:55.211Z", "app_name": "wiki", "migration": "0001_initial"}}, {"pk": 2, "model": "south.migrationhistory", "fields": {"applied": "2013-03-06T12:10:38.695Z", "app_name": "wiki", "migration": "0002_auto__add_field_articlecontent_parent__add_field_articlecontent_child"}}, {"pk": 1, "model": "wiki.article", "fields": {"slug": "TDT4165"}}, {"pk": 2, "model": "wiki.article", "fields": {"slug": "TDT4186"}}, {"pk": 3, "model": "wiki.article", "fields": {"slug": "TTM4100"}}, {"pk": 4, "model": "wiki.article", "fields": {"slug": "TDT4145"}}, {"pk": 5, "model": "wiki.article", "fields": {"slug": "TDT4205"}}, {"pk": 6, "model": "wiki.article", "fields": {"slug": "TDT4110"}}, {"pk": 8, "model": "wiki.article", "fields": {"slug": "TDT4258"}}, {"pk": 9, "model": "wiki.article", "fields": {"slug": "TDT4171"}}, {"pk": 10, "model": "wiki.article", "fields": {"slug": "TDT4125"}}, {"pk": 12, "model": "wiki.article", "fields": {"slug": "TDT4140"}}, {"pk": 13, "model": "wiki.article", "fields": {"slug": "TI\u00d8"}}, {"pk": 14, "model": "wiki.article", "fields": {"slug": "TI\u00d84258"}}, {"pk": 19, "model": "auth.permission", "fields": {"codename": "add_logentry", "name": "Can add log entry", "content_type": 7}}, {"pk": 20, "model": "auth.permission", "fields": {"codename": "change_logentry", "name": "Can change log entry", "content_type": 7}}, {"pk": 21, "model": "auth.permission", "fields": {"codename": "delete_logentry", "name": "Can delete log entry", "content_type": 7}}, {"pk": 4, "model": "auth.permission", "fields": {"codename": "add_group", "name": "Can add group", "content_type": 2}}, {"pk": 5, "model": "auth.permission", "fields": {"codename": "change_group", "name": "Can change group", "content_type": 2}}, {"pk": 6, "model": "auth.permission", "fields": {"codename": "delete_group", "name": "Can delete group", "content_type": 2}}, {"pk": 1, "model": "auth.permission", "fields": {"codename": "add_permission", "name": "Can add permission", "content_type": 1}}, {"pk": 2, "model": "auth.permission", "fields": {"codename": "change_permission", "name": "Can change permission", "content_type": 1}}, {"pk": 3, "model": "auth.permission", "fields": {"codename": "delete_permission", "name": "Can delete permission", "content_type": 1}}, {"pk": 7, "model": "auth.permission", "fields": {"codename": "add_user", "name": "Can add user", "content_type": 3}}, {"pk": 8, "model": "auth.permission", "fields": {"codename": "change_user", "name": "Can change user", "content_type": 3}}, {"pk": 9, "model": "auth.permission", "fields": {"codename": "delete_user", "name": "Can delete user", "content_type": 3}}, {"pk": 10, "model": "auth.permission", "fields": {"codename": "add_contenttype", "name": "Can add content type", "content_type": 4}}, {"pk": 11, "model": "auth.permission", "fields": {"codename": "change_contenttype", "name": "Can change content type", "content_type": 4}}, {"pk": 12, "model": "auth.permission", "fields": {"codename": "delete_contenttype", "name": "Can delete content type", "content_type": 4}}, {"pk": 22, "model": "auth.permission", "fields": {"codename": "add_registrationprofile", "name": "Can add registration profile", "content_type": 8}}, {"pk": 23, "model": "auth.permission", "fields": {"codename": "change_registrationprofile", "name": "Can change registration profile", "content_type": 8}}, {"pk": 24, "model": "auth.permission", "fields": {"codename": "delete_registrationprofile", "name": "Can delete registration profile", "content_type": 8}}, {"pk": 13, "model": "auth.permission", "fields": {"codename": "add_session", "name": "Can add session", "content_type": 5}}, {"pk": 14, "model": "auth.permission", "fields": {"codename": "change_session", "name": "Can change session", "content_type": 5}}, {"pk": 15, "model": "auth.permission", "fields": {"codename": "delete_session", "name": "Can delete session", "content_type": 5}}, {"pk": 16, "model": "auth.permission", "fields": {"codename": "add_site", "name": "Can add site", "content_type": 6}}, {"pk": 17, "model": "auth.permission", "fields": {"codename": "change_site", "name": "Can change site", "content_type": 6}}, {"pk": 18, "model": "auth.permission", "fields": {"codename": "delete_site", "name": "Can delete site", "content_type": 6}}, {"pk": 25, "model": "auth.permission", "fields": {"codename": "add_migrationhistory", "name": "Can add migration history", "content_type": 9}}, {"pk": 26, "model": "auth.permission", "fields": {"codename": "change_migrationhistory", "name": "Can change migration history", "content_type": 9}}, {"pk": 27, "model": "auth.permission", "fields": {"codename": "delete_migrationhistory", "name": "Can delete migration history", "content_type": 9}}, {"pk": 28, "model": "auth.permission", "fields": {"codename": "add_article", "name": "Can add article", "content_type": 10}}, {"pk": 29, "model": "auth.permission", "fields": {"codename": "change_article", "name": "Can change article", "content_type": 10}}, {"pk": 30, "model": "auth.permission", "fields": {"codename": "delete_article", "name": "Can delete article", "content_type": 10}}, {"pk": 31, "model": "auth.permission", "fields": {"codename": "add_articlecontent", "name": "Can add article content", "content_type": 11}}, {"pk": 32, "model": "auth.permission", "fields": {"codename": "change_articlecontent", "name": "Can change article content", "content_type": 11}}, {"pk": 33, "model": "auth.permission", "fields": {"codename": "delete_articlecontent", "name": "Can delete article content", "content_type": 11}}, {"pk": 12, "model": "auth.user", "fields": {"username": "kradalby", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-04-23T06:31:17.366Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$9OKw7uflcn3x$UJsCfdlYICj454nxXg6hlcZK5IVi7VXbTkGIpueQxTs=", "email": "kradalby@kradalby.no", "date_joined": "2013-04-23T06:31:17.019Z"}}, {"pk": 6, "model": "auth.user", "fields": {"username": "iverjo", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-02-19T18:34:53.002Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$84SoqAZWlWUW$QHIMuXE/1rKK0ZzhBZORrPlALeCyYS7riP2eR4YUZZ0=", "email": "iver@jord.al", "date_joined": "2013-02-19T18:34:51.791Z"}}, {"pk": 8, "model": "auth.user", "fields": {"username": "niklasam", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-02-20T14:23:31.266Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$MctNWs1pN945$xXmYEANzIItrlFyoQXf8ydSMj+ZvyG1hKvlUyWpKs3Y=", "email": "niklasam@stud.ntnu.no", "date_joined": "2013-02-20T14:23:30.245Z"}}, {"pk": 13, "model": "auth.user", "fields": {"username": "Ose", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-04-27T13:33:36.819Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$DrPNFx2lsrkn$mMdj183daUs6FQreic9pHTOmT7jbmEaCbbqk+SCf+Ao=", "email": "mathiabo@stud.ntnu.no", "date_joined": "2013-04-27T13:33:34.409Z"}}, {"pk": 20, "model": "auth.user", "fields": {"username": "Raane", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-12T05:14:50.171Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$j9DHnpaRBZU6$xGRENjKCl4YCaQhn31xEWIGbFKQAmAfzZHNh/l9hnK0=", "email": "Raane.Holm@gmail.com", "date_joined": "2013-05-11T09:54:59.709Z"}}, {"pk": 3, "model": "auth.user", "fields": {"username": "sigveseb", "first_name": "", "last_name": "", "is_active": true, "is_superuser": true, "is_staff": true, "last_login": "2013-05-12T06:57:29.354Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$AOJux4YNMnrE$SubrTqLlmtgIWFEEP3Q6GTnu2b7hfdxtKGtZlqkHALY=", "email": "sigvefarstad@gmail.com", "date_joined": "2013-02-03T08:08:05Z"}}, {"pk": 2, "model": "auth.user", "fields": {"username": "stiaje", "first_name": "", "last_name": "", "is_active": true, "is_superuser": true, "is_staff": true, "last_login": "2013-05-12T12:41:05.282Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$8sJjoRfhrsDx$BG+5XT2/ra558UBIJmBWodL+qcV3JVuqaf7VmUCVnrg=", "email": "me@stianj.com", "date_joined": "2013-02-03T03:35:40Z"}}, {"pk": 14, "model": "auth.user", "fields": {"username": "perok", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-06T21:25:39.041Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$cj039pCwlL6g$MVUCrowz4z90rtKY7bV3wt4pTA2lVMqbMMTOccoEtVc=", "email": "perokane@gmail.com", "date_joined": "2013-05-06T21:25:38.132Z"}}, {"pk": 15, "model": "auth.user", "fields": {"username": "giraff", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-08T10:56:36.723Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$vAUOo4jNS2i0$3udJE60SB9+oEoB4a+X0iPuPLRwpJkQ3azwSLw7JslA=", "email": "giraff@hotmail.com", "date_joined": "2013-05-08T10:56:36.393Z"}}, {"pk": 22, "model": "auth.user", "fields": {"username": "iverjo2", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-12T15:14:56.588Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$cnhocJTcNsgh$kJqKSVeTnJEXlZj1Y8b7D0UG21Q9X8SLbJVO10y8EA0=", "email": "iver@jord.al", "date_joined": "2013-05-12T15:14:55.111Z"}}, {"pk": 5, "model": "auth.user", "fields": {"username": "cristea", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-08T18:00:13.673Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$oiUU5vrxS5O4$e82EltAA9b0I19X/umA/4ERyYE2O6QT1MgxUjA4lAog=", "email": "christoffer.tonnessen@gmail.com", "date_joined": "2013-02-19T14:39:15.707Z"}}, {"pk": 16, "model": "auth.user", "fields": {"username": "oystekre", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-09T09:19:42.151Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$N88NK9fjLr6e$Hxp4e2zWZUNi7pS2CHUoAQZLyQuHdoBSr+iHAJGrEEQ=", "email": "okrepp91@gmail.com", "date_joined": "2013-05-09T09:19:40.620Z"}}, {"pk": 1, "model": "auth.user", "fields": {"username": "admin", "first_name": "", "last_name": "", "is_active": false, "is_superuser": true, "is_staff": true, "last_login": "2013-03-06T12:22:42Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$OVZY9oVZl5PV$0rlDy0Kv/f6jUn7awmxDNLyomQTEEUac//jCgjDqY8Y=", "email": "admin@wikipendium.no", "date_joined": "2013-02-03T03:33:15Z"}}, {"pk": 17, "model": "auth.user", "fields": {"username": "runholm", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-10T00:54:49.043Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$nHcplJ8lXVn7$9VM78LNE18ITvFr1vSCCnO6im63XxacW/M3S4Qjol2k=", "email": "Raane.Holm@gmail.com", "date_joined": "2013-05-10T00:54:48.732Z"}}, {"pk": 10, "model": "auth.user", "fields": {"username": "jack", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-03-08T14:57:18.555Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$xkUR6CaYW8wc$i4K7PMjOfSVb9xMZ5+3l5oFq89kCWPa3nH9DSiyuCuw=", "email": "jack@b.de", "date_joined": "2013-03-08T14:57:18.025Z"}}, {"pk": 18, "model": "auth.user", "fields": {"username": "odd", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-10T10:55:34.441Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$MO4Ka3BrrQY0$9j7LueZ7SWw8ymvj0m21kGeo2XOhPqnlPeObP9wezhk=", "email": "trondrud@stud.ntnu.no", "date_joined": "2013-05-10T10:55:34.083Z"}}, {"pk": 24, "model": "auth.user", "fields": {"username": "Lionleaf", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-13T09:15:32.649Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$E3z1bdXywfR9$QF5flxVmUqJK+JL1NdHh6GAY2mKPj0cRDLZ4gcTfSPc=", "email": "andreas.l.selvik@gmail.com", "date_joined": "2013-05-13T09:15:31.970Z"}}, {"pk": 11, "model": "auth.user", "fields": {"username": "hei", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-03-12T17:21:59.508Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$pZ7DGjJr1syj$5tcGq2R6WXfNBznR178u0+M21qBWvOS7/Zo/9ysVnKA=", "email": "hei@hei.no", "date_joined": "2013-03-12T17:21:33.053Z"}}, {"pk": 9, "model": "auth.user", "fields": {"username": "emiltayl", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-13T12:19:16.107Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$I2TkMCV0kTep$MdJEQos5ULvghUDg2eYFbun49EpX/rIMD/wo90Usn9U=", "email": "emil@wodinaz.com", "date_joined": "2013-03-08T12:34:25.876Z"}}, {"pk": 25, "model": "auth.user", "fields": {"username": "hk", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-13T13:03:16.121Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$GDprIJKOfgGy$4jMgYSC36ls8+g7jtQFxdT4CxWhf34H1GtKrvZ+h858=", "email": "mattekongen@gmail.com", "date_joined": "2013-05-13T13:03:15.826Z"}}, {"pk": 19, "model": "auth.user", "fields": {"username": "relekang", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-10T16:56:44.092Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$PSw1ruto1C2t$Gb3UJQSxiesGhtfMareLLvv+8SDaKByhhwwz8mJOLIA=", "email": "me@rolflekang.com", "date_joined": "2013-05-10T16:56:43.724Z"}}, {"pk": 21, "model": "auth.user", "fields": {"username": "sindreij", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-11T13:53:21.169Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$bEuFPMUHFhin$bY7d9gQhaw0d0fXg6wMFoEBUiGQuPYpgGwglUujaz+U=", "email": "sindre@sindrejohansen.no", "date_joined": "2013-05-11T13:53:20.896Z"}}, {"pk": 7, "model": "auth.user", "fields": {"username": "aleksanb", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-13T14:44:46.627Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$TCbo6iGI7TP6$n83smO52Y5cFZEnJnhLkhC7pTRRuSN2sM5ByLjUOxXc=", "email": "aleksanderburkow@gmail.com", "date_joined": "2013-02-19T18:37:52.023Z"}}, {"pk": 23, "model": "auth.user", "fields": {"username": "Skarding", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-14T11:12:23.758Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$ZpdgBrhK3msn$nNd6IqArzvxZN4JHTGj4dGBy13rd2cMDlHbIZreNiPI=", "email": "skarding@stud.ntnu.no", "date_joined": "2013-05-13T08:47:52.885Z"}}, {"pk": 26, "model": "auth.user", "fields": {"username": "kakefabrikken", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-14T16:11:32.853Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$JyaMwnxCIj29$sLWZ0UZ6KWWzq8x2STeIepCmqsHNCAMT/CJ800Cu/5Y=", "email": "bluh@hotmail.com", "date_joined": "2013-05-14T16:11:30.592Z"}}, {"pk": 4, "model": "auth.user", "fields": {"username": "Assios", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-15T10:39:54.022Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$w9qZ4s5CywAd$/kzg8GUqymnjfXc0DF9ahEvDPSwogSTKq4qMK7P7GSc=", "email": "asbjorn@steinskog.me", "date_joined": "2013-02-19T14:37:09.798Z"}}, {"pk": 27, "model": "auth.user", "fields": {"username": "trulsbjo", "first_name": "", "last_name": "", "is_active": true, "is_superuser": false, "is_staff": false, "last_login": "2013-05-15T22:12:56.956Z", "groups": [], "user_permissions": [], "password": "pbkdf2_sha256$10000$hP5rHT5Wlk8X$3nNpz6O0fTTOEvrLTw7p5dN+ns3i4cRm+531XEg03r0=", "email": "truls.hamborg@gmail.com", "date_joined": "2013-05-15T22:12:56.652Z"}}, {"pk": 77, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-05-14T12:12:01.883Z", "object_repr": "[135] Software Engineering", "object_id": "135", "change_message": "Changed content.", "user": 2, "content_type": 11}}, {"pk": 76, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-05-10T12:18:43.764Z", "object_repr": "TEST", "object_id": "11", "change_message": "", "user": 2, "content_type": 10}}, {"pk": 75, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:44:56.922Z", "object_repr": "[45] Data Modelling and Database Systems", "object_id": "45", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 74, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:44:56.871Z", "object_repr": "[44] Data Modelling and Database Systems", "object_id": "44", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 73, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:44:56.840Z", "object_repr": "[43] Data Modelling and Database Systems", "object_id": "43", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 72, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:44:56.816Z", "object_repr": "[42] Data Modelling and Database Systems", "object_id": "42", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 71, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:44:56.786Z", "object_repr": "[41] Data Modelling and Database Systems", "object_id": "41", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 70, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:28:40.471Z", "object_repr": "[31] Introduction to Information Technology", "object_id": "31", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 69, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:28:23.538Z", "object_repr": "[15] KTN", "object_id": "15", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 68, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:28:23.491Z", "object_repr": "[14] KTN", "object_id": "14", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 67, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:28:23.463Z", "object_repr": "[13] KTN", "object_id": "13", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 66, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:28:23.440Z", "object_repr": "[12] KTN", "object_id": "12", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 65, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:28:23.421Z", "object_repr": "[11] KTN", "object_id": "11", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 64, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:28:23.328Z", "object_repr": "[10] KTN", "object_id": "10", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 63, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:28:11.476Z", "object_repr": "[36] Compilers", "object_id": "36", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 62, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:28:11.397Z", "object_repr": "[35] Compilers", "object_id": "35", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 61, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:28:11.330Z", "object_repr": "[34] Compilers", "object_id": "34", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 60, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:28:11.286Z", "object_repr": "[33] Compilers", "object_id": "33", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 59, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:28:11.084Z", "object_repr": "[30] Compilers", "object_id": "30", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 58, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:28:10.928Z", "object_repr": "[29] Compilers", "object_id": "29", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 57, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:27:50.551Z", "object_repr": "[18] Programming Languages", "object_id": "18", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 56, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:27:50.503Z", "object_repr": "[16] Programmeringsspr\u00e5k", "object_id": "16", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 55, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:27:50.484Z", "object_repr": "[3] Programmeringsspr\u00e5k", "object_id": "3", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 54, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:27:50.442Z", "object_repr": "[2] Programmeringsspr\u00e5k", "object_id": "2", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 53, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:27:50.395Z", "object_repr": "[1] Programmeringsspr\u00e5k", "object_id": "1", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 52, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:27:34.576Z", "object_repr": "[17] Operating Systems", "object_id": "17", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 51, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:27:34.553Z", "object_repr": "[9] Operating Systems", "object_id": "9", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 50, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:27:34.504Z", "object_repr": "[8] Operating Systems", "object_id": "8", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 49, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:27:34.452Z", "object_repr": "[7] Operating Systems", "object_id": "7", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 48, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:27:34.400Z", "object_repr": "[6] Operating Systems", "object_id": "6", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 47, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:27:34.372Z", "object_repr": "[5] Operating Systems", "object_id": "5", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 46, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:27:34.276Z", "object_repr": "[4] Operativsystemer", "object_id": "4", "change_message": "", "user": 2, "content_type": 11}}, {"pk": 45, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:25:45.184Z", "object_repr": "admin", "object_id": "1", "change_message": "Changed password and is_active.", "user": 1, "content_type": 3}}, {"pk": 44, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:25:27.632Z", "object_repr": "stiaje", "object_id": "2", "change_message": "Changed password, is_staff and is_superuser.", "user": 1, "content_type": 3}}, {"pk": 43, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:25:17.728Z", "object_repr": "sigveseb", "object_id": "3", "change_message": "Changed password, is_staff and is_superuser.", "user": 1, "content_type": 3}}, {"pk": 42, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:19:59.040Z", "object_repr": "USERS", "object_id": "7", "change_message": "", "user": 1, "content_type": 10}}, {"pk": 41, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:19:04.376Z", "object_repr": "[26] Data Modelling and Database Systems", "object_id": "26", "change_message": "", "user": 1, "content_type": 11}}, {"pk": 40, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:19:04.354Z", "object_repr": "[25] Data Modelling and Database Systems", "object_id": "25", "change_message": "", "user": 1, "content_type": 11}}, {"pk": 39, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:19:04.332Z", "object_repr": "[24] Data Modelling and Database Systems", "object_id": "24", "change_message": "", "user": 1, "content_type": 11}}, {"pk": 38, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:19:04.266Z", "object_repr": "[23] Data Modelling and Database Systems", "object_id": "23", "change_message": "", "user": 1, "content_type": 11}}, {"pk": 37, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:19:04.199Z", "object_repr": "[22] Data Modelling and Database Systems", "object_id": "22", "change_message": "", "user": 1, "content_type": 11}}, {"pk": 36, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:19:04.116Z", "object_repr": "[21] Data Modelling and Database Systems", "object_id": "21", "change_message": "", "user": 1, "content_type": 11}}, {"pk": 35, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-03-06T12:19:03.995Z", "object_repr": "[20] Data Modelling and Database Systems", "object_id": "20", "change_message": "", "user": 1, "content_type": 11}}, {"pk": 34, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:17:53.395Z", "object_repr": "[20] Data Modelling and Database Systems", "object_id": "20", "change_message": "Changed child.", "user": 1, "content_type": 11}}, {"pk": 33, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:17:50.797Z", "object_repr": "[18] Programming Languages", "object_id": "18", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 32, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:17:46.301Z", "object_repr": "[21] Data Modelling and Database Systems", "object_id": "21", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 31, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:17:30.130Z", "object_repr": "[22] Data Modelling and Database Systems", "object_id": "22", "change_message": "No fields changed.", "user": 1, "content_type": 11}}, {"pk": 30, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:17:28.258Z", "object_repr": "[22] Data Modelling and Database Systems", "object_id": "22", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 29, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:17:19.046Z", "object_repr": "[23] Data Modelling and Database Systems", "object_id": "23", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 28, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:17:10.382Z", "object_repr": "[24] Data Modelling and Database Systems", "object_id": "24", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 27, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:16:59.186Z", "object_repr": "[25] Data Modelling and Database Systems", "object_id": "25", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 26, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:16:56.163Z", "object_repr": "[27] Programming Languages", "object_id": "27", "change_message": "Changed parent.", "user": 1, "content_type": 11}}, {"pk": 25, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:16:46.407Z", "object_repr": "[26] Data Modelling and Database Systems", "object_id": "26", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 24, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:16:31.836Z", "object_repr": "[28] Data Modelling and Database Systems", "object_id": "28", "change_message": "Changed parent.", "user": 1, "content_type": 11}}, {"pk": 23, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:16:25.508Z", "object_repr": "[18] Programming Languages", "object_id": "18", "change_message": "Changed parent.", "user": 1, "content_type": 11}}, {"pk": 22, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:16:13.186Z", "object_repr": "[10] KTN", "object_id": "10", "change_message": "Changed child.", "user": 1, "content_type": 11}}, {"pk": 21, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:16:06.039Z", "object_repr": "[11] KTN", "object_id": "11", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 20, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:16:03.632Z", "object_repr": "[18] Programming Languages", "object_id": "18", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 19, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:15:55.028Z", "object_repr": "[12] KTN", "object_id": "12", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 18, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:15:49.074Z", "object_repr": "[16] Programmeringsspr\u00e5k", "object_id": "16", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 17, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:15:41.588Z", "object_repr": "[13] KTN", "object_id": "13", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 16, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:15:32.401Z", "object_repr": "[3] Programmeringsspr\u00e5k", "object_id": "3", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 15, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:15:31.536Z", "object_repr": "[14] KTN", "object_id": "14", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 14, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:15:22.697Z", "object_repr": "[15] KTN", "object_id": "15", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 13, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:15:02.370Z", "object_repr": "[2] Programmeringsspr\u00e5k", "object_id": "2", "change_message": "Changed parent and child.", "user": 1, "content_type": 11}}, {"pk": 12, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:14:59.030Z", "object_repr": "[19] Communication Services and Networks", "object_id": "19", "change_message": "Changed parent.", "user": 1, "content_type": 11}}, {"pk": 11, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-03-06T12:14:44.937Z", "object_repr": "[1] Programmeringsspr\u00e5k", "object_id": "1", "change_message": "Changed child.", "user": 1, "content_type": 11}}, {"pk": 5, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-02-19T18:42:38.304Z", "object_repr": "ITGK", "object_id": "6", "change_message": "", "user": 1, "content_type": 10}}, {"pk": 4, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-02-05T02:19:58.455Z", "object_repr": "example.com", "object_id": "1", "change_message": "", "user": 1, "content_type": 6}}, {"pk": 10, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-02-03T06:41:52.325Z", "object_repr": "admin", "object_id": "1", "change_message": "Changed password, first_name and last_name.", "user": 1, "content_type": 3}}, {"pk": 9, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-02-03T06:15:26.210Z", "object_repr": "IT 101", "object_id": "10", "change_message": "No fields changed.", "user": 1, "content_type": 11}}, {"pk": 8, "model": "admin.logentry", "fields": {"action_flag": 2, "action_time": "2013-02-03T06:15:20.134Z", "object_repr": "IT 101", "object_id": "10", "change_message": "Changed lang.", "user": 1, "content_type": 11}}, {"pk": 7, "model": "admin.logentry", "fields": {"action_flag": 1, "action_time": "2013-02-03T06:11:22.275Z", "object_repr": "IT 101", "object_id": "10", "change_message": "", "user": 1, "content_type": 11}}, {"pk": 6, "model": "admin.logentry", "fields": {"action_flag": 1, "action_time": "2013-02-03T06:10:46.744Z", "object_repr": "TDT4110", "object_id": "5", "change_message": "", "user": 1, "content_type": 10}}, {"pk": 3, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-02-03T05:38:53.790Z", "object_repr": "OP4", "object_id": "3", "change_message": "", "user": 1, "content_type": 10}}, {"pk": 2, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-02-03T05:38:52.621Z", "object_repr": "MM", "object_id": "2", "change_message": "", "user": 1, "content_type": 10}}, {"pk": 1, "model": "admin.logentry", "fields": {"action_flag": 3, "action_time": "2013-02-03T05:38:51.612Z", "object_repr": "ASD", "object_id": "1", "change_message": "", "user": 1, "content_type": 10}}, {"pk": 94, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T13:39:46.587Z", "edited_by": 3, "parent": 93, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||", "child": 95, "article": 10}}, {"pk": 101, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T17:02:25.791Z", "edited_by": 3, "parent": 100, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n## Spring 2013\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams](Old exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\n## Straight up regular local search\r\n## Variable-depth search\r\n## Kernighan-Lin variable depth search\r\n## Simulated annealing\r\n## Tabu search\r\n## Randomized Tabu search\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 102, "article": 10}}, {"pk": 180, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-13T14:06:36.247Z", "edited_by": 9, "parent": 179, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) + (int), but disallow (str) + (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar G = (N,\u03c3,P,S) consists of:\r\n\r\n* A finite set N of nonterminal symbols\r\n* A finite set \u03c3 of terminal symbols that is disjoint from N.\r\n* A finite set P of production rules, each rule on the form:\r\n  (\u03c3 \u222a N)* N(\u03c3 \u222a N)* \u2192 (\u03c3 \u222a N)* \r\n* A symbol S \u2208 N that is the start symbol\r\n\r\n\r\nThe language of a grammar G, L(G), is defined as all the sentences that can be derived in a finite number of steps from the start symbol S.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, \u03b5 or \u039b for the empty string, and S for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form V\u2192v, where V is a non-terminal and v may be a combination of terminals and non-terminals.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form \u03b1A\u03b2\u2192\u03b1\u03b3\u03b2, where \u03b1, \u03b2 and \u03b3 are strings of nonterminals and terminals, and \u03b3 is not \u03b5.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string (\u03b5) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols \u03b1 is the set of terminals that begin the strings derived from \u03b1. FIRST(\u03b1) can be constructed like this for each symbol X in \u03b1:\r\n\r\n1. If X is a terminal, FIRST(X) = {X}.\r\n2. If X\u2192\u03b5 is a production, add \u03b5 to FIRST(X)\r\n3. If X\u2192Y1Y2\u2026Yk then add FIRST(Y1Y2\u2026Yk) to FIRST(X)\r\n4. FIRST(Y1Y2\u2026Yk) is one of:\r\n    * FIRST(Y1) if FIRST(Y1) doesn't contain \u03b5.\r\n    * FIRST(Y1) + FIRST(Y2\u2026Yk) - {\u03b5}\r\n\r\nIf FIRST(Yi) contains \u03b5 for all 0\u2264i\u2264k \u2208 \u2124, add \u03b5 to FIRST(X)\r\n\r\nThe **FOLLOW** set of a nonterminal A is the set of terminals a that can appear immediately to the right of A. In other words: the set of terminals a such that there exists a derivation of the form S\u2192\u03b1Aa\u03b2. FOLLOW(A) for all nonterminals A can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW(S), where S is the start symbol.\r\n2. If there is a production A\u2192\u03b1B\u03b2, then everything in FIRST(\u03b2) - {\u03b5} is placed in FOLLOW(B)\r\n3. If there is a production A\u2192\u03b1B or a production A\u2192\u03b1B\u03b2 where FIRST(\u03b2) contains \u03b5, everything in FOLLOW(A) is in FOLLOW(B).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites.\r\n\r\nShift-reduce parsing is a category of commonly used methods for bottom-up parsing. Shift-reduce is based on two basic steps: shift and reduce. Shift advances in the input stream by one token, making the new symbol a new single-node parse tree. Reduce applies a completed grammar rule to one the recent parse trees, joining them together as one tree with a new root symbol.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. LR-parsing is a shift-reduce parsing method. Canonical LR-parsing, though fast, is prohibitively spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n\r\n\r\n\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Optimizations and when to apply them\r\n\r\nCopied from slide number 27 of [\"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum](http://www.cs.cornell.edu/Courses/cs412/2008sp/lectures/lec23.pdf).\r\n\r\n### High IR\r\n\r\n#### Function Inlining\r\n\r\n#### Function cloning\r\n\r\n### Low IR\r\n\r\n#### Constant folding\r\n\r\nEvaluate constant expressions and change them with their value (change x = 3 * 2 to x = 6).\r\n\r\n#### Constant propagation\r\n\r\nIf we know that x has the value 6, we may change subsequent reads of x with the value 6 until x is overwritten.\r\n\r\n#### Value numbering\r\n\r\n#### Dead code elimination\r\n\r\n#### Loop-invariant code motion\r\n\r\n#### Common sub-expression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n#### Strength reduction\r\n\r\n#### Constant folding & propagation (?)\r\n\r\n#### Branch prediction/optimization\r\n\r\n#### Loop unrolling\r\n\r\n### Assembly\r\n\r\n#### Register allocation\r\n\r\n#### Cache optimization\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node d dominates a node n if every path from the start node to n must go through d. We write: d dom n, or d >> n. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": null, "article": 5}}, {"pk": 108, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T19:28:35.728Z", "edited_by": 3, "parent": 107, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n## Simulated annealing\r\n## Tabu search\r\n## Randomized Tabu search\r\n\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 109, "article": 10}}, {"pk": 115, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-09T11:03:20.936Z", "edited_by": 3, "parent": 114, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search remembers the last _k_ visited candidate solutions, and avoids revisiting them. This prevents a possibility to be considered repeatedly, getting stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n\r\n\r\n\r\n", "child": 116, "article": 10}}, {"pk": 48, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-03-06T13:32:25.098Z", "edited_by": 2, "parent": null, "title": "Data Modelling and Database Systems", "content": "# Types of failures\r\n\r\nFailures of type 1-4 are common, and the system is expected to automatically recover from these. Failures of type 5 and 6 are less common, and recovering from them can be a major undertaking. \r\n\r\n1.  A computer failure / crash\r\n2.  A transaction or system error\r\n3.  Local errors or exception conditions detected by the transaction\r\n4.  Concurrency control enforcement\r\n5.  Disk failure\r\n6.  Physical problems and catastrophes\r\n\r\n# Database storage stuctures\r\n\r\nDatabases are typically stored on secondary (non-volatile) storage.\r\n\r\n## List of storage structures\r\n\r\n|| Heap file || The simplest storage structure: an unordered list of records. Records are inserted at the end of the heap file, giving O(1) insertion. Retrieval requires a linear search ( O(n) ), and is therefore quite inefficient for large values of n. Deleted records are only marked as deleted, and not actually removed. This is O(1) (in addition to the time it takes to retrieve a record to be deleted). Highly volatile heap-files must be re-structured often to prune away records that are marked as deleted. ||\r\n|| Hash buckets || Hash functions directly calculate the page address of a record based on values in the record. A good hashing function has an even spread across the address space. When doing calculations on hash buckets (say, for an exam), it is safe to assume about a 60% fill rate. Ignoring collisions, hash buckets are O(1) for retrieval, insertion, modification and deletion. Hash buckets often suck for range retrieval. ||\r\n|| B%2B-trees || A pretty good all-around storage structure. Retrieval, insertion, modification and deletion is O(log n). Good for volatile data, as the index dynamically expands and contracts as the data set grows and shrinks. Less optimal for non-volatile/stable files - use ISAM for that. ||\r\n|| ISAM || (Indexed Sequential Access Method) ||\r\n\r\n## Hashing schemes\r\n\r\n|| Extendible hashing || Extendible hashing treats a hash as a bit string, and uses a trie for bucket lookup. Re-hashing is done incrementally, one bucket at a time, as needed. Both LSB and MSB implementations are common. ||\r\n|| Linear hashing || Linear hashing allows for expansion of the hash table one slot at a time, round-robin. ||\r\n\r\n# Transactions\r\n\r\nA transaction is a unit of work performed within a DBMS. They provide reliability and isolation. Transactions are \"all or nothing\", in the sense that the entire unit of work must complete without error, or have no effect at all.\r\n\r\n## ACID: Atomicitiy, consistency, isolation and durability\r\n\r\nDatabases must conform to the ACID principles:\r\n\r\n|| Atomicity || A unit of work cannot be split into smaller parts\u2014a unit of work can either complete entirely, or fail entirely. ||\r\n|| Consistency || A transaction always leaves the database in a valid state upon completion, with all data valid according to defined rules such as constraints, triggers, cascades etc. ||\r\n|| Isolation || No transaction can interfere with another transaction. ||\r\n|| Durability || When a transaction completes, it remains complete, even in the case of power loss, crashes etc. ||\r\n\r\nA typical transaction follows the following lifecycle:\r\n\r\n1.  Begin the transaction\r\n2.  Execute a set of data manipulations/queries\r\n3.  If no errors occur, then commit the transaction and end it\r\n4.  If errors occur, then rollback the transaction and end it\r\n\r\n## Policies used in transaction control\r\n\r\nThe following are four policies that can be used in transaction control in a database:\r\n\r\n|| NO-FORCE policy || Changes made to objects are not required to be written to disk in-place. Changes must still be logged to maintain durability, and can be applied to the object at a later time. This reduces seek-time on disk for a commit, as the log is usually sequentially stored in memory, and objects can be scattered. Frequently updated objects can also merge accumulated writes from the log, thereby reducing the total number of writes on the object. Faster, but adds rollback complexity. ||\r\n|| FORCE policy || Changes made to objects are required to be written to disk in-place. Sucks. ||\r\n|| STEAL policy || Allows a transaction to be written on a nonvolatile storage before its commit occurs. Faster, but adds rollback complexity. ||\r\n|| NO-STEAL policy || Does not allow a transaction to be written on a nonvolative storage before its commit occurs. Sucks. ||\r\n\r\nPerformance wise, if anyone asks, STEAL NO-FORCE is the way to go.\r\n\r\n## The system log\r\n\r\nA DBMS maintains a system log tracking all transaction operations to be able to recover from failures. The log must be kept on non-volatile storage, for obvious reasons. A log consists of records of different types:\r\n\r\n1.  [start\\_transaction, transaction\\_id]\r\n2.  [write\\_item, transaction\\_id, item\\_id, old\\_value, new_value]\r\n3.  [read\\_item, transaction\\_id, item_id]\r\n4.  [commit, transaction_id]\r\n5.  [abort, transaction_id]\r\n\r\n## ARIES (aka Algorithms for Recovery and Isolation Exploiting Semantics)\r\n\r\nRecovery algorithm designed to work with a no-force, steal database approach. ARIES has three main principles:\r\n\r\n|| Write ahead logging || Any change to an object is first recorded in the log, and the log must be written to non-volatile storage before changes to the object are made. ||\r\n|| Repeating history during redo || On restart after a crash, ARIES retraces the steps in history necessary to bring the database to the exact state at the time of the crash, and then undos the transactions that were still active at the time of the crash. ||\r\n|| Logging changes during undo || Changes made to the database during undo are logged to prevent unwanted repeats. ||\r\n\r\nTwo datastructures must be maintained to gather enough information for logging: the dirty page table (DPT) and the transaction table (TT):\r\n\r\n|| Dirty page table (DPT) || Keeps record of all the pages that have been modified and not yet written to disk. It also stores the sequence number of the first sequence that made a page dirty (recLSN). ||\r\n|| Transaction table (TT) || Contains all transactions that are currently running, as well as the sequence number of the last sequence that caused an entry in the log. ||\r\n\r\nARIES periodically saves the DPT and the TT to the log file, creating checkpoints, to avoid rescanning the entire log file in the case of a redo. This allows for skipping blocks that are not in the DPT, or that are in the DPT but have a recLSN that is greater than the logg post LSN.\r\n\r\n## Schedule (aka History)\r\n\r\nSchedules are lists of operations (usually data reads/writes, locks, commits etc) ordered by time. Schedules can be classified into different classes:\r\n\r\n|| Serial || Transactions are non-interleaved. No transaction starts before the previous one has ended. Example: D = R1(X)W1(X)C1W(2)C2. ||\r\n|| Serializable || Schedules that have equivalent outcomes to a serial schedule are serializable. A schedule is serializable if and only if its precedence graph is acyclic ||\r\n|| Recoverable || Transactions commit only after all transactions whose changes they read, commit. ||\r\n|| Unrecoverable || A schedule which is not recoverable ||\r\n|| ACA (Avoids Cascading Aborts, aka Cascadeless) || Transactions may not read uncommitted changes from another transaction during the same schedule. All ACA are recoverable. ||\r\n|| Strict || Write operations in one transaction which precedes a conflicting operation in another transaction must be committed before the conflicting operation in the second transaction occurs. All strict schedules are ACA. ||\r\n\r\n# Database normalization\r\n\r\nDatabase normalization is the process of organizing the fields and tables of a relational database to minimalize redundancy and dependency.\r\n\r\n## Normal forms (aka NF)\r\n\r\nA normal form is a set of criteria for determining a table's degree of vulnerability to logical inconsistencies and anomalies. The most common normal forms are:\r\n\r\n|| 1NF  || A table represents a relation and has no repeating groups. ||\r\n|| 2NF  || No non-prime attribute in the table is partially dependent on any candiate key. ||\r\n|| 3NF  || Every non-prime attribute in the table is directly dependent on every candidate key. ||\r\n|| BCNF || Every non-trivial functional dependency is a dependency on a superkey. ||\r\n\r\nNote that to meet the requirements of a normal form, a table must also meet all the requirementes of the lower normal forms.\r\n\r\n### Relational algebra\r\n\r\n#### Primitive operators\r\n\r\n|| Projection (\u03c0) || The projection of a relation R on a set of attributes A is the set of all tuples in R restricted to the set of attributes in A. Basically the SELECT of relational algebra. ||\r\n|| Selection (\u03c3) || A selection with the set of conditions C selects all the tuples in a relation R such that the conditions in C hold. Basically the WHERE of relational algebra. ||\r\n|| Rename (\u03c1) || Renames attributes in a relation. Basically the AS of relational algebra. ||\r\n|| Natural Join (\u22c8) || Joins two relations R and S. The result of the join is the set of all combinations of tuples in R and S that are equal on their common attribute names. Basically the INNER JOIN of relational algebra.||", "child": 49, "article": 4}}, {"pk": 129, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T09:20:31.735Z", "edited_by": 3, "parent": 128, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = { a_1, a_2, ..., a_k }$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n* Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals ** lsjifasdfw ** and ** areighifpo ** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n* Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: ** lsjifhifpo ** could mutate to ** lsjifaifpo **, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n* (regrouping)\r\n* (colonization-extinction)\r\n* (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 130, "article": 10}}, {"pk": 39, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-02-28T11:26:32.333Z", "edited_by": 3, "parent": null, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")", "child": 50, "article": 5}}, {"pk": 50, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-03-08T12:42:22.767Z", "edited_by": 9, "parent": 39, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.", "child": 51, "article": 5}}, {"pk": 51, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-03-08T12:42:34.423Z", "edited_by": 9, "parent": 50, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.", "child": 52, "article": 5}}, {"pk": 56, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-03T10:19:20.344Z", "edited_by": 3, "parent": null, "title": "Energieffektive Datasystemer", "content": "# Eksamensinformasjon\r\n\r\n29. Mai kl 9, 3 timer, kategori D\r\n\r\nMultiple choice 25%\r\nTeoretiske og praktiske oppgaver, der praktisk betyr assembly, pseudokode o.l.\r\n\r\n\r\n", "child": 57, "article": 8}}, {"pk": 144, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T10:03:20.471Z", "edited_by": 3, "parent": 143, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\n## The Chomsky hierarchy\r\n\r\nRegular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n# Parsing\r\n\r\n\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n# Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n## LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. \r\n\r\n### Look-Ahead LR parser (LALR-parser)\r\n\r\nFIRST and FOLLOW sets\r\nLL-parsing\r\nLeft-recursive grammars\r\nLR-parsing\r\nSLR\r\nLALR\r\nHandling ambiguity\r\nOperator precedence\r\nShift/reduce\r\nThe dangling else problem\r\n\r\n\r\n\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 145, "article": 5}}, {"pk": 57, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-03T10:19:49.002Z", "edited_by": 3, "parent": 56, "title": "Energieffektive Datasystemer", "content": "# Eksamensinformasjon\r\n\r\n29. Mai kl 9, 3 timer, kategori D\r\n\r\nMultiple choice 25%\r\nTeoretiske og praktiske oppgaver, der praktisk betyr assembly, pseudokode o.l.\r\n\r\n\r\n", "child": 58, "article": 8}}, {"pk": 58, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-03T10:21:07.341Z", "edited_by": 3, "parent": 57, "title": "Energieffektive Datasystemer", "content": "# Eksamensinformasjon\r\n\r\n29. Mai kl 9, 3 timer, kategori D\r\n\r\nMultiple choice 25%\r\nTeoretiske og praktiske oppgaver, der praktisk betyr assembly, pseudokode o.l.\r\n\r\nPensum: Bok, forelesninger, \u00f8vinger\r\n\r\n\r\n", "child": 59, "article": 8}}, {"pk": 59, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-03T10:22:38.949Z", "edited_by": 3, "parent": 58, "title": "Energieffektive Datasystemer", "content": "# Eksamensinformasjon\r\n\r\n29. Mai kl 9, 3 timer, kategori D\r\n\r\nMultiple choice 25%\r\nTeoretiske og praktiske oppgaver, der praktisk betyr assembly, pseudokode o.l.\r\n\r\nPensum: Bok, forelesninger, \u00f8vinger\r\n\r\nSp\u00f8rsm\u00e5l? Send til djupdal@idi.ntnu.no\r\n\r\nDeveloping for embedded systems, kapittel 1, 4 og 7", "child": 60, "article": 8}}, {"pk": 60, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-03T10:23:33.314Z", "edited_by": 3, "parent": 59, "title": "Energieffektive Datasystemer", "content": "# Eksamensinformasjon\r\n\r\n29. Mai kl 9, 3 timer, kategori D\r\n\r\nMultiple choice 25%\r\nTeoretiske og praktiske oppgaver, der praktisk betyr assembly, pseudokode o.l.\r\n\r\nPensum: Bok, forelesninger, \u00f8vinger\r\n\r\nSp\u00f8rsm\u00e5l? Send til djupdal@idi.ntnu.no\r\n\r\nDeveloping for embedded systems, kapittel 1, 4 og 7\r\nVesentlig: prosessen rundt utvikling og design for embedded\r\n\r\nProcessors and instructions, kap 2 og 3", "child": 61, "article": 8}}, {"pk": 32, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-02-19T18:45:31.223Z", "edited_by": 5, "parent": null, "title": "Introduction to Information Technology", "content": "Soon to come", "child": null, "article": 6}}, {"pk": 61, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-03T10:31:07.179Z", "edited_by": 3, "parent": 60, "title": "Energieffektive Datasystemer", "content": "# Eksamensinformasjon\r\n\r\n29. Mai kl 9, 3 timer, kategori D\r\n\r\nMultiple choice 25%\r\nTeoretiske og praktiske oppgaver, der praktisk betyr assembly, pseudokode o.l.\r\n\r\nPensum: Bok, forelesninger, \u00f8vinger\r\n\r\nSp\u00f8rsm\u00e5l? Send til djupdal@idi.ntnu.no\r\n\r\nDeveloping for embedded systems, kapittel 1, 4 og 7\r\nVesentlig: prosessen rundt utvikling og design for embedded\r\n\r\nProcessors and instructions, kap 2 og 3\r\n\r\nIo devices, memory mapping, polling\r\n\r\nInterrupts, exceptions er sentralt\r\n\r\n\r\n", "child": 62, "article": 8}}, {"pk": 62, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-03T10:34:16.440Z", "edited_by": 3, "parent": 61, "title": "Energieffektive Datasystemer", "content": "# Eksamensinformasjon\r\n\r\n29. Mai kl 9, 3 timer, kategori D\r\n\r\nMultiple choice 25%\r\nTeoretiske og praktiske oppgaver, der praktisk betyr assembly, pseudokode o.l.\r\n\r\nPensum: Bok, forelesninger, \u00f8vinger\r\n\r\nSp\u00f8rsm\u00e5l? Send til djupdal@idi.ntnu.no\r\n\r\nDeveloping for embedded systems, kapittel 1, 4 og 7\r\nVesentlig: prosessen rundt utvikling og design for embedded\r\n\r\nProcessors and instructions, kap 2 og 3\r\n\r\nIo devices, memory mapping, polling\r\n\r\nInterrupts, exceptions er sentralt\r\n\r\nTyper, hardware, reset interrupt page fault,\r\n\r\nSoftware, etc\r\n\r\nException behaviour\r\n\r\n\r\n\r\nArm processor modes,\r\nNormal mode , user and system/kernel\r\n\r\nException modes\r\nSupervisor, abort, undefined, interrupt, fast interrupt\r\n\r\n\r\n", "child": 63, "article": 8}}, {"pk": 64, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-06T13:23:14.013Z", "edited_by": 9, "parent": null, "title": "Artificial Intelligence Methods", "content": "*   Bayesian networks    *   Syntax and semantics", "child": 65, "article": 9}}, {"pk": 38, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-02-23T03:16:01.147Z", "edited_by": 1, "parent": null, "title": "Operating Systems", "content": "# Introduction\r\n\r\nAn operating system (OS) is a collection of software that provides an abstract machine on top of a physical machine. The OS provides resource management. An OS makes a computer easier to use for end users and programmers. Typical services provided by an OS includes storage/file systems, process/thread management, security and communications.\r\n\r\n# Processes\r\n\r\nA process is an instance of a computer program that is being executed. It contains the progam code and its current memory state. A CPU can execute a single process at a time. Multiprogramming is a way for CPUs to fake concurrency by switching between different processes, often quickly. The Scheduler, using a schelduling algorithm, decides which process gets to use a CPU at any given time.\r\n\r\n## Scheduling\r\n\r\nScheduling algorithms should be fair and balanced. For batch systems, the goal is to maximize process throughput, minimize turnaround time, and maximize CPU utilization. For interactive systems, the goal is to minimize response time, and ensure proportionality in latency (meeting the user's expectations). For real time systems, the goal is to meet deadlines and behave predictably.\r\n\r\nA scheduling algorithm can either be preemptive or non-preemptive. A non-preemptive lets active processes decide when they are done using the CPU, while a preemptive algorithm can decide to switch processes at any time.\r\n\r\n### Scheduling Algorithms in Batch Systems\r\n\r\nFirst-Come First-ServedNon-preemptive. Active processes are run to completion or blockage. New or unblocked processes are put in the ready queue. Maximizes CPU usage.Shortest Job FirstNon-preemptive. Uses statistics to make educated guesses about run-time of processes. The ready queue is a priority queue prioritizing the shortest jobs first. Minimizes turnaround.Shortest Remaining Time NextPreemptive. Like Shortest Job First, except a new process may take the place of the active process if its remaining time is lower than that of the active process.\r\n\r\n### Scheduling Algorithms in Interactive Systems\r\n\r\n||Round-Robin Scheduling||Preemptive. Each process is assigned a time interval, or quantum. Processes that run longer than quantum are switched. Switching also occurs when the actuve process becomes blocked, or terminates. The quantum time must be tweaked to allow the best balance of response time and CPU utilization.||\r\n||Priority Scheduling||Preemptive. Processes are given a priority score, assigned statically or dynamically. The ready queue is a prority queue on process priority. May use quantums.||\r\n||Shortest Process Next||Preemptive. Estimates which process is shortest, and prioritizes accordingly.||\r\n||Guaranteed Scheduling||Preemptive. When there are n processes, each process gets 1/n CPU cycles.||\r\n||Lottery Scheduling||Preemptive. Prioritize processes at (optionally weighted) random.||\r\n\r\n### Scheduling Algorithms in Real Time Systems\r\n\r\nTBD.\r\n\r\n# Memory Management\r\n\r\nThe essential requirement of memory management is to provide a memory abstraction which allows dynamically allocating portions of memory to a process. This section details different memory management strategies.\r\n\r\n## No Memory Abstraction\r\n\r\nThe entire memory is addressable by every process. Usually a complete disaster when multiprogramming is involved.\r\n\r\n## Address Spaces\r\n\r\nA process is given a private address space, which is a private area of memory denoted by a base address and a size limit. Processes use relative addresses which are translated to absolute addresses by the OS.\r\n\r\n## Swapping\r\n\r\nSwapping dumps a non-running process' address space to disk to free up memory when the system runs out of space. Swapping takes time.\r\n\r\n## Virtual Memory\r\n\r\nTo decrease swaping times, address spaces are split into chunks called pages. Each page can then be swapped independantly, even while the process is running. When a page which is not in memory is requested, it gets loaded in on-the fly. This is called a page fault.\r\n\r\n## Physical Representation\r\n\r\nPhysically, memory structures are usually stored in a tree-like hierarchy, with pointers to the next nodes.\r\n\r\n## Page Replacement Algorithms\r\n\r\n### Not Recently Used (NRU)\r\n\r\nPages are given two flags: referenced and modified. Each time a page is read from, the referenced flag is set. Each time a page is written to, the modified flag is set. Every now and then (e.g. every 20 msec), a clock signal triggers the resetting of the referenced flag. Based on these flags, we can classify the pages at any given time in one of four classes:\r\n\r\n||Class 0||Not referenced, not modified.||\r\n||Class 1||Not referenced, modified.||\r\n||Class 2||Referenced, not modified.||\r\n||Class 3||Referenced, modified.||\r\n\r\nNRU swaps out a random page from the lowest-numbered non-empty class.\r\n\r\n### First-In, First-Out (FIFO)\r\n\r\nOldest page gets swapped out first.\r\n\r\n### Least Recently Used (LRU)\r\n\r\nSwaps out the page which has remained unused for the longest time.\r\n\r\n### WSClock\r\n\r\nCombination of the Working Set and Clock algorithms. We regard the memory as an initially-empty cirular list of frames, and maintain a pointer to a single frame (\"current page\"). Each page has a referenced flag and a modified flag, as well as a time of last use field. Again, eventual clock ticks reset the referenced flag. The algorithm:\r\n\r\n    t = some constant, e.g. 120 msec\r\n    \r\n    On page fault:\r\n        if current page is referenced:\r\n            set current page as not referened\r\n            set current page to next page\r\n            restart procedure\r\n    \r\n        else:\r\n            if time difference between now and current page's time of last use is larger than t:\r\n                if current page is modified:\r\n                    schedule disk write for current page\r\n                    set current page to next page\r\n                    restart procedure\r\n                else:\r\n                    swap out current page\r\n\r\nAdditionally, we must consider the case where the current page pointer completes one entire circle in the circular list of frames.\r\n\r\nIn the case where at least one write has been scheduled, the algorithm simply continues until the write is complete, at which point the newly-written page will be ready for eviction.\r\n\r\nOn the other hand, if no writes have been scheduled, the first non-modified page will be swapped out. If there are no non-modified pages, the current page gets swapped out.\r\n\r\n# File systems\r\n\r\nA file system is an abstraction to store, retrieve and update a set of files. A file is a logical unit of information created by a process. Information stored in a file is persistent, and can be accessed by different processes.\r\n\r\n## File Structure\r\n\r\nThere are many different ways to logically structure a file. Here is a list of three common structures:\r\n\r\nByte sequence||Maximum flexibility. The OS does not help, and does not get in the way. UNIX, MS-DOS and Windows uses this.\r\nRecord sequence||Crap. Reads and writes entire fixed-size records at once. Old, punch card-based systems used this, but it is really unpopular now.\r\nTree||Tree of variable size records, allows quick lookup on a specific key. Some mainframes use this.\r\nFile Types\r\n\r\nOperating systems often support different file types. Here are some common file types:\r\n\r\nRegular files||Good ol' regular binary files.\r\nDirectories||System files for maintaining the structure of the file system.\r\nCharacter special files||IO-devices masquerading as byte-sequenced files to abstract writing and reading to and from IO devices.\r\nBlock special files||IO-devices masquerading as record-sequenced files to abstract writing and reading to and from IO devices.\r\n\r\n## Directories\r\n\r\nA file system usually organizes its files in some sort of directory organization. Here are some different ways to organize files:\r\n\r\nSingle-Level Directory Systems||Every file is in one large root directory. Often used on simple embedded devices.\r\nHierarchical Directory Systems||Allows directories to contain other directories, creating a directory tree. Tree directory systems allow for sane organization of files.\r\n\r\n## File System Layout\r\n\r\nA file system is typically stored on a partition on a disk. Every partition starts with a boot block, but other than that, layout on disk can vary wildly from file system to file system.\r\n\r\nBoot block\tFile allocation table\tRoot directory\tFile data area\r\nFigure 1: layout of the FAT-16 file system\r\nAs an example, figure 1 shows the layout of the FAT-16 file system, a basic hierarchical file system. Here, the boot block contains administrative data about the file system, in addition to the bootstrapping program which is run if an operating system attempts to boot the partition. The file allocation table (FAT) contains a list of all the clusters/chunks in the file data area, together with a pointer to the next cluster for each cluster. The root directory is the statically allocated logical root of the file system, containing files and subdirectories. The file data area contains the rest of the data, organized in clusters as referenced from the file allocation table.\r\n\r\n# IO\r\n\r\n# Deadlocks\r\n\r\nA set of processes is deadlocked if each process in the set is waiting for an event that only another process in the set can cause. Four conditions must hold for there to be a deadlock:\r\n\r\nMutual exclusion||A resource can only be assigned to one or no process at once.\r\nHold and wait||Processes currently holding resources can request more resources.\r\nNo preemption||Processes cannot lose a resource against its will.\r\nCircular wait||There must be a circular chain of two or more processes, each waiting for a resource held by the next member of the chain.\r\n\r\nDeadlock prevention can be done by preventing one or more of the four conditions above.\r\n\r\nRemoving a deadlock after it has occurred usually entails killing processes, or doing resource rollbacks.\r\n", "child": null, "article": 2}}, {"pk": 65, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-06T13:34:16.332Z", "edited_by": 9, "parent": 64, "title": "Artificial Intelligence Methods", "content": "*   Strong/weak AI\r\n*   Agents\r\n    *   Rationality\r\n*   Bayesian networks\r\n    *   Syntax and semantics\r\n    *   Uses and problems\r\n*   Reasoning\r\n    *   Case-based reasoning\r\n    *   Instance-based reasoning\r\n*   Markov processes\r\n    *   Markov assumptiom\r\n*   Neural networks\r\n*   Decision trees ", "child": 66, "article": 9}}, {"pk": 77, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T11:08:18.356Z", "edited_by": 3, "parent": 76, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3_bool = {0, 1}\r\n* \u03a3_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }", "child": 78, "article": 10}}, {"pk": 88, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T12:56:19.776Z", "edited_by": 3, "parent": 87, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n\r\n", "child": 89, "article": 10}}, {"pk": 52, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-03-15T00:03:33.362Z", "edited_by": 9, "parent": 51, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.", "child": 53, "article": 5}}, {"pk": 95, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T13:44:02.246Z", "edited_by": 3, "parent": 94, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n\r\n", "child": 96, "article": 10}}, {"pk": 53, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-04-11T11:07:55.129Z", "edited_by": 3, "parent": 52, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* <  emiltayl> FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek\r\n<  emiltayl> lecture 27, slide 22\r\n<  emiltayl> f\u00e5r ikke lagt til kompendium n\u00e5\r\n<  emiltayl> ogs\u00e5 slide 15 i neste sett\r\n\r\n", "child": 54, "article": 5}}, {"pk": 160, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T16:18:36.436Z", "edited_by": 21, "parent": 159, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n$$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string ($$\\epsilon$$) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## shift/reduce\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 161, "article": 5}}, {"pk": 54, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-04-11T11:08:25.639Z", "edited_by": 3, "parent": 53, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n\r\n", "child": 55, "article": 5}}, {"pk": 63, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-03T11:07:15.988Z", "edited_by": 3, "parent": 62, "title": "Energieffektive Datasystemer", "content": "# Eksamensinformasjon\r\nMuliticore mindre relevant\r\nDesignprosessen veldig relevant\r\n\r\n29. Mai kl 9, 3 timer, kategori D\r\n\r\nMultiple choice 25%\r\nTeoretiske og praktiske oppgaver, der praktisk betyr assembly, pseudokode o.l.\r\n\r\nPensum: Bok, forelesninger, \u00f8vinger\r\n\r\nSp\u00f8rsm\u00e5l? Send til djupdal@idi.ntnu.no\r\n\r\nDeveloping for embedded systems, kapittel 1, 4 og 7\r\nVesentlig: prosessen rundt utvikling og design for embedded\r\n\r\nProcessors and instructions, kap 2 og 3\r\n\r\nIo devices, memory mapping, polling\r\n\r\nInterrupts, exceptions er sentralt\r\n\r\nTyper, hardware, reset interrupt page fault,\r\n\r\nSoftware, etc\r\n\r\nException behaviour\r\n\r\n\r\n\r\nArm processor modes,\r\nNormal mode , user and system/kernel\r\n\r\nException modes\r\nSupervisor, abort, undefined, interrupt, fast interrupt\r\n\r\n\r\n", "child": null, "article": 8}}, {"pk": 55, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-04-18T10:48:12.468Z", "edited_by": 3, "parent": 54, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 67, "article": 5}}, {"pk": 102, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T17:02:57.546Z", "edited_by": 3, "parent": 101, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\n## Straight up regular local search\r\n## Variable-depth search\r\n## Kernighan-Lin variable depth search\r\n## Simulated annealing\r\n## Tabu search\r\n## Randomized Tabu search\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 103, "article": 10}}, {"pk": 75, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T10:53:08.066Z", "edited_by": 3, "parent": null, "title": "Algorithm Construction, Advanced Course", "content": "-", "child": 76, "article": 10}}, {"pk": 161, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-12T07:34:12.424Z", "edited_by": 3, "parent": 160, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n$$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string ($$\\epsilon$$) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols $$\\alpha$$ is the set of terminals that begin the strings derived from $$\\alpha$$. FIRST($$\\alpha$$) can be constructed like this for each symbol $$X$$ in $$\\alpha$$:\r\n\r\n1. If $$X$$ is a terminal, FIRST($$X$$) = {$$X$}.\r\n2. If $$X \\rightarrow \\epsilon$$ is a production, add $$\\epsilon$$ to FIRST($$X$$)\r\n3. If $$X \\rightarrow Y\\_1 Y\\_2 ... Y\\_k$$ then add FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) to FIRST($$X$$)\r\n4. FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) is one of:\r\n    * FIRST($$Y\\_1$$) if FIRST($$Y\\_1) doesn't contain $$\\epsilon$$.\r\n    * FIRST($$Y\\_1$$) + FIRST($$Y\\_2 ... Y\\_k$$) - {$$\\epsilon$$}\r\nIf FIRST($$Y_i$$) contains $$\\epsilon$$ for all $$0 \\le i \\le k \\in \\integers$$, add $$\\epsilon$$ to FIRST($$X$$)\r\n\r\nThe **FOLLOW** set of a nonterminal $$A$$ is the set of terminals $$a$$ that can appear immediately to the right of $$A$$. In other words: the set of terminals $$a$$ such that there exists a derivation of the form $$S \\rightarrow \\alpha A a \\beta$$. FOLLOW($$A$$) for all nonterminals $$A$$ can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW($$S$$), where $$S$$ is the start symbol.\r\n2. If there is a production $$A \\rightarrow \\alpha B \\beta, then everything in FIRST($$\\beta$$) - {$$\\epsilon$$} is placed in FOLLOW($$B$$)\r\n3. If there is a production $$A \\rightarrow \\alpha B$$ or a production $$A \\rightarrow \\alpha B \\betawhere FIRST($$\\beta$$) contains $$\\epsilon$$, everything in FOLLOW($$A$$) is in FOLLOW($$B$$).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## shift/reduce\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 162, "article": 5}}, {"pk": 89, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T13:00:43.841Z", "edited_by": 3, "parent": 88, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n\r\n\r\n\r\n\r\n", "child": 90, "article": 10}}, {"pk": 109, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T19:34:22.596Z", "edited_by": 3, "parent": 108, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search remembers the last _k_ visited candidate solutions, and avoids revisiting them. This prevents a possibility to be considered repeatedly, getting stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 110, "article": 10}}, {"pk": 116, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-09T11:07:47.232Z", "edited_by": 3, "parent": 115, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search remembers the last _k_ visited candidate solutions, and avoids revisiting them. This prevents a possibility to be considered repeatedly, getting stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 117, "article": 10}}, {"pk": 167, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-12T12:30:02.635Z", "edited_by": 3, "parent": 142, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook, and even this compendium!). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n# Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP, with the restriction that the triangle inequality holds, can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$, where the edge weights satisfy the triangle inequality.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP with $$\\rho = 1.5$$. Why does this approximate TSP with $$\\rho = 1.5$$, you ask? Well:\r\n\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!\r\n\r\n\r\n# Appendix: list of algorithms featured in the textbook\r\n\r\n* DPKP\r\n* Ford-Fulkerson\r\n* Vertex cover algorithm\r\n* B&B for MAXSAT and TSP\r\n* D&C-3SAT\r\n* Local search scheme\r\n* Kernigan-Lin variable-depth search\r\n* Metropolis algorithm\r\n* Simulated Annealing\r\n* Tabu search\r\n* Randomized tabu search\r\n* Genetic algorithm scheme\r\n* Methods for relaxing problems to LP\r\n* Simplex\r\n* Primal-dual scheme\r\n* Primal-dual (MINVCP)\r\n* Greedy makespan schedule\r\n* Algorithm for greedy makespan schedule\r\n* Algorithm for VCP\r\n* Algorithm for SCP\r\n* Algorithm for WEIGHT-VCP\r\n* Algorithm for MAX-CUT\r\n* Greedy-simple KP\r\n* PTAS for SKP\r\n* modified PTAS for SKP\r\n* FPTAS for KP\r\n* TSP $$\\delta$$-ineq 2-approx\r\n* Christofides algorithm\r\n* Sekanina's algorithm\r\n\r\n\r\n# Appendix: proofs\r\n\r\n(TODO: write the actual proofs)\r\n\r\n* HC $$\\le\\_p$$ RHC\r\n* RHC $$\\le\\_p$$ SUBOPT\\_TSP\r\n\r\n\r\n", "child": 168, "article": 10}}, {"pk": 173, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-13T13:00:16.915Z", "edited_by": 9, "parent": 172, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n  $$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string ($$\\epsilon$$) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols $$\\alpha$$ is the set of terminals that begin the strings derived from $$\\alpha$$. FIRST($$\\alpha$$) can be constructed like this for each symbol $$X$$ in $$\\alpha$$:\r\n\r\n1. If $$X$$ is a terminal, FIRST($$X$$) = {$$X$$}.\r\n2. If $$X \\rightarrow \\epsilon$$ is a production, add $$\\epsilon$$ to FIRST($$X$$)\r\n3. If $$X \\rightarrow Y\\_1 Y\\_2 ... Y\\_k$$ then add FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) to FIRST($$X$$)\r\n4. FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) is one of:\r\n    * FIRST($$Y\\_1$$) if FIRST($$Y\\_1$$) doesn't contain $$\\epsilon$$.\r\n    * FIRST($$Y\\_1$$) + FIRST($$Y\\_2 ... Y\\_k$$) - {$$\\epsilon$$}\r\n\r\nIf FIRST($$Y_i$$) contains $$\\epsilon$$ for all $$0 \\le i \\le k \\in \\mathbb{\\integers}$$, add $$\\epsilon$$ to FIRST($$X$$)\r\n\r\nThe **FOLLOW** set of a nonterminal $$A$$ is the set of terminals $$a$$ that can appear immediately to the right of $$A$$. In other words: the set of terminals $$a$$ such that there exists a derivation of the form $$S \\rightarrow \\alpha A a \\beta$$. FOLLOW($$A$$) for all nonterminals $$A$$ can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW($$S$$), where $$S$$ is the start symbol.\r\n2. If there is a production $$A \\rightarrow \\alpha B \\beta$$, then everything in FIRST($$\\beta$$) - {$$\\epsilon$$} is placed in FOLLOW($$B$$)\r\n3. If there is a production $$A \\rightarrow \\alpha B$$ or a production $$A \\rightarrow \\alpha B \\beta$$ where FIRST($$\\beta$$) contains $$\\epsilon$$, everything in FOLLOW($$A$$) is in FOLLOW($$B$$).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites.\r\n\r\nShift-reduce parsing is a category of commonly used methods for bottom-up parsing. Shift-reduce is based on two basic steps: shift and reduce. Shift advances in the input stream by one token, making the new symbol a new single-node parse tree. Reduce applies a completed grammar rule to one the recent parse trees, joining them together as one tree with a new root symbol.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. LR-parsing is a shift-reduce parsing method. Canonical LR-parsing, though fast, is prohibitively spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n\r\n\r\n\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $$dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 174, "article": 5}}, {"pk": 130, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T09:21:25.204Z", "edited_by": 3, "parent": 129, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = { a\\_1, a\\_2, ..., a\\_k }$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n* Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals ** lsjifasdfw ** and ** areighifpo ** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n* Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: ** lsjifhifpo ** could mutate to ** lsjifaifpo **, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n* (regrouping)\r\n* (colonization-extinction)\r\n* (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 131, "article": 10}}, {"pk": 174, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-13T13:00:50.860Z", "edited_by": 9, "parent": 173, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n  $$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string ($$\\epsilon$$) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols $$\\alpha$$ is the set of terminals that begin the strings derived from $$\\alpha$$. FIRST($$\\alpha$$) can be constructed like this for each symbol $$X$$ in $$\\alpha$$:\r\n\r\n1. If $$X$$ is a terminal, FIRST($$X$$) = {$$X$$}.\r\n2. If $$X \\rightarrow \\epsilon$$ is a production, add $$\\epsilon$$ to FIRST($$X$$)\r\n3. If $$X \\rightarrow Y\\_1 Y\\_2 ... Y\\_k$$ then add FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) to FIRST($$X$$)\r\n4. FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) is one of:\r\n    * FIRST($$Y\\_1$$) if FIRST($$Y\\_1$$) doesn't contain $$\\epsilon$$.\r\n    * FIRST($$Y\\_1$$) + FIRST($$Y\\_2 ... Y\\_k$$) - {$$\\epsilon$$}\r\n\r\nIf FIRST($$Y_i$$) contains $$\\epsilon$$ for all $$0 \\le i \\le k \\in \\mathbb{\\integers}$$, add $$\\epsilon$$ to FIRST($$X$$)\r\n\r\nThe **FOLLOW** set of a nonterminal $$A$$ is the set of terminals $$a$$ that can appear immediately to the right of $$A$$. In other words: the set of terminals $$a$$ such that there exists a derivation of the form $$S \\rightarrow \\alpha A a \\beta$$. FOLLOW($$A$$) for all nonterminals $$A$$ can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW($$S$$), where $$S$$ is the start symbol.\r\n2. If there is a production $$A \\rightarrow \\alpha B \\beta$$, then everything in FIRST($$\\beta$$) - {$$\\epsilon$$} is placed in FOLLOW($$B$$)\r\n3. If there is a production $$A \\rightarrow \\alpha B$$ or a production $$A \\rightarrow \\alpha B \\beta$$ where FIRST($$\\beta$$) contains $$\\epsilon$$, everything in FOLLOW($$A$$) is in FOLLOW($$B$$).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites.\r\n\r\nShift-reduce parsing is a category of commonly used methods for bottom-up parsing. Shift-reduce is based on two basic steps: shift and reduce. Shift advances in the input stream by one token, making the new symbol a new single-node parse tree. Reduce applies a completed grammar rule to one the recent parse trees, joining them together as one tree with a new root symbol.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. LR-parsing is a shift-reduce parsing method. Canonical LR-parsing, though fast, is prohibitively spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n\r\n\r\n\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $$d$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 175, "article": 5}}, {"pk": 181, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-14T11:18:30.888Z", "edited_by": 23, "parent": 169, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook, and even this compendium!). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n# Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP, with the restriction that the triangle inequality holds, can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(_x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n## The Primal-Dual schema\r\n\r\n(taken from cmu.edu)\r\n\r\nWe typically devise algorithms for minimization problems in the following way:\r\n\r\n1. Write down an LP relaxation of the problem, and find its dual. Try to find some intuitive meaning for the dual variables.\r\n2. Start with vectors x = 0, y = 0, which will be dual feasible, but primal infeasible.\r\n\r\n3. Until the primal is feasible:\r\n    1. increase the dual values y\\_i in some controlled fashion until some dual constraint(s) goes tight (i.e. until sum\\_i y_i * a\\_ij = c\\_j for some j), while always maintaining the deal feasibility of y.\r\n    2. Select some subset of the tight dual constraints, and increase the primal variable corresponding to them by an integral amount.\r\n\r\n4. For the analysis, prove that the output pair of vectors (x,y) satisfies c^[trans] * x <= rho * y^[trans] * b for as small a value of rho as possible. Keep this goal in mind when deciding how to raise the dual and primal variables.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$, where the edge weights satisfy the triangle inequality.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP with $$\\rho = 1.5$$. Why does this approximate TSP with $$\\rho = 1.5$$, you ask? Well:\r\n\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!\r\n\r\n\r\n# Appendix: list of algorithms featured in the textbook\r\n\r\n* DPKP\r\n* Ford-Fulkerson\r\n* Vertex cover algorithm\r\n* B&B for MAXSAT and TSP\r\n* D&C-3SAT\r\n* Local search scheme\r\n* Kernigan-Lin variable-depth search\r\n* Metropolis algorithm\r\n* Simulated Annealing\r\n* Tabu search\r\n* Randomized tabu search\r\n* Genetic algorithm scheme\r\n* Methods for relaxing problems to LP\r\n* Simplex\r\n* Primal-dual scheme\r\n* Primal-dual (MINVCP)\r\n* Greedy makespan schedule\r\n* Algorithm for greedy makespan schedule\r\n* Algorithm for VCP\r\n* Algorithm for SCP\r\n* Algorithm for WEIGHT-VCP\r\n* Algorithm for MAX-CUT\r\n* Greedy-simple KP\r\n* PTAS for SKP\r\n* modified PTAS for SKP\r\n* FPTAS for KP\r\n* TSP \u25b3-ineq 2-approx\r\n* Christofides algorithm\r\n* Sekanina's algorithm\r\n\r\n\r\n# Appendix: proofs\r\n\r\n(TODO: write the actual proofs)\r\n\r\n* HC $$\\le\\_p$$ RHC\r\n* RHC $$\\le\\_p$$ SUBOPT\\_TSP\r\n\r\n\r\n", "child": 183, "article": 10}}, {"pk": 136, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T17:18:28.523Z", "edited_by": 3, "parent": 134, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize $$sum cost(s) \\dot x\\_s for all s \\in S$$\r\n\r\nsubject to\r\n* $$sum x\\_s for s:e \\in S\\_s \\ge 1 for all e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 137, "article": 10}}, {"pk": 137, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T17:19:54.184Z", "edited_by": 3, "parent": 136, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 138, "article": 10}}, {"pk": 143, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T09:41:59.133Z", "edited_by": 3, "parent": 74, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n\r\n\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 144, "article": 5}}, {"pk": 192, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-16T13:54:55.535Z", "edited_by": 9, "parent": 191, "title": "Artificial Intelligence Methods", "content": "*   Strong/weak AI\r\n*   Agents\r\n    *   Rationality\r\n*   Bayesian networks\r\n    *   Syntax and semantics\r\n    *   Uses and problems\r\n    *   Conditional independence\r\n*   Decision networks/Influence diagrams\r\n*   Reasoning\r\n    *   Case-based reasoning\r\n    *   Instance-based reasoning\r\n*   Markov processes\r\n    *   Markov assumptiom\r\n*   Neural networks\r\n*   Decision trees\r\n\r\n# Bayesian Networks\r\n\r\nA Bayesian network is a model of a set of random variables and their conditional dependencies through a directed acyclic graph. In the graph, the nodes represent the random variables and the edges conditional dependencies.\r\n\r\nMuch of the calculation in a bayesian network is based on Bayes' theorem: $$ P(A|B)=\\frac{P(B|A)P(A)}{P(B)} $$.\r\n\r\n## Bayesian networks in use\r\n\r\n* Good: Reason with uncertain knowledge. \"Easy\" to set up with domain experts (medicine, diagnostics).\r\n* Worse: Many dependencies to set up for most meaningful networks.\r\n* Ugly: Non-discrete models\r\n\r\nBad at for example image analysis, ginormous networks.\r\n\r\n# Markov Chains\r\n\r\nA Markov chain is a random (and generally discrete) process where the next state only depends on the current state, and not previous states. There also exists higher-order markov chains, where the next state will depend on the previous n states. Note that any k'th-order Markov process can be expressed as a first order Markov process.\r\n\r\nIn a Markov chain, you typically have observable variables which tell us something about something we want to know, but can not observe directly.\r\n\r\n## Operations on Markov Chains\r\n\r\n### Filtering\r\n\r\nCalculating the unobservable variables based on the evidence (the observable variables).\r\n\r\n### Prediction\r\n\r\nTrying to predict how the variables will behave in the future based on the current evidence.\r\n\r\n### Smoothing\r\n\r\nEstimate past states including evidence from later states.\r\n\r\n### Most likely explanation\r\n\r\nFind the most likely set of states.\r\n\r\n## Bernoulli scheme\r\n\r\nA special case of Markov chains where the next state is independent of the current state.\r\n\r\n# Learning\r\n\r\n## Decision trees\r\n\r\nDecision trees are a way of making decision when the different cases are describable by attribute-value pairs. The target function should be discretely valued. Decision trees may also perform well with noisy training data.\r\n\r\nA decision tree is simply a tree where each internal node represents an attribute, and it's edges represent the different values that attribute may hold. In order to reach a decision, you follow the tree downwards the appropriate values until you reach a leaf node, in which case you have reached the decision.\r\n\r\n### Building decision trees\r\n\r\nThe general procedure for building decision is a simple recursive algorithm, where you select an attribute to split the training data on, and then continue until all the data is classified/there are no more attributes to split on. How big the decision tree is depends on how you select which attribute you split on.\r\n\r\nFor an optimal solution, you want to split on the attribute which holds the most information, the attribute which will reduce the entropy the most.\r\n\r\n#### Overfitting\r\n\r\nWith training sets that are large, you may end up incorporating the noise from the training data which increases the error rate of the decision tree. A way of avoiding this is to calculate how good the tree is at classifying data while removing every node (including the nodes below it), and then greedily remove the one that increases the accuracy the most.\r\n\r\n# Case-based reasoning\r\n\r\nCase-based reasoning assumes that similar problems have similar solutions and that what works today still will work tomorrow. It uses this as a framework to learn and deduce what to do in both known and unknown circumstances.\r\n\r\n## Main components\r\n\r\n* Case base\r\n  * Previous cases\r\n* A method for retrieving relevant cases with solutions\r\n* Method for adapting to current case\r\n* Method for learning from solved problem\r\n\r\n## Approaches\r\n\r\n### Instance-based\r\n\r\n*Answer:* Describe the four main steps of the case-based reasoning (CBR) cycle. What\r\nis the difference between \u201cinstance-based reasoning\u201d and \u201ctypical CBR\u201d?\r\n\r\nAn approach based on classical machine learning for *classification* tasks. Attribute-value pairs and a similarity metric. Focus on automated learning without intervention, requires large knowledge base.\r\n\r\n### Memory-based reasoning\r\n\r\nAs instance-based, only massively parallel(?), finds distance to every case in the database.\r\n\r\n### (Classic) Case-based reasoning\r\n\r\nMotivated by psychological research. Uses background-knowledge to specialise systems. Is able to adapt similar cases on a larger scale than instance-based.\r\n\r\n### Analogy-based\r\n\r\nSimilar to case-based, but with emphasis on solving cross-domain cases, reusing knowledge from other domains.", "child": null, "article": 9}}, {"pk": 150, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T12:43:14.093Z", "edited_by": 3, "parent": 149, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n    $$(\\sigma \\union N)*N(\\sigma \\union N)* \\rightarrow (\\sigma \\union \\N)*\r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Parsing\r\n\r\n\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n* alias analysis\r\n* pointer analysis\r\n* shape analysis\r\n* escape analysis\r\n* array access analysis\r\n* dependence analysis\r\n* control flow analysis\r\n* data flow analysis\r\n    * use-define chain analysis\r\n    * live variable analysis\r\n    * available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 151, "article": 5}}, {"pk": 156, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T14:55:33.942Z", "edited_by": 3, "parent": 155, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n    $$(\\sigma \\union N)*N(\\sigma \\union N)* \\rightarrow (\\sigma \\union \\N)*$$\r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string is a valid input, allowing for \"spontaneous\" transitions between states without input.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## shift/reduce\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 157, "article": 5}}, {"pk": 194, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-16T14:29:38.255Z", "edited_by": 3, "parent": 193, "title": "Teknologiledelse", "content": "# Hvordan ikke stryke i tekled\r\n\r\n1. Gj\u00f8r oppgaver fra webutvilking.org/tekled\r\n2. Pugg matteformlene og regnereglene under\r\n3. (ikke forsov deg til eksamen)\r\n\r\n# Formler\r\n\r\nTotalkostnad = TC = PQ\r\n\r\nPris = P = en eller annen $$f(Q)$$", "child": 197, "article": 13}}, {"pk": 96, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T13:55:36.248Z", "edited_by": 3, "parent": 95, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n\r\n", "child": 97, "article": 10}}, {"pk": 67, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-07T11:02:32.304Z", "edited_by": 9, "parent": 55, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * $$ Regular \\subseteq Context-free \\subseteq Context-sensitive \\subseteq Unrestricted $$\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n      * The dangling else problem\r\n* Optimizations\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n  * Interference graphs\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 68, "article": 5}}, {"pk": 162, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-12T07:39:37.873Z", "edited_by": 3, "parent": 161, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n$$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string ($$\\epsilon$$) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols $$\\alpha$$ is the set of terminals that begin the strings derived from $$\\alpha$$. FIRST($$\\alpha$$) can be constructed like this for each symbol $$X$$ in $$\\alpha$$:\r\n\r\n1. If $$X$$ is a terminal, FIRST($$X$$) = {$$X$$}.\r\n2. If $$X \\rightarrow \\epsilon$$ is a production, add $$\\epsilon$$ to FIRST($$X$$)\r\n3. If $$X \\rightarrow Y\\_1 Y\\_2 ... Y\\_k$$ then add FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) to FIRST($$X$$)\r\n4. FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) is one of:\r\n    * FIRST($$Y\\_1$$) if FIRST($$Y\\_1) doesn't contain $$\\epsilon$$.\r\n    * FIRST($$Y\\_1$$) + FIRST($$Y\\_2 ... Y\\_k$$) - {$$\\epsilon$$}\r\n\r\nIf FIRST($$Y_i$$) contains $$\\epsilon$$ for all $$0 \\le i \\le k \\in \\mathbb{\\integers}$$, add $$\\epsilon$$ to FIRST($$X$$)\r\n\r\nThe **FOLLOW** set of a nonterminal $$A$$ is the set of terminals $$a$$ that can appear immediately to the right of $$A$$. In other words: the set of terminals $$a$$ such that there exists a derivation of the form $$S \\rightarrow \\alpha A a \\beta$$. FOLLOW($$A$$) for all nonterminals $$A$$ can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW($$S$$), where $$S$$ is the start symbol.\r\n2. If there is a production $$A \\rightarrow \\alpha B \\beta, then everything in FIRST($$\\beta$$) - {$$\\epsilon$$} is placed in FOLLOW($$B$$)\r\n3. If there is a production $$A \\rightarrow \\alpha B$$ or a production $$A \\rightarrow \\alpha B \\betawhere FIRST($$\\beta$$) contains $$\\epsilon$$, everything in FOLLOW($$A$$) is in FOLLOW($$B$$).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## shift/reduce\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 163, "article": 5}}, {"pk": 68, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-07T11:03:05.809Z", "edited_by": 9, "parent": 67, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.# Recommended reading\r\n* Language classification\r\n * Strong and weak typing\r\n * Dynamic and static typing\r\n* Grammars and parsing\r\n * $$ Regular \\subseteq Context-free \\subseteq Context-sensitive \\subseteq Unrestricted $$\r\n * FIRST and FOLLOW sets\r\n * LL-parsing\r\n  * Left-recursive grammars\r\n * LR-parsing\r\n  * SLR\r\n  * LALR\r\n * Handling ambiguity\r\n  * Operator precedence\r\n  * Shift/reduce\r\n   * The dangling else problem\r\n* Optimizations\r\n * Data-flow analysis\r\n  * Control flow graphs\r\n  * Interference graphs\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 69, "article": 5}}, {"pk": 103, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T17:25:19.694Z", "edited_by": 3, "parent": 102, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n** || Complexity class || Model of computation             || Resource constraint || **\r\n   || DTIME(_f_(_n_))  || Deterministic Turing machine     || Time _f_(_n_)       ||\r\n   || P                || Deterministic Turing machine     || Time poly(_n_)      ||\r\n   || EXPTIME          || Deterministic Turing machine     || Time 2^(poly(_n_))  ||\r\n   || NTIME(_f_(_n_))  || Non-deterministic Turing machine || Time _f_(_n_)       ||\r\n   || NP               || Non-deterministic Turing machine || Time poly(_n_)      ||\r\n   || NEXPTIME         || Non-deterministic Turing machine || Time 2^(poly(_n_))  ||\r\n\r\n## Space\r\n\r\n** || Complexity class || Model of computation             || Resource constraint || **\r\n   || DSPACE(_f_(_n_)) || Deterministic Turing machine     || Space _f_(_n_)      ||\r\n   || L                || Deterministic Turing machine     || Space O(log _n_)    ||\r\n   || PSPACE           || Deterministic Turing machine     || Space poly(_n_)     ||\r\n   || EXPSPACE         || Deterministic Turing machine     || Space 2^(poly(_n_)) ||\r\n   || NSPACE(_f_(_n_)) || Non-deterministic Turing machine || Space _f_(_n_)      ||\r\n   || NL               || Non-deterministic Turing machine || Space O(log _n_)    ||\r\n   || NPSPACE          || Non-deterministic Turing machine || Space poly(_n_)     ||\r\n   || NEXPSPACE        || Non-deterministic Turing machine || Space 2^(poly(_n_)) ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\n## Straight up regular local search\r\n## Variable-depth search\r\n## Kernighan-Lin variable depth search\r\n## Simulated annealing\r\n## Tabu search\r\n## Randomized Tabu search\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 104, "article": 10}}, {"pk": 69, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-07T11:03:17.968Z", "edited_by": 9, "parent": 68, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Recommended reading\r\n* Language classification\r\n * Strong and weak typing\r\n * Dynamic and static typing\r\n* Grammars and parsing\r\n * $$ Regular \\subseteq Context-free \\subseteq Context-sensitive \\subseteq Unrestricted $$\r\n * FIRST and FOLLOW sets\r\n * LL-parsing\r\n  * Left-recursive grammars\r\n * LR-parsing\r\n  * SLR\r\n  * LALR\r\n * Handling ambiguity\r\n  * Operator precedence\r\n  * Shift/reduce\r\n   * The dangling else problem\r\n* Optimizations\r\n * Data-flow analysis\r\n  * Control flow graphs\r\n  * Interference graphs\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 70, "article": 5}}, {"pk": 71, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-07T11:09:53.605Z", "edited_by": 9, "parent": 70, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Recommended reading\r\n* Language classification\r\n * Strong and weak typing\r\n * Dynamic and static typing\r\n* Grammars and parsing\r\n * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n * FIRST and FOLLOW sets\r\n * LL-parsing\r\n  * Left-recursive grammars\r\n * LR-parsing\r\n  * SLR\r\n  * LALR\r\n * Handling ambiguity\r\n  * Operator precedence\r\n  * Shift/reduce\r\n   * The dangling else problem\r\n* Optimizations\r\n * Data-flow analysis\r\n  * Control flow graphs\r\n    * Dominator trees\r\n  * Interference graphs\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 72, "article": 5}}, {"pk": 110, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T20:27:21.895Z", "edited_by": 3, "parent": 109, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search remembers the last _k_ visited candidate solutions, and avoids revisiting them. This prevents a possibility to be considered repeatedly, getting stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 111, "article": 10}}, {"pk": 90, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T13:17:06.030Z", "edited_by": 3, "parent": 89, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n## Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n\r\n\r\n\r\n", "child": 91, "article": 10}}, {"pk": 168, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-12T16:20:22.984Z", "edited_by": 3, "parent": 167, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook, and even this compendium!). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n# Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP, with the restriction that the triangle inequality holds, can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n## The Primal-Dual schema\r\n\r\n(taken from cmu.edu)\r\n\r\nWe typically devise algorithms for minimization problems in the following way:\r\n\r\n1. Write down an LP relaxation of the problem, and find its dual. Try to find some intuitive meaning for the dual variables.\r\n2. Start with vectors x = 0, y = 0, which will be dual feasible, but primal infeasible.\r\n\r\n3. Until the primal is feasible:\r\n    1. increase the dual values y\\_i in some controlled fashion until some dual constraint(s) goes tight (i.e. until sum\\_i y_i * a\\_ij = c\\_j for some j), while always maintaining the deal feasibility of y.\r\n    2. Select some subset of the tight dual constraints, and increase the primal variable corresponding to them by an integral amount.\r\n\r\n4. For the analysis, prove that the output pair of vectors (x,y) satisfies c^[trans] * x <= rho * y^[trans] * b for as small a value of rho as possible. Keep this goal in mind when deciding how to raise the dual and primal variables.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$, where the edge weights satisfy the triangle inequality.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP with $$\\rho = 1.5$$. Why does this approximate TSP with $$\\rho = 1.5$$, you ask? Well:\r\n\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!\r\n\r\n\r\n# Appendix: list of algorithms featured in the textbook\r\n\r\n* DPKP\r\n* Ford-Fulkerson\r\n* Vertex cover algorithm\r\n* B&B for MAXSAT and TSP\r\n* D&C-3SAT\r\n* Local search scheme\r\n* Kernigan-Lin variable-depth search\r\n* Metropolis algorithm\r\n* Simulated Annealing\r\n* Tabu search\r\n* Randomized tabu search\r\n* Genetic algorithm scheme\r\n* Methods for relaxing problems to LP\r\n* Simplex\r\n* Primal-dual scheme\r\n* Primal-dual (MINVCP)\r\n* Greedy makespan schedule\r\n* Algorithm for greedy makespan schedule\r\n* Algorithm for VCP\r\n* Algorithm for SCP\r\n* Algorithm for WEIGHT-VCP\r\n* Algorithm for MAX-CUT\r\n* Greedy-simple KP\r\n* PTAS for SKP\r\n* modified PTAS for SKP\r\n* FPTAS for KP\r\n* TSP $$\\delta$$-ineq 2-approx\r\n* Christofides algorithm\r\n* Sekanina's algorithm\r\n\r\n\r\n# Appendix: proofs\r\n\r\n(TODO: write the actual proofs)\r\n\r\n* HC $$\\le\\_p$$ RHC\r\n* RHC $$\\le\\_p$$ SUBOPT\\_TSP\r\n\r\n\r\n", "child": 169, "article": 10}}, {"pk": 117, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-09T11:10:20.951Z", "edited_by": 3, "parent": 116, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search remembers the last _k_ visited candidate solutions, and avoids revisiting them. This prevents a possibility to be considered repeatedly, getting stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 118, "article": 10}}, {"pk": 175, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-13T13:02:05.401Z", "edited_by": 9, "parent": 174, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) + (int), but disallow (str) + (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n  $$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string ($$\\epsilon$$) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols $$\\alpha$$ is the set of terminals that begin the strings derived from $$\\alpha$$. FIRST($$\\alpha$$) can be constructed like this for each symbol $$X$$ in $$\\alpha$$:\r\n\r\n1. If $$X$$ is a terminal, FIRST($$X$$) = {$$X$$}.\r\n2. If $$X \\rightarrow \\epsilon$$ is a production, add $$\\epsilon$$ to FIRST($$X$$)\r\n3. If $$X \\rightarrow Y\\_1 Y\\_2 ... Y\\_k$$ then add FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) to FIRST($$X$$)\r\n4. FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) is one of:\r\n    * FIRST($$Y\\_1$$) if FIRST($$Y\\_1$$) doesn't contain $$\\epsilon$$.\r\n    * FIRST($$Y\\_1$$) + FIRST($$Y\\_2 ... Y\\_k$$) - {$$\\epsilon$$}\r\n\r\nIf FIRST($$Y_i$$) contains $$\\epsilon$$ for all $$0 \\le i \\le k \\in \\mathbb{\\integers}$$, add $$\\epsilon$$ to FIRST($$X$$)\r\n\r\nThe **FOLLOW** set of a nonterminal $$A$$ is the set of terminals $$a$$ that can appear immediately to the right of $$A$$. In other words: the set of terminals $$a$$ such that there exists a derivation of the form $$S \\rightarrow \\alpha A a \\beta$$. FOLLOW($$A$$) for all nonterminals $$A$$ can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW($$S$$), where $$S$$ is the start symbol.\r\n2. If there is a production $$A \\rightarrow \\alpha B \\beta$$, then everything in FIRST($$\\beta$$) - {$$\\epsilon$$} is placed in FOLLOW($$B$$)\r\n3. If there is a production $$A \\rightarrow \\alpha B$$ or a production $$A \\rightarrow \\alpha B \\beta$$ where FIRST($$\\beta$$) contains $$\\epsilon$$, everything in FOLLOW($$A$$) is in FOLLOW($$B$$).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites.\r\n\r\nShift-reduce parsing is a category of commonly used methods for bottom-up parsing. Shift-reduce is based on two basic steps: shift and reduce. Shift advances in the input stream by one token, making the new symbol a new single-node parse tree. Reduce applies a completed grammar rule to one the recent parse trees, joining them together as one tree with a new root symbol.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. LR-parsing is a shift-reduce parsing method. Canonical LR-parsing, though fast, is prohibitively spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n\r\n\r\n\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $$d$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 176, "article": 5}}, {"pk": 131, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T09:22:04.443Z", "edited_by": 3, "parent": 130, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\{a\\_1, a\\_2, ..., a\\_k\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n* Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals ** lsjifasdfw ** and ** areighifpo ** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n* Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: ** lsjifhifpo ** could mutate to ** lsjifaifpo **, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n* (regrouping)\r\n* (colonization-extinction)\r\n* (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 132, "article": 10}}, {"pk": 182, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-14T15:16:48.994Z", "edited_by": 2, "parent": 49, "title": "Data Modelling and Database Systems", "content": "# Types of failures\r\n\r\nFailures of type 1-4 are common, and the system is expected to automatically recover from these. Failures of type 5 and 6 are less common, and recovering from them can be a major undertaking. \r\n\r\n1.  A computer failure / crash\r\n2.  A transaction or system error\r\n3.  Local errors or exception conditions detected by the transaction\r\n4.  Concurrency control enforcement\r\n5.  Disk failure\r\n6.  Physical problems and catastrophes\r\n\r\n# Database storage stuctures\r\n\r\nDatabases are typically stored on secondary (non-volatile) storage.\r\n\r\n## List of storage structures\r\n\r\n|| Heap file || The simplest storage structure: an unordered list of records. Records are inserted at the end of the heap file, giving O(1) insertion. Retrieval requires a linear search ( O(n) ), and is therefore quite inefficient for large values of n. Deleted records are only marked as deleted, and not actually removed. This is O(1) (in addition to the time it takes to retrieve a record to be deleted). Highly volatile heap-files must be re-structured often to prune away records that are marked as deleted. ||\r\n|| Hash buckets || Hash functions directly calculate the page address of a record based on values in the record. A good hashing function has an even spread across the address space. When doing calculations on hash buckets (say, for an exam), it is safe to assume about a 60% fill rate. Ignoring collisions, hash buckets are O(1) for retrieval, insertion, modification and deletion. Hash buckets often suck for range retrieval. ||\r\n|| B+-trees || A pretty good all-around storage structure. Retrieval, insertion, modification and deletion is O(log n). Good for volatile data, as the index dynamically expands and contracts as the data set grows and shrinks. Less optimal for non-volatile/stable files - use ISAM for that. ||\r\n|| ISAM || (Indexed Sequential Access Method) ||\r\n\r\n## Hashing schemes\r\n\r\n|| Extendible hashing || Extendible hashing treats a hash as a bit string, and uses a trie for bucket lookup. Re-hashing is done incrementally, one bucket at a time, as needed. Both LSB and MSB implementations are common. ||\r\n|| Linear hashing || Linear hashing allows for expansion of the hash table one slot at a time, round-robin. ||\r\n\r\n# Transactions\r\n\r\nA transaction is a unit of work performed within a DBMS. They provide reliability and isolation. Transactions are \"all or nothing\", in the sense that the entire unit of work must complete without error, or have no effect at all.\r\n\r\n## ACID: Atomicitiy, consistency, isolation and durability\r\n\r\nDatabases must conform to the ACID principles:\r\n\r\n|| Atomicity || A unit of work cannot be split into smaller parts\u2014a unit of work can either complete entirely, or fail entirely. ||\r\n|| Consistency || A transaction always leaves the database in a valid state upon completion, with all data valid according to defined rules such as constraints, triggers, cascades etc. ||\r\n|| Isolation || No transaction can interfere with another transaction. ||\r\n|| Durability || When a transaction completes, it remains complete, even in the case of power loss, crashes etc. ||\r\n\r\nA typical transaction follows the following lifecycle:\r\n\r\n1.  Begin the transaction\r\n2.  Execute a set of data manipulations/queries\r\n3.  If no errors occur, then commit the transaction and end it\r\n4.  If errors occur, then rollback the transaction and end it\r\n\r\n## Policies used in transaction control\r\n\r\nThe following are four policies that can be used in transaction control in a database:\r\n\r\n|| NO-FORCE policy || Changes made to objects are not required to be written to disk in-place. Changes must still be logged to maintain durability, and can be applied to the object at a later time. This reduces seek-time on disk for a commit, as the log is usually sequentially stored in memory, and objects can be scattered. Frequently updated objects can also merge accumulated writes from the log, thereby reducing the total number of writes on the object. Faster, but adds rollback complexity. ||\r\n|| FORCE policy || Changes made to objects are required to be written to disk in-place. Sucks. ||\r\n|| STEAL policy || Allows a transaction to be written on a nonvolatile storage before its commit occurs. Faster, but adds rollback complexity. ||\r\n|| NO-STEAL policy || Does not allow a transaction to be written on a nonvolative storage before its commit occurs. Sucks. ||\r\n\r\nPerformance wise, if anyone asks, STEAL NO-FORCE is the way to go.\r\n\r\n## The system log\r\n\r\nA DBMS maintains a system log tracking all transaction operations to be able to recover from failures. The log must be kept on non-volatile storage, for obvious reasons. A log consists of records of different types:\r\n\r\n1.  [start\\_transaction, transaction\\_id]\r\n2.  [write\\_item, transaction\\_id, item\\_id, old\\_value, new_value]\r\n3.  [read\\_item, transaction\\_id, item_id]\r\n4.  [commit, transaction_id]\r\n5.  [abort, transaction_id]\r\n\r\n## ARIES (aka Algorithms for Recovery and Isolation Exploiting Semantics)\r\n\r\nRecovery algorithm designed to work with a no-force, steal database approach. ARIES has three main principles:\r\n\r\n|| Write ahead logging || Any change to an object is first recorded in the log, and the log must be written to non-volatile storage before changes to the object are made. ||\r\n|| Repeating history during redo || On restart after a crash, ARIES retraces the steps in history necessary to bring the database to the exact state at the time of the crash, and then undos the transactions that were still active at the time of the crash. ||\r\n|| Logging changes during undo || Changes made to the database during undo are logged to prevent unwanted repeats. ||\r\n\r\nTwo datastructures must be maintained to gather enough information for logging: the dirty page table (DPT) and the transaction table (TT):\r\n\r\n|| Dirty page table (DPT) || Keeps record of all the pages that have been modified and not yet written to disk. It also stores the sequence number of the first sequence that made a page dirty (recLSN). ||\r\n|| Transaction table (TT) || Contains all transactions that are currently running, as well as the sequence number of the last sequence that caused an entry in the log. ||\r\n\r\nARIES periodically saves the DPT and the TT to the log file, creating checkpoints, to avoid rescanning the entire log file in the case of a redo. This allows for skipping blocks that are not in the DPT, or that are in the DPT but have a recLSN that is greater than the logg post LSN.\r\n\r\n## Schedule (aka History)\r\n\r\nSchedules are lists of operations (usually data reads/writes, locks, commits etc) ordered by time. Schedules can be classified into different classes:\r\n\r\n|| Serial || Transactions are non-interleaved. No transaction starts before the previous one has ended. Example: D = R1(X)W1(X)C1W(2)C2. ||\r\n|| Serializable || Schedules that have equivalent outcomes to a serial schedule are serializable. A schedule is serializable if and only if its precedence graph is acyclic ||\r\n|| Recoverable || Transactions commit only after all transactions whose changes they read, commit. ||\r\n|| Unrecoverable || A schedule which is not recoverable ||\r\n|| ACA (Avoids Cascading Aborts, aka Cascadeless) || Transactions may not read uncommitted changes from another transaction during the same schedule. All ACA are recoverable. ||\r\n|| Strict || Write operations in one transaction which precedes a conflicting operation in another transaction must be committed before the conflicting operation in the second transaction occurs. All strict schedules are ACA. ||\r\n\r\n# Database normalization\r\n\r\nDatabase normalization is the process of organizing the fields and tables of a relational database to minimalize redundancy and dependency.\r\n\r\n## Normal forms (aka NF)\r\n\r\nA normal form is a set of criteria for determining a table's degree of vulnerability to logical inconsistencies and anomalies. The most common normal forms are:\r\n\r\n|| 1NF  || A table represents a relation and has no repeating groups. ||\r\n|| 2NF  || No non-prime attribute in the table is partially dependent on any candiate key. ||\r\n|| 3NF  || Every non-prime attribute in the table is directly dependent on every candidate key. ||\r\n|| BCNF || Every non-trivial functional dependency is a dependency on a superkey. ||\r\n\r\nNote that to meet the requirements of a normal form, a table must also meet all the requirementes of the lower normal forms.\r\n\r\n# Relational algebra\r\n\r\n## Primitive operators\r\n\r\n|| Projection (\u03c0) || The projection of a relation R on a set of attributes A is the set of all tuples in R restricted to the set of attributes in A. Basically the SELECT of relational algebra. ||\r\n|| Selection (\u03c3) || A selection with the set of conditions C selects all the tuples in a relation R such that the conditions in C hold. Basically the WHERE of relational algebra. ||\r\n|| Rename (\u03c1) || Renames attributes in a relation. Basically the AS of relational algebra. ||\r\n|| Natural Join (\u22c8) || Joins two relations R and S. The result of the join is the set of all combinations of tuples in R and S that are equal on their common attribute names. Basically the INNER JOIN of relational algebra.||", "child": null, "article": 4}}, {"pk": 49, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-03-06T13:32:51.765Z", "edited_by": 2, "parent": 48, "title": "Data Modelling and Database Systems", "content": "# Types of failures\r\n\r\nFailures of type 1-4 are common, and the system is expected to automatically recover from these. Failures of type 5 and 6 are less common, and recovering from them can be a major undertaking. \r\n\r\n1.  A computer failure / crash\r\n2.  A transaction or system error\r\n3.  Local errors or exception conditions detected by the transaction\r\n4.  Concurrency control enforcement\r\n5.  Disk failure\r\n6.  Physical problems and catastrophes\r\n\r\n# Database storage stuctures\r\n\r\nDatabases are typically stored on secondary (non-volatile) storage.\r\n\r\n## List of storage structures\r\n\r\n|| Heap file || The simplest storage structure: an unordered list of records. Records are inserted at the end of the heap file, giving O(1) insertion. Retrieval requires a linear search ( O(n) ), and is therefore quite inefficient for large values of n. Deleted records are only marked as deleted, and not actually removed. This is O(1) (in addition to the time it takes to retrieve a record to be deleted). Highly volatile heap-files must be re-structured often to prune away records that are marked as deleted. ||\r\n|| Hash buckets || Hash functions directly calculate the page address of a record based on values in the record. A good hashing function has an even spread across the address space. When doing calculations on hash buckets (say, for an exam), it is safe to assume about a 60% fill rate. Ignoring collisions, hash buckets are O(1) for retrieval, insertion, modification and deletion. Hash buckets often suck for range retrieval. ||\r\n|| B%2B-trees || A pretty good all-around storage structure. Retrieval, insertion, modification and deletion is O(log n). Good for volatile data, as the index dynamically expands and contracts as the data set grows and shrinks. Less optimal for non-volatile/stable files - use ISAM for that. ||\r\n|| ISAM || (Indexed Sequential Access Method) ||\r\n\r\n## Hashing schemes\r\n\r\n|| Extendible hashing || Extendible hashing treats a hash as a bit string, and uses a trie for bucket lookup. Re-hashing is done incrementally, one bucket at a time, as needed. Both LSB and MSB implementations are common. ||\r\n|| Linear hashing || Linear hashing allows for expansion of the hash table one slot at a time, round-robin. ||\r\n\r\n# Transactions\r\n\r\nA transaction is a unit of work performed within a DBMS. They provide reliability and isolation. Transactions are \"all or nothing\", in the sense that the entire unit of work must complete without error, or have no effect at all.\r\n\r\n## ACID: Atomicitiy, consistency, isolation and durability\r\n\r\nDatabases must conform to the ACID principles:\r\n\r\n|| Atomicity || A unit of work cannot be split into smaller parts\u2014a unit of work can either complete entirely, or fail entirely. ||\r\n|| Consistency || A transaction always leaves the database in a valid state upon completion, with all data valid according to defined rules such as constraints, triggers, cascades etc. ||\r\n|| Isolation || No transaction can interfere with another transaction. ||\r\n|| Durability || When a transaction completes, it remains complete, even in the case of power loss, crashes etc. ||\r\n\r\nA typical transaction follows the following lifecycle:\r\n\r\n1.  Begin the transaction\r\n2.  Execute a set of data manipulations/queries\r\n3.  If no errors occur, then commit the transaction and end it\r\n4.  If errors occur, then rollback the transaction and end it\r\n\r\n## Policies used in transaction control\r\n\r\nThe following are four policies that can be used in transaction control in a database:\r\n\r\n|| NO-FORCE policy || Changes made to objects are not required to be written to disk in-place. Changes must still be logged to maintain durability, and can be applied to the object at a later time. This reduces seek-time on disk for a commit, as the log is usually sequentially stored in memory, and objects can be scattered. Frequently updated objects can also merge accumulated writes from the log, thereby reducing the total number of writes on the object. Faster, but adds rollback complexity. ||\r\n|| FORCE policy || Changes made to objects are required to be written to disk in-place. Sucks. ||\r\n|| STEAL policy || Allows a transaction to be written on a nonvolatile storage before its commit occurs. Faster, but adds rollback complexity. ||\r\n|| NO-STEAL policy || Does not allow a transaction to be written on a nonvolative storage before its commit occurs. Sucks. ||\r\n\r\nPerformance wise, if anyone asks, STEAL NO-FORCE is the way to go.\r\n\r\n## The system log\r\n\r\nA DBMS maintains a system log tracking all transaction operations to be able to recover from failures. The log must be kept on non-volatile storage, for obvious reasons. A log consists of records of different types:\r\n\r\n1.  [start\\_transaction, transaction\\_id]\r\n2.  [write\\_item, transaction\\_id, item\\_id, old\\_value, new_value]\r\n3.  [read\\_item, transaction\\_id, item_id]\r\n4.  [commit, transaction_id]\r\n5.  [abort, transaction_id]\r\n\r\n## ARIES (aka Algorithms for Recovery and Isolation Exploiting Semantics)\r\n\r\nRecovery algorithm designed to work with a no-force, steal database approach. ARIES has three main principles:\r\n\r\n|| Write ahead logging || Any change to an object is first recorded in the log, and the log must be written to non-volatile storage before changes to the object are made. ||\r\n|| Repeating history during redo || On restart after a crash, ARIES retraces the steps in history necessary to bring the database to the exact state at the time of the crash, and then undos the transactions that were still active at the time of the crash. ||\r\n|| Logging changes during undo || Changes made to the database during undo are logged to prevent unwanted repeats. ||\r\n\r\nTwo datastructures must be maintained to gather enough information for logging: the dirty page table (DPT) and the transaction table (TT):\r\n\r\n|| Dirty page table (DPT) || Keeps record of all the pages that have been modified and not yet written to disk. It also stores the sequence number of the first sequence that made a page dirty (recLSN). ||\r\n|| Transaction table (TT) || Contains all transactions that are currently running, as well as the sequence number of the last sequence that caused an entry in the log. ||\r\n\r\nARIES periodically saves the DPT and the TT to the log file, creating checkpoints, to avoid rescanning the entire log file in the case of a redo. This allows for skipping blocks that are not in the DPT, or that are in the DPT but have a recLSN that is greater than the logg post LSN.\r\n\r\n## Schedule (aka History)\r\n\r\nSchedules are lists of operations (usually data reads/writes, locks, commits etc) ordered by time. Schedules can be classified into different classes:\r\n\r\n|| Serial || Transactions are non-interleaved. No transaction starts before the previous one has ended. Example: D = R1(X)W1(X)C1W(2)C2. ||\r\n|| Serializable || Schedules that have equivalent outcomes to a serial schedule are serializable. A schedule is serializable if and only if its precedence graph is acyclic ||\r\n|| Recoverable || Transactions commit only after all transactions whose changes they read, commit. ||\r\n|| Unrecoverable || A schedule which is not recoverable ||\r\n|| ACA (Avoids Cascading Aborts, aka Cascadeless) || Transactions may not read uncommitted changes from another transaction during the same schedule. All ACA are recoverable. ||\r\n|| Strict || Write operations in one transaction which precedes a conflicting operation in another transaction must be committed before the conflicting operation in the second transaction occurs. All strict schedules are ACA. ||\r\n\r\n# Database normalization\r\n\r\nDatabase normalization is the process of organizing the fields and tables of a relational database to minimalize redundancy and dependency.\r\n\r\n## Normal forms (aka NF)\r\n\r\nA normal form is a set of criteria for determining a table's degree of vulnerability to logical inconsistencies and anomalies. The most common normal forms are:\r\n\r\n|| 1NF  || A table represents a relation and has no repeating groups. ||\r\n|| 2NF  || No non-prime attribute in the table is partially dependent on any candiate key. ||\r\n|| 3NF  || Every non-prime attribute in the table is directly dependent on every candidate key. ||\r\n|| BCNF || Every non-trivial functional dependency is a dependency on a superkey. ||\r\n\r\nNote that to meet the requirements of a normal form, a table must also meet all the requirementes of the lower normal forms.\r\n\r\n# Relational algebra\r\n\r\n## Primitive operators\r\n\r\n|| Projection (\u03c0) || The projection of a relation R on a set of attributes A is the set of all tuples in R restricted to the set of attributes in A. Basically the SELECT of relational algebra. ||\r\n|| Selection (\u03c3) || A selection with the set of conditions C selects all the tuples in a relation R such that the conditions in C hold. Basically the WHERE of relational algebra. ||\r\n|| Rename (\u03c1) || Renames attributes in a relation. Basically the AS of relational algebra. ||\r\n|| Natural Join (\u22c8) || Joins two relations R and S. The result of the join is the set of all combinations of tuples in R and S that are equal on their common attribute names. Basically the INNER JOIN of relational algebra.||", "child": 182, "article": 4}}, {"pk": 138, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T17:48:53.279Z", "edited_by": 3, "parent": 137, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n# Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 139, "article": 10}}, {"pk": 145, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T10:15:04.971Z", "edited_by": 3, "parent": 144, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\n## The Chomsky hierarchy\r\n\r\nRegular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n# Parsing\r\n\r\n\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n# Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n## LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n# FIRST and FOLLOW sets\r\n# Handling ambiguity\r\n# Operator precedence\r\n# The dangling else problem\r\n\r\n\r\n\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 146, "article": 5}}, {"pk": 151, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T12:48:02.053Z", "edited_by": 3, "parent": 150, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n    $$(\\sigma \\union N)*N(\\sigma \\union N)* \\rightarrow (\\sigma \\union \\N)*\r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Parsing\r\n\r\n\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### use-define chain analysis\r\n###### live variable analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 152, "article": 5}}, {"pk": 157, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T14:56:56.344Z", "edited_by": 3, "parent": 156, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n$$(\\sigma \\union N)*N(\\sigma \\union N)* \\rightarrow (\\sigma \\union \\N)*$$\r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string is a valid input, allowing for \"spontaneous\" transitions between states without input.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## shift/reduce\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 158, "article": 5}}, {"pk": 70, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-07T11:04:09.540Z", "edited_by": 9, "parent": 69, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Recommended reading\r\n* Language classification\r\n * Strong and weak typing\r\n * Dynamic and static typing\r\n* Grammars and parsing\r\n * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n * FIRST and FOLLOW sets\r\n * LL-parsing\r\n  * Left-recursive grammars\r\n * LR-parsing\r\n  * SLR\r\n  * LALR\r\n * Handling ambiguity\r\n  * Operator precedence\r\n  * Shift/reduce\r\n   * The dangling else problem\r\n* Optimizations\r\n * Data-flow analysis\r\n  * Control flow graphs\r\n  * Interference graphs\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 71, "article": 5}}, {"pk": 97, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T15:16:42.988Z", "edited_by": 3, "parent": 96, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\n## Straight up regular local search\r\n## Variable-depth search\r\n## Kernighan-Lin variable depth search\r\n## Simulated annealing\r\n## Tabu search\r\n## Randomized Tabu search\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n\r\n", "child": 98, "article": 10}}, {"pk": 72, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-07T11:14:45.508Z", "edited_by": 9, "parent": 71, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n  * Left-recursive grammars\r\n  * LR-parsing\r\n  * SLR\r\n  * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Dominator trees\r\n    * Interference graphs\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 73, "article": 5}}, {"pk": 163, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-12T07:40:03.498Z", "edited_by": 3, "parent": 162, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n$$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string ($$\\epsilon$$) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols $$\\alpha$$ is the set of terminals that begin the strings derived from $$\\alpha$$. FIRST($$\\alpha$$) can be constructed like this for each symbol $$X$$ in $$\\alpha$$:\r\n\r\n1. If $$X$$ is a terminal, FIRST($$X$$) = {$$X$$}.\r\n2. If $$X \\rightarrow \\epsilon$$ is a production, add $$\\epsilon$$ to FIRST($$X$$)\r\n3. If $$X \\rightarrow Y\\_1 Y\\_2 ... Y\\_k$$ then add FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) to FIRST($$X$$)\r\n4. FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) is one of:\r\n    * FIRST($$Y\\_1$$) if FIRST($$Y\\_1$$) doesn't contain $$\\epsilon$$.\r\n    * FIRST($$Y\\_1$$) + FIRST($$Y\\_2 ... Y\\_k$$) - {$$\\epsilon$$}\r\n\r\nIf FIRST($$Y_i$$) contains $$\\epsilon$$ for all $$0 \\le i \\le k \\in \\mathbb{\\integers}$$, add $$\\epsilon$$ to FIRST($$X$$)\r\n\r\nThe **FOLLOW** set of a nonterminal $$A$$ is the set of terminals $$a$$ that can appear immediately to the right of $$A$$. In other words: the set of terminals $$a$$ such that there exists a derivation of the form $$S \\rightarrow \\alpha A a \\beta$$. FOLLOW($$A$$) for all nonterminals $$A$$ can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW($$S$$), where $$S$$ is the start symbol.\r\n2. If there is a production $$A \\rightarrow \\alpha B \\beta, then everything in FIRST($$\\beta$$) - {$$\\epsilon$$} is placed in FOLLOW($$B$$)\r\n3. If there is a production $$A \\rightarrow \\alpha B$$ or a production $$A \\rightarrow \\alpha B \\betawhere FIRST($$\\beta$$) contains $$\\epsilon$$, everything in FOLLOW($$A$$) is in FOLLOW($$B$$).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## shift/reduce\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 164, "article": 5}}, {"pk": 76, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T11:08:07.301Z", "edited_by": 3, "parent": 75, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n\u03a3_bool = {0, 1}\r\n\u03a3_latin = {a, b, c, d, e , ... , z }\r\n\u03a3_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }", "child": 77, "article": 10}}, {"pk": 104, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T17:27:16.070Z", "edited_by": 3, "parent": 103, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\n## Straight up regular local search\r\n## Variable-depth search\r\n## Kernighan-Lin variable depth search\r\n## Simulated annealing\r\n## Tabu search\r\n## Randomized Tabu search\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 105, "article": 10}}, {"pk": 81, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T11:25:09.104Z", "edited_by": 3, "parent": 80, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n", "child": 82, "article": 10}}, {"pk": 111, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-09T07:59:12.900Z", "edited_by": 3, "parent": 110, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search remembers the last _k_ visited candidate solutions, and avoids revisiting them. This prevents a possibility to be considered repeatedly, getting stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S_\\_n such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S_\\_n}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v_\\_n}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 112, "article": 10}}, {"pk": 91, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T13:18:24.033Z", "edited_by": 3, "parent": 90, "title": "Algorithm Construction, Advanced Course", "content": "y= 2*x\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n## Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n\r\n\r\n\r\n", "child": 92, "article": 10}}, {"pk": 118, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-09T18:58:37.463Z", "edited_by": 3, "parent": 117, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search remembers the last _k_ visited candidate solutions, and avoids revisiting them. This prevents a possibility to be considered repeatedly, getting stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized.\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 119, "article": 10}}, {"pk": 176, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-13T13:19:52.986Z", "edited_by": 9, "parent": 175, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) + (int), but disallow (str) + (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar G = (N,\u03c3,P,S) consists of:\r\n\r\n* A finite set N of nonterminal symbols\r\n* A finite set \u03c3 of terminal symbols that is disjoint from N.\r\n* A finite set P of production rules, each rule on the form:\r\n  (\u03c3 \u222a N)* N(\u03c3 \u222a N)* \u2192 (\u03c3 \u222a N)* \r\n* A symbol S \u2208 N that is the start symbol\r\n\r\n\r\nThe language of a grammar G, L(G), is defined as all the sentences that can be derived in a finite number of steps from the start symbol S.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, \u03b5 or \u039b for the empty string, and S for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form V\u2192v, where V is a non-terminal and v may be a combination of terminals and non-terminals.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form \u03b1A\u03b2\u2192\u03b1\u03b3\u03b2, where \u03b1, \u03b2 and \u03b3 are strings of nonterminals and terminals, and \u03b3 is not \u03b5.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string (\u03b5) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols \u03b1 is the set of terminals that begin the strings derived from \u03b1. FIRST(\u03b1) can be constructed like this for each symbol X in \u03b1:\r\n\r\n1. If X is a terminal, FIRST(X) = {X}.\r\n2. If X\u2192\u03b5 is a production, add \u03b5 to FIRST(X)\r\n3. If X\u2192Y<sub>1</sub>Y<sub>2</sub>...Y<sub>k</sub> then add FIRST(Y<sub>1</sub>Y<sub>2</sub>...Y<sub>k</sub>) to FIRST(X)\r\n4. FIRST(Y<sub>1</sub>Y<sub>2</sub>...Y<sub>k</sub>) is one of:\r\n    * FIRST(Y<sub>1</sub>) if FIRST(Y<sub>1</sub>) doesn't contain \u03b5.\r\n    * FIRST(Y<sub>1</sub>) + FIRST(Y<sub>2</sub>...Y<sub>k</sub>) - {\u03b5}\r\n\r\nIf FIRST(Y<sub>i</sub>) contains \u03b5 for all 0\u2264i\u2264k \u2208 \u2124, add \u03b5 to FIRST(X)\r\n\r\nThe **FOLLOW** set of a nonterminal A is the set of terminals a that can appear immediately to the right of A. In other words: the set of terminals a such that there exists a derivation of the form S\u2192\u03b1Aa\u03b2. FOLLOW(A) for all nonterminals A can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW(S), where S is the start symbol.\r\n2. If there is a production A\u2192\u03b1B\u03b2, then everything in FIRST(\u03b2) - {\u03b5} is placed in FOLLOW(B)\r\n3. If there is a production A\u2192\u03b1B or a production A\u2192\u03b1B\u03b2 where FIRST(\u03b2) contains \u03b5, everything in FOLLOW(A) is in FOLLOW(B).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites.\r\n\r\nShift-reduce parsing is a category of commonly used methods for bottom-up parsing. Shift-reduce is based on two basic steps: shift and reduce. Shift advances in the input stream by one token, making the new symbol a new single-node parse tree. Reduce applies a completed grammar rule to one the recent parse trees, joining them together as one tree with a new root symbol.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. LR-parsing is a shift-reduce parsing method. Canonical LR-parsing, though fast, is prohibitively spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n\r\n\r\n\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression 3 + 2 can be replaced by the expression 5.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node d dominates a node n if every path from the start node to n must go through d. We write: d dom n, or d >> n. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 177, "article": 5}}, {"pk": 125, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T08:43:56.091Z", "edited_by": 3, "parent": 121, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n## Initialization\r\n## Selection\r\n## Genetic operators\r\n* Crossover/recombination\r\n* Mutation\r\n* (regrouping)\r\n* (colonization-extinction)\r\n* (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 126, "article": 10}}, {"pk": 169, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-12T16:41:48.838Z", "edited_by": 3, "parent": 168, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook, and even this compendium!). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n# Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP, with the restriction that the triangle inequality holds, can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(_x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n## The Primal-Dual schema\r\n\r\n(taken from cmu.edu)\r\n\r\nWe typically devise algorithms for minimization problems in the following way:\r\n\r\n1. Write down an LP relaxation of the problem, and find its dual. Try to find some intuitive meaning for the dual variables.\r\n2. Start with vectors x = 0, y = 0, which will be dual feasible, but primal infeasible.\r\n\r\n3. Until the primal is feasible:\r\n    1. increase the dual values y\\_i in some controlled fashion until some dual constraint(s) goes tight (i.e. until sum\\_i y_i * a\\_ij = c\\_j for some j), while always maintaining the deal feasibility of y.\r\n    2. Select some subset of the tight dual constraints, and increase the primal variable corresponding to them by an integral amount.\r\n\r\n4. For the analysis, prove that the output pair of vectors (x,y) satisfies c^[trans] * x <= rho * y^[trans] * b for as small a value of rho as possible. Keep this goal in mind when deciding how to raise the dual and primal variables.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$, where the edge weights satisfy the triangle inequality.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP with $$\\rho = 1.5$$. Why does this approximate TSP with $$\\rho = 1.5$$, you ask? Well:\r\n\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!\r\n\r\n\r\n# Appendix: list of algorithms featured in the textbook\r\n\r\n* DPKP\r\n* Ford-Fulkerson\r\n* Vertex cover algorithm\r\n* B&B for MAXSAT and TSP\r\n* D&C-3SAT\r\n* Local search scheme\r\n* Kernigan-Lin variable-depth search\r\n* Metropolis algorithm\r\n* Simulated Annealing\r\n* Tabu search\r\n* Randomized tabu search\r\n* Genetic algorithm scheme\r\n* Methods for relaxing problems to LP\r\n* Simplex\r\n* Primal-dual scheme\r\n* Primal-dual (MINVCP)\r\n* Greedy makespan schedule\r\n* Algorithm for greedy makespan schedule\r\n* Algorithm for VCP\r\n* Algorithm for SCP\r\n* Algorithm for WEIGHT-VCP\r\n* Algorithm for MAX-CUT\r\n* Greedy-simple KP\r\n* PTAS for SKP\r\n* modified PTAS for SKP\r\n* FPTAS for KP\r\n* TSP $$\\delta$$-ineq 2-approx\r\n* Christofides algorithm\r\n* Sekanina's algorithm\r\n\r\n\r\n# Appendix: proofs\r\n\r\n(TODO: write the actual proofs)\r\n\r\n* HC $$\\le\\_p$$ RHC\r\n* RHC $$\\le\\_p$$ SUBOPT\\_TSP\r\n\r\n\r\n", "child": 181, "article": 10}}, {"pk": 132, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T09:23:34.690Z", "edited_by": 3, "parent": 131, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n* Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals ** lsjifasdfw ** and ** areighifpo ** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n* Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: ** lsjifhifpo ** could mutate to ** lsjifaifpo **, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n* (regrouping)\r\n* (colonization-extinction)\r\n* (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 133, "article": 10}}, {"pk": 139, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T17:52:50.212Z", "edited_by": 3, "parent": 138, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n# Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 140, "article": 10}}, {"pk": 183, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-15T06:18:53.643Z", "edited_by": 24, "parent": 181, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook, and even this compendium!). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n## Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP, with the restriction that the triangle inequality holds, can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(_x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n## The Primal-Dual schema\r\n\r\n(taken from cmu.edu)\r\n\r\nWe typically devise algorithms for minimization problems in the following way:\r\n\r\n1. Write down an LP relaxation of the problem, and find its dual. Try to find some intuitive meaning for the dual variables.\r\n2. Start with vectors x = 0, y = 0, which will be dual feasible, but primal infeasible.\r\n\r\n3. Until the primal is feasible:\r\n    1. increase the dual values y\\_i in some controlled fashion until some dual constraint(s) goes tight (i.e. until sum\\_i y_i * a\\_ij = c\\_j for some j), while always maintaining the deal feasibility of y.\r\n    2. Select some subset of the tight dual constraints, and increase the primal variable corresponding to them by an integral amount.\r\n\r\n4. For the analysis, prove that the output pair of vectors (x,y) satisfies c^[trans] * x <= rho * y^[trans] * b for as small a value of rho as possible. Keep this goal in mind when deciding how to raise the dual and primal variables.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$, where the edge weights satisfy the triangle inequality.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP with $$\\rho = 1.5$$. Why does this approximate TSP with $$\\rho = 1.5$$, you ask? Well:\r\n\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!\r\n\r\n\r\n# Appendix: list of algorithms featured in the textbook\r\n\r\n* DPKP\r\n* Ford-Fulkerson\r\n* Vertex cover algorithm\r\n* B&B for MAXSAT and TSP\r\n* D&C-3SAT\r\n* Local search scheme\r\n* Kernigan-Lin variable-depth search\r\n* Metropolis algorithm\r\n* Simulated Annealing\r\n* Tabu search\r\n* Randomized tabu search\r\n* Genetic algorithm scheme\r\n* Methods for relaxing problems to LP\r\n* Simplex\r\n* Primal-dual scheme\r\n* Primal-dual (MINVCP)\r\n* Greedy makespan schedule\r\n* Algorithm for greedy makespan schedule\r\n* Algorithm for VCP\r\n* Algorithm for SCP\r\n* Algorithm for WEIGHT-VCP\r\n* Algorithm for MAX-CUT\r\n* Greedy-simple KP\r\n* PTAS for SKP\r\n* modified PTAS for SKP\r\n* FPTAS for KP\r\n* TSP \u25b3-ineq 2-approx\r\n* Christofides algorithm\r\n* Sekanina's algorithm\r\n\r\n\r\n# Appendix: proofs\r\n\r\n(TODO: write the actual proofs)\r\n\r\n* HC $$\\le\\_p$$ RHC\r\n* RHC $$\\le\\_p$$ SUBOPT\\_TSP\r\n\r\n\r\n", "child": 184, "article": 10}}, {"pk": 146, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T10:16:47.593Z", "edited_by": 3, "parent": 145, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\n## The Chomsky hierarchy\r\n\r\nRegular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n# Parsing\r\n\r\n\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 147, "article": 5}}, {"pk": 152, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T12:57:59.312Z", "edited_by": 3, "parent": 151, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n    $$(\\sigma \\union N)*N(\\sigma \\union N)* \\rightarrow (\\sigma \\union \\N)*\r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Parsing\r\n\r\n\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 153, "article": 5}}, {"pk": 196, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-16T14:35:02.222Z", "edited_by": 15, "parent": 195, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook, and even this compendium!). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n### Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP, with the restriction that the triangle inequality holds, can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **pseudo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(_x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Approximation algorithms\r\n\r\nApproximation algorithms are algorithms that calculate an approximate answer to an optimization problem. Informally, approximation algorithms are said to have an approximation ratio of $$\\rho$$ if the algorithm at worst produces an answer which is $$\\rho$$ times worse than the optimal answer. In a manner analogous to numerical stability, approximation algorithms are said to be stable if small changes in the approximation parameters result in correspondingly small changes in the answer. \r\n\r\n## Polynomial-time approximation scheme (PTAS)\r\n\r\nA polynomial-time approximation algorithm is a PTAS if the approximation error is bounded for each input, and Time_A(x, 1/error) is polynomial in |x|.\r\n\r\n## Fully polynomial-timme approximation scheme (FPTAS)\r\n\r\nAn PTAS is FPTAS is it is polynomial in |x| and 1/error.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n## The Primal-Dual schema\r\n\r\n(taken from cmu.edu)\r\n\r\nWe typically devise algorithms for minimization problems in the following way:\r\n\r\n1. Write down an LP relaxation of the problem, and find its dual. Try to find some intuitive meaning for the dual variables.\r\n2. Start with vectors x = 0, y = 0, which will be dual feasible, but primal infeasible.\r\n\r\n3. Until the primal is feasible:\r\n    1. increase the dual values y\\_i in some controlled fashion until some dual constraint(s) goes tight (i.e. until sum\\_i y_i * a\\_ij = c\\_j for some j), while always maintaining the deal feasibility of y.\r\n    2. Select some subset of the tight dual constraints, and increase the primal variable corresponding to them by an integral amount.\r\n\r\n4. For the analysis, prove that the output pair of vectors (x,y) satisfies $$c^[trans] * x <= $\\rho * y^[trans] * b$$ for as small a value of rho as possible. Keep this goal in mind when deciding how to raise the dual and primal variables.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$, where the edge weights satisfy the triangle inequality.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP with $$\\rho = 1.5$$. Why does this approximate TSP with $$\\rho = 1.5$$, you ask? Well:\r\n\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!\r\n\r\n\r\n# Appendix: list of algorithms featured in the textbook\r\n\r\n* DPKP\r\n* Ford-Fulkerson\r\n* Vertex cover algorithm\r\n* B&amp;B for MAXSAT and TSP\r\n* D&amp;C-3SAT\r\n* Local search scheme\r\n* Kernigan-Lin variable-depth search\r\n* Metropolis algorithm\r\n* Simulated Annealing\r\n* Tabu search\r\n* Randomized tabu search\r\n* Genetic algorithm scheme\r\n* Methods for relaxing problems to LP\r\n* Simplex\r\n* Primal-dual scheme\r\n* Primal-dual (MINVCP)\r\n* Greedy makespan schedule\r\n* Algorithm for greedy makespan schedule\r\n* Algorithm for VCP\r\n* Algorithm for SCP\r\n* Algorithm for WEIGHT-VCP\r\n* Algorithm for MAX-CUT\r\n* Greedy-simple KP\r\n* PTAS for SKP\r\n* modified PTAS for SKP\r\n* FPTAS for KP\r\n* TSP \u25b3-ineq 2-approx\r\n* Christofides algorithm\r\n* Sekanina's algorithm\r\n\r\n\r\n# Appendix: proofs\r\n\r\n(TODO: write the actual proofs)\r\n\r\n* HC $$\\le\\_p$$ RHC\r\n* RHC $$\\le\\_p$$ SUBOPT\\_TSP\r\n\r\n\r\n", "child": null, "article": 10}}, {"pk": 158, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T14:57:46.097Z", "edited_by": 3, "parent": 157, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n$$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$\r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string is a valid input, allowing for \"spontaneous\" transitions between states without input.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## shift/reduce\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 159, "article": 5}}, {"pk": 195, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-16T14:34:22.592Z", "edited_by": 15, "parent": 186, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook, and even this compendium!). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n### Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP, with the restriction that the triangle inequality holds, can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **pseudo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(_x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Approximation algorithms\r\n\r\nApproximation algorithms are algorithms that calculate an approximate answer to an optimization problem. Informally, approximation algorithms are said to have an approximation ratio of $$\\rho$$ if the algorithm at worst produces an answer which is $$\\rho$$ times worse than the optimal answer. In a manner analogous to numerical stability, approximation algorithms are said to be stable if small changes in the approximation parameters result in correspondingly small changes in the answer. \r\n\r\n## Polynomial-time approximation scheme (PTAS)\r\n\r\nA polynomial-time approximation algorithm is a PTAS if the approximation error is bounded for each input, and Time_A(x, 1/error) is polynomial in |x|.\r\n\r\n## Fully polynomial-timme approximation scheme (FPTAS)\r\n\r\nAn PTAS is FPTAS is it is polynomial in |x| and 1/error.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n## The Primal-Dual schema\r\n\r\n(taken from cmu.edu)\r\n\r\nWe typically devise algorithms for minimization problems in the following way:\r\n\r\n1. Write down an LP relaxation of the problem, and find its dual. Try to find some intuitive meaning for the dual variables.\r\n2. Start with vectors x = 0, y = 0, which will be dual feasible, but primal infeasible.\r\n\r\n3. Until the primal is feasible:\r\n    1. increase the dual values y\\_i in some controlled fashion until some dual constraint(s) goes tight (i.e. until sum\\_i y_i * a\\_ij = c\\_j for some j), while always maintaining the deal feasibility of y.\r\n    2. Select some subset of the tight dual constraints, and increase the primal variable corresponding to them by an integral amount.\r\n\r\n4. For the analysis, prove that the output pair of vectors (x,y) satisfies $c^[trans] * x <= \\rho * y^[trans] * b$ for as small a value of rho as possible. Keep this goal in mind when deciding how to raise the dual and primal variables.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$, where the edge weights satisfy the triangle inequality.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP with $$\\rho = 1.5$$. Why does this approximate TSP with $$\\rho = 1.5$$, you ask? Well:\r\n\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!\r\n\r\n\r\n# Appendix: list of algorithms featured in the textbook\r\n\r\n* DPKP\r\n* Ford-Fulkerson\r\n* Vertex cover algorithm\r\n* B&amp;B for MAXSAT and TSP\r\n* D&amp;C-3SAT\r\n* Local search scheme\r\n* Kernigan-Lin variable-depth search\r\n* Metropolis algorithm\r\n* Simulated Annealing\r\n* Tabu search\r\n* Randomized tabu search\r\n* Genetic algorithm scheme\r\n* Methods for relaxing problems to LP\r\n* Simplex\r\n* Primal-dual scheme\r\n* Primal-dual (MINVCP)\r\n* Greedy makespan schedule\r\n* Algorithm for greedy makespan schedule\r\n* Algorithm for VCP\r\n* Algorithm for SCP\r\n* Algorithm for WEIGHT-VCP\r\n* Algorithm for MAX-CUT\r\n* Greedy-simple KP\r\n* PTAS for SKP\r\n* modified PTAS for SKP\r\n* FPTAS for KP\r\n* TSP \u25b3-ineq 2-approx\r\n* Christofides algorithm\r\n* Sekanina's algorithm\r\n\r\n\r\n# Appendix: proofs\r\n\r\n(TODO: write the actual proofs)\r\n\r\n* HC $$\\le\\_p$$ RHC\r\n* RHC $$\\le\\_p$$ SUBOPT\\_TSP\r\n\r\n\r\n", "child": 196, "article": 10}}, {"pk": 159, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T14:58:29.307Z", "edited_by": 3, "parent": 158, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n$$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string is a valid input, allowing for \"spontaneous\" transitions between states without input.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## shift/reduce\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 160, "article": 5}}, {"pk": 73, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-07T11:20:13.117Z", "edited_by": 9, "parent": 72, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Dominator trees\r\n    * Interference graphs\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 74, "article": 5}}, {"pk": 98, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T15:20:53.943Z", "edited_by": 3, "parent": 97, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\n## Straight up regular local search\r\n## Variable-depth search\r\n## Kernighan-Lin variable depth search\r\n## Simulated annealing\r\n## Tabu search\r\n## Randomized Tabu search\r\n\r\n# Genetic algorithms\r\n\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 99, "article": 10}}, {"pk": 78, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T11:19:31.453Z", "edited_by": 3, "parent": 77, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3_bool = {0, 1}\r\n* \u03a3_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time_A(x) and Space_A(x)\r\n\r\n> Let \u03a3_input and \u03a3_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3_input* to \u03a3_output*. For every _x_ \u2208 \u03a3_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n", "child": 79, "article": 10}}, {"pk": 164, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-12T07:40:52.919Z", "edited_by": 3, "parent": 163, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n$$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string ($$\\epsilon$$) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols $$\\alpha$$ is the set of terminals that begin the strings derived from $$\\alpha$$. FIRST($$\\alpha$$) can be constructed like this for each symbol $$X$$ in $$\\alpha$$:\r\n\r\n1. If $$X$$ is a terminal, FIRST($$X$$) = {$$X$$}.\r\n2. If $$X \\rightarrow \\epsilon$$ is a production, add $$\\epsilon$$ to FIRST($$X$$)\r\n3. If $$X \\rightarrow Y\\_1 Y\\_2 ... Y\\_k$$ then add FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) to FIRST($$X$$)\r\n4. FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) is one of:\r\n    * FIRST($$Y\\_1$$) if FIRST($$Y\\_1$$) doesn't contain $$\\epsilon$$.\r\n    * FIRST($$Y\\_1$$) + FIRST($$Y\\_2 ... Y\\_k$$) - {$$\\epsilon$$}\r\n\r\nIf FIRST($$Y_i$$) contains $$\\epsilon$$ for all $$0 \\le i \\le k \\in \\mathbb{\\integers}$$, add $$\\epsilon$$ to FIRST($$X$$)\r\n\r\nThe **FOLLOW** set of a nonterminal $$A$$ is the set of terminals $$a$$ that can appear immediately to the right of $$A$$. In other words: the set of terminals $$a$$ such that there exists a derivation of the form $$S \\rightarrow \\alpha A a \\beta$$. FOLLOW($$A$$) for all nonterminals $$A$$ can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW($$S$$), where $$S$$ is the start symbol.\r\n2. If there is a production $$A \\rightarrow \\alpha B \\beta$$, then everything in FIRST($$\\beta$$) - {$$\\epsilon$$} is placed in FOLLOW($$B$$)\r\n3. If there is a production $$A \\rightarrow \\alpha B$$ or a production $$A \\rightarrow \\alpha B \\betawhere FIRST($$\\beta$$) contains $$\\epsilon$$, everything in FOLLOW($$A$$) is in FOLLOW($$B$$).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## shift/reduce\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 165, "article": 5}}, {"pk": 79, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T11:20:22.479Z", "edited_by": 3, "parent": 78, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3_input* to \u03a3_output*. For every _x_ \u2208 \u03a3_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n", "child": 80, "article": 10}}, {"pk": 80, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T11:20:42.475Z", "edited_by": 3, "parent": 79, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n", "child": 81, "article": 10}}, {"pk": 105, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T17:52:24.384Z", "edited_by": 3, "parent": 104, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\n## Straight up regular local search\r\n## Variable-depth search\r\n## Kernighan-Lin variable depth search\r\n## Simulated annealing\r\n## Tabu search\r\n## Randomized Tabu search\r\n\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nFor an example of branch and bound, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 106, "article": 10}}, {"pk": 86, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T11:58:04.415Z", "edited_by": 3, "parent": 85, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n", "child": 87, "article": 10}}, {"pk": 92, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T13:18:50.207Z", "edited_by": 3, "parent": 91, "title": "Algorithm Construction, Advanced Course", "content": "$y = 2*x$\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n## Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n\r\n\r\n\r\n", "child": 93, "article": 10}}, {"pk": 112, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-09T08:00:36.883Z", "edited_by": 3, "parent": 111, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search remembers the last _k_ visited candidate solutions, and avoids revisiting them. This prevents a possibility to be considered repeatedly, getting stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 113, "article": 10}}, {"pk": 170, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-13T12:54:56.568Z", "edited_by": 9, "parent": 166, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n  $$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string ($$\\epsilon$$) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols $$\\alpha$$ is the set of terminals that begin the strings derived from $$\\alpha$$. FIRST($$\\alpha$$) can be constructed like this for each symbol $$X$$ in $$\\alpha$$:\r\n\r\n1. If $$X$$ is a terminal, FIRST($$X$$) = {$$X$$}.\r\n2. If $$X \\rightarrow \\epsilon$$ is a production, add $$\\epsilon$$ to FIRST($$X$$)\r\n3. If $$X \\rightarrow Y\\_1 Y\\_2 ... Y\\_k$$ then add FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) to FIRST($$X$$)\r\n4. FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) is one of:\r\n    * FIRST($$Y\\_1$$) if FIRST($$Y\\_1$$) doesn't contain $$\\epsilon$$.\r\n    * FIRST($$Y\\_1$$) + FIRST($$Y\\_2 ... Y\\_k$$) - {$$\\epsilon$$}\r\n\r\nIf FIRST($$Y_i$$) contains $$\\epsilon$$ for all $$0 \\le i \\le k \\in \\mathbb{\\integers}$$, add $$\\epsilon$$ to FIRST($$X$$)\r\n\r\nThe **FOLLOW** set of a nonterminal $$A$$ is the set of terminals $$a$$ that can appear immediately to the right of $$A$$. In other words: the set of terminals $$a$$ such that there exists a derivation of the form $$S \\rightarrow \\alpha A a \\beta$$. FOLLOW($$A$$) for all nonterminals $$A$$ can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW($$S$$), where $$S$$ is the start symbol.\r\n2. If there is a production $$A \\rightarrow \\alpha B \\beta$$, then everything in FIRST($$\\beta$$) - {$$\\epsilon$$} is placed in FOLLOW($$B$$)\r\n3. If there is a production $$A \\rightarrow \\alpha B$$ or a production $$A \\rightarrow \\alpha B \\beta$$ where FIRST($$\\beta$$) contains $$\\epsilon$$, everything in FOLLOW($$A$$) is in FOLLOW($$B$$).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites.\r\n\r\nShift-reduce parsing is a category of commonly used methods for bottom-up parsing. Shift-reduce is based on two basic steps: shift and reduce. Shift advances in the input stream by one token, making the new symbol a new single-node parse tree. Reduce applies a completed grammar rule to one the recent parse trees, joining them together as one tree with a new root symbol.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. LR-parsing is a shift-reduce parsing method. Canonical LR-parsing, though fast, is prohibitively spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n\r\n\r\n\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 171, "article": 5}}, {"pk": 119, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-09T19:59:26.128Z", "edited_by": 3, "parent": 118, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 120, "article": 10}}, {"pk": 126, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T09:17:06.770Z", "edited_by": 3, "parent": 125, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [#genetic-template] for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| ** Genetics term ** || ** Corresponding algorithmics term **                       ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [#genetic-template](template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$ P = {a_1, a_2, ..., a_k} $$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n* Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals ** lsjifasdfw ** and ** areighifpo ** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n* Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: ** lsjifhifpo ** could mutate to ** lsjifaifpo **, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n* (regrouping)\r\n* (colonization-extinction)\r\n* (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 127, "article": 10}}, {"pk": 177, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-13T13:23:13.115Z", "edited_by": 9, "parent": 176, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) + (int), but disallow (str) + (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar G = (N,\u03c3,P,S) consists of:\r\n\r\n* A finite set N of nonterminal symbols\r\n* A finite set \u03c3 of terminal symbols that is disjoint from N.\r\n* A finite set P of production rules, each rule on the form:\r\n  (\u03c3 \u222a N)* N(\u03c3 \u222a N)* \u2192 (\u03c3 \u222a N)* \r\n* A symbol S \u2208 N that is the start symbol\r\n\r\n\r\nThe language of a grammar G, L(G), is defined as all the sentences that can be derived in a finite number of steps from the start symbol S.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, \u03b5 or \u039b for the empty string, and S for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form V\u2192v, where V is a non-terminal and v may be a combination of terminals and non-terminals.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form \u03b1A\u03b2\u2192\u03b1\u03b3\u03b2, where \u03b1, \u03b2 and \u03b3 are strings of nonterminals and terminals, and \u03b3 is not \u03b5.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string (\u03b5) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols \u03b1 is the set of terminals that begin the strings derived from \u03b1. FIRST(\u03b1) can be constructed like this for each symbol X in \u03b1:\r\n\r\n1. If X is a terminal, FIRST(X) = {X}.\r\n2. If X\u2192\u03b5 is a production, add \u03b5 to FIRST(X)\r\n3. If X\u2192Y1Y2\u2026Yk then add FIRST(Y1Y2\u2026Yk) to FIRST(X)\r\n4. FIRST(Y1Y2\u2026Yk) is one of:\r\n    * FIRST(Y1) if FIRST(Y1) doesn't contain \u03b5.\r\n    * FIRST(Y1) + FIRST(Y2\u2026Yk) - {\u03b5}\r\n\r\nIf FIRST(Yi) contains \u03b5 for all 0\u2264i\u2264k \u2208 \u2124, add \u03b5 to FIRST(X)\r\n\r\nThe **FOLLOW** set of a nonterminal A is the set of terminals a that can appear immediately to the right of A. In other words: the set of terminals a such that there exists a derivation of the form S\u2192\u03b1Aa\u03b2. FOLLOW(A) for all nonterminals A can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW(S), where S is the start symbol.\r\n2. If there is a production A\u2192\u03b1B\u03b2, then everything in FIRST(\u03b2) - {\u03b5} is placed in FOLLOW(B)\r\n3. If there is a production A\u2192\u03b1B or a production A\u2192\u03b1B\u03b2 where FIRST(\u03b2) contains \u03b5, everything in FOLLOW(A) is in FOLLOW(B).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites.\r\n\r\nShift-reduce parsing is a category of commonly used methods for bottom-up parsing. Shift-reduce is based on two basic steps: shift and reduce. Shift advances in the input stream by one token, making the new symbol a new single-node parse tree. Reduce applies a completed grammar rule to one the recent parse trees, joining them together as one tree with a new root symbol.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. LR-parsing is a shift-reduce parsing method. Canonical LR-parsing, though fast, is prohibitively spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n\r\n\r\n\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression 3 + 2 can be replaced by the expression 5.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node d dominates a node n if every path from the start node to n must go through d. We write: d dom n, or d >> n. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 178, "article": 5}}, {"pk": 133, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T09:24:34.021Z", "edited_by": 3, "parent": 132, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n* Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n* Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n* (regrouping)\r\n* (colonization-extinction)\r\n* (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 134, "article": 10}}, {"pk": 140, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T18:01:46.669Z", "edited_by": 3, "parent": 139, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n# Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP, with the restriction that the triangle inequality holds, can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$, where the edge weights satisfy the triangle inequality.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP with $$\\rho = 1.5$$. Why does this approximate TSP with $$\\rho = 1.5$$, you ask? Well:\r\n\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 141, "article": 10}}, {"pk": 184, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-15T06:19:19.752Z", "edited_by": 24, "parent": 183, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook, and even this compendium!). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n### Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP, with the restriction that the triangle inequality holds, can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(_x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n## The Primal-Dual schema\r\n\r\n(taken from cmu.edu)\r\n\r\nWe typically devise algorithms for minimization problems in the following way:\r\n\r\n1. Write down an LP relaxation of the problem, and find its dual. Try to find some intuitive meaning for the dual variables.\r\n2. Start with vectors x = 0, y = 0, which will be dual feasible, but primal infeasible.\r\n\r\n3. Until the primal is feasible:\r\n    1. increase the dual values y\\_i in some controlled fashion until some dual constraint(s) goes tight (i.e. until sum\\_i y_i * a\\_ij = c\\_j for some j), while always maintaining the deal feasibility of y.\r\n    2. Select some subset of the tight dual constraints, and increase the primal variable corresponding to them by an integral amount.\r\n\r\n4. For the analysis, prove that the output pair of vectors (x,y) satisfies c^[trans] * x <= rho * y^[trans] * b for as small a value of rho as possible. Keep this goal in mind when deciding how to raise the dual and primal variables.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$, where the edge weights satisfy the triangle inequality.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP with $$\\rho = 1.5$$. Why does this approximate TSP with $$\\rho = 1.5$$, you ask? Well:\r\n\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!\r\n\r\n\r\n# Appendix: list of algorithms featured in the textbook\r\n\r\n* DPKP\r\n* Ford-Fulkerson\r\n* Vertex cover algorithm\r\n* B&B for MAXSAT and TSP\r\n* D&C-3SAT\r\n* Local search scheme\r\n* Kernigan-Lin variable-depth search\r\n* Metropolis algorithm\r\n* Simulated Annealing\r\n* Tabu search\r\n* Randomized tabu search\r\n* Genetic algorithm scheme\r\n* Methods for relaxing problems to LP\r\n* Simplex\r\n* Primal-dual scheme\r\n* Primal-dual (MINVCP)\r\n* Greedy makespan schedule\r\n* Algorithm for greedy makespan schedule\r\n* Algorithm for VCP\r\n* Algorithm for SCP\r\n* Algorithm for WEIGHT-VCP\r\n* Algorithm for MAX-CUT\r\n* Greedy-simple KP\r\n* PTAS for SKP\r\n* modified PTAS for SKP\r\n* FPTAS for KP\r\n* TSP \u25b3-ineq 2-approx\r\n* Christofides algorithm\r\n* Sekanina's algorithm\r\n\r\n\r\n# Appendix: proofs\r\n\r\n(TODO: write the actual proofs)\r\n\r\n* HC $$\\le\\_p$$ RHC\r\n* RHC $$\\le\\_p$$ SUBOPT\\_TSP\r\n\r\n\r\n", "child": 185, "article": 10}}, {"pk": 147, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T10:24:59.737Z", "edited_by": 3, "parent": 146, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n    $$(\\sigma \\union N)*N(\\sigma \\union N)* \\rightarrow (\\sigma \\union \\N)*\r\n* A s --\r\n\r\n## The Chomsky hierarchy\r\n\r\nRegular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n# Parsing\r\n\r\n\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 148, "article": 5}}, {"pk": 66, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-06T13:38:51.265Z", "edited_by": 9, "parent": 65, "title": "Artificial Intelligence Methods", "content": "*   Strong/weak AI\r\n*   Agents\r\n    *   Rationality\r\n*   Bayesian networks\r\n    *   Syntax and semantics\r\n    *   Uses and problems\r\n    *   Conditional independence\r\n*   Decision networks/Influence diagrams\r\n*   Reasoning\r\n    *   Case-based reasoning\r\n    *   Instance-based reasoning\r\n*   Markov processes\r\n    *   Markov assumptiom\r\n*   Neural networks\r\n*   Decision trees\r\n", "child": 187, "article": 9}}, {"pk": 153, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T13:51:44.459Z", "edited_by": 3, "parent": 152, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n    $$(\\sigma \\union N)*N(\\sigma \\union N)* \\rightarrow (\\sigma \\union \\N)*\r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Parsing\r\n\r\n\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 154, "article": 5}}, {"pk": 187, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-15T10:54:22.625Z", "edited_by": 9, "parent": 66, "title": "Artificial Intelligence Methods", "content": "*   Strong/weak AI\r\n*   Agents\r\n    *   Rationality\r\n*   Bayesian networks\r\n    *   Syntax and semantics\r\n    *   Uses and problems\r\n    *   Conditional independence\r\n*   Decision networks/Influence diagrams\r\n*   Reasoning\r\n    *   Case-based reasoning\r\n    *   Instance-based reasoning\r\n*   Markov processes\r\n    *   Markov assumptiom\r\n*   Neural networks\r\n*   Decision trees\r\n\r\n# Bayesian Networks\r\nA Bayesian network is a model of a set of random variables and their conditional dependencies through a directed acyclic graph. In the graph, the nodes represent the random variables and the edges conditional dependencies.\r\n\r\nMuch of the calculation in a bayesian network is based on Bayes' theorem: $$ P(A|B)=\\frac{P(B|A)P(A)}{P(B)} $$.", "child": 188, "article": 9}}, {"pk": 82, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T11:34:12.862Z", "edited_by": 3, "parent": 81, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n", "child": 83, "article": 10}}, {"pk": 93, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T13:19:09.232Z", "edited_by": 3, "parent": 92, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n## Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n\r\n\r\n\r\n", "child": 94, "article": 10}}, {"pk": 83, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T11:42:08.422Z", "edited_by": 3, "parent": 82, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\n\r\n\r\n\r\n", "child": 84, "article": 10}}, {"pk": 85, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T11:50:48.173Z", "edited_by": 3, "parent": 84, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous decision problems\r\n\r\nThis is a list of important or otherwise famous decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n* NP-complete\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n* NP-complete\r\n\r\n### Clique problem (CLIQUE)\r\n\r\n\r\n\r\n### Vertex cover problem (VCP)\r\n\r\n# Optimization problems\r\n\r\n", "child": 86, "article": 10}}, {"pk": 99, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T15:39:20.005Z", "edited_by": 3, "parent": 98, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\n## Straight up regular local search\r\n## Variable-depth search\r\n## Kernighan-Lin variable depth search\r\n## Simulated annealing\r\n## Tabu search\r\n## Randomized Tabu search\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 100, "article": 10}}, {"pk": 87, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T12:53:00.715Z", "edited_by": 3, "parent": 86, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n\r\n", "child": 88, "article": 10}}, {"pk": 165, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-12T07:41:15.246Z", "edited_by": 3, "parent": 164, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n$$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string ($$\\epsilon$$) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols $$\\alpha$$ is the set of terminals that begin the strings derived from $$\\alpha$$. FIRST($$\\alpha$$) can be constructed like this for each symbol $$X$$ in $$\\alpha$$:\r\n\r\n1. If $$X$$ is a terminal, FIRST($$X$$) = {$$X$$}.\r\n2. If $$X \\rightarrow \\epsilon$$ is a production, add $$\\epsilon$$ to FIRST($$X$$)\r\n3. If $$X \\rightarrow Y\\_1 Y\\_2 ... Y\\_k$$ then add FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) to FIRST($$X$$)\r\n4. FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) is one of:\r\n    * FIRST($$Y\\_1$$) if FIRST($$Y\\_1$$) doesn't contain $$\\epsilon$$.\r\n    * FIRST($$Y\\_1$$) + FIRST($$Y\\_2 ... Y\\_k$$) - {$$\\epsilon$$}\r\n\r\nIf FIRST($$Y_i$$) contains $$\\epsilon$$ for all $$0 \\le i \\le k \\in \\mathbb{\\integers}$$, add $$\\epsilon$$ to FIRST($$X$$)\r\n\r\nThe **FOLLOW** set of a nonterminal $$A$$ is the set of terminals $$a$$ that can appear immediately to the right of $$A$$. In other words: the set of terminals $$a$$ such that there exists a derivation of the form $$S \\rightarrow \\alpha A a \\beta$$. FOLLOW($$A$$) for all nonterminals $$A$$ can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW($$S$$), where $$S$$ is the start symbol.\r\n2. If there is a production $$A \\rightarrow \\alpha B \\beta$$, then everything in FIRST($$\\beta$$) - {$$\\epsilon$$} is placed in FOLLOW($$B$$)\r\n3. If there is a production $$A \\rightarrow \\alpha B$$ or a production $$A \\rightarrow \\alpha B \\beta$$ where FIRST($$\\beta$$) contains $$\\epsilon$$, everything in FOLLOW($$A$$) is in FOLLOW($$B$$).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## shift/reduce\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 166, "article": 5}}, {"pk": 106, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T17:53:18.690Z", "edited_by": 3, "parent": 105, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\n## Straight up regular local search\r\n## Variable-depth search\r\n## Kernighan-Lin variable depth search\r\n## Simulated annealing\r\n## Tabu search\r\n## Randomized Tabu search\r\n\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 107, "article": 10}}, {"pk": 171, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-13T12:56:26.878Z", "edited_by": 9, "parent": 170, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n  $$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string ($$\\epsilon$$) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols $$\\alpha$$ is the set of terminals that begin the strings derived from $$\\alpha$$. FIRST($$\\alpha$$) can be constructed like this for each symbol $$X$$ in $$\\alpha$$:\r\n\r\n1. If $$X$$ is a terminal, FIRST($$X$$) = {$$X$$}.\r\n2. If $$X \\rightarrow \\epsilon$$ is a production, add $$\\epsilon$$ to FIRST($$X$$)\r\n3. If $$X \\rightarrow Y\\_1 Y\\_2 ... Y\\_k$$ then add FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) to FIRST($$X$$)\r\n4. FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) is one of:\r\n    * FIRST($$Y\\_1$$) if FIRST($$Y\\_1$$) doesn't contain $$\\epsilon$$.\r\n    * FIRST($$Y\\_1$$) + FIRST($$Y\\_2 ... Y\\_k$$) - {$$\\epsilon$$}\r\n\r\nIf FIRST($$Y_i$$) contains $$\\epsilon$$ for all $$0 \\le i \\le k \\in \\mathbb{\\integers}$$, add $$\\epsilon$$ to FIRST($$X$$)\r\n\r\nThe **FOLLOW** set of a nonterminal $$A$$ is the set of terminals $$a$$ that can appear immediately to the right of $$A$$. In other words: the set of terminals $$a$$ such that there exists a derivation of the form $$S \\rightarrow \\alpha A a \\beta$$. FOLLOW($$A$$) for all nonterminals $$A$$ can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW($$S$$), where $$S$$ is the start symbol.\r\n2. If there is a production $$A \\rightarrow \\alpha B \\beta$$, then everything in FIRST($$\\beta$$) - {$$\\epsilon$$} is placed in FOLLOW($$B$$)\r\n3. If there is a production $$A \\rightarrow \\alpha B$$ or a production $$A \\rightarrow \\alpha B \\beta$$ where FIRST($$\\beta$$) contains $$\\epsilon$$, everything in FOLLOW($$A$$) is in FOLLOW($$B$$).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites.\r\n\r\nShift-reduce parsing is a category of commonly used methods for bottom-up parsing. Shift-reduce is based on two basic steps: shift and reduce. Shift advances in the input stream by one token, making the new symbol a new single-node parse tree. Reduce applies a completed grammar rule to one the recent parse trees, joining them together as one tree with a new root symbol.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. LR-parsing is a shift-reduce parsing method. Canonical LR-parsing, though fast, is prohibitively spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n\r\n\r\n\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 172, "article": 5}}, {"pk": 113, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-09T08:23:17.949Z", "edited_by": 3, "parent": 112, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search remembers the last _k_ visited candidate solutions, and avoids revisiting them. This prevents a possibility to be considered repeatedly, getting stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n\r\n\r\n\r\n", "child": 114, "article": 10}}, {"pk": 120, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-09T20:01:10.356Z", "edited_by": 3, "parent": 119, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n## Initialization\r\n## Selection\r\n## Genetic operators\r\n* Crossover/recombination\r\n* Mutation\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 121, "article": 10}}, {"pk": 178, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-13T14:00:30.475Z", "edited_by": 9, "parent": 177, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) + (int), but disallow (str) + (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar G = (N,\u03c3,P,S) consists of:\r\n\r\n* A finite set N of nonterminal symbols\r\n* A finite set \u03c3 of terminal symbols that is disjoint from N.\r\n* A finite set P of production rules, each rule on the form:\r\n  (\u03c3 \u222a N)* N(\u03c3 \u222a N)* \u2192 (\u03c3 \u222a N)* \r\n* A symbol S \u2208 N that is the start symbol\r\n\r\n\r\nThe language of a grammar G, L(G), is defined as all the sentences that can be derived in a finite number of steps from the start symbol S.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, \u03b5 or \u039b for the empty string, and S for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form V\u2192v, where V is a non-terminal and v may be a combination of terminals and non-terminals.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form \u03b1A\u03b2\u2192\u03b1\u03b3\u03b2, where \u03b1, \u03b2 and \u03b3 are strings of nonterminals and terminals, and \u03b3 is not \u03b5.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string (\u03b5) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols \u03b1 is the set of terminals that begin the strings derived from \u03b1. FIRST(\u03b1) can be constructed like this for each symbol X in \u03b1:\r\n\r\n1. If X is a terminal, FIRST(X) = {X}.\r\n2. If X\u2192\u03b5 is a production, add \u03b5 to FIRST(X)\r\n3. If X\u2192Y1Y2\u2026Yk then add FIRST(Y1Y2\u2026Yk) to FIRST(X)\r\n4. FIRST(Y1Y2\u2026Yk) is one of:\r\n    * FIRST(Y1) if FIRST(Y1) doesn't contain \u03b5.\r\n    * FIRST(Y1) + FIRST(Y2\u2026Yk) - {\u03b5}\r\n\r\nIf FIRST(Yi) contains \u03b5 for all 0\u2264i\u2264k \u2208 \u2124, add \u03b5 to FIRST(X)\r\n\r\nThe **FOLLOW** set of a nonterminal A is the set of terminals a that can appear immediately to the right of A. In other words: the set of terminals a such that there exists a derivation of the form S\u2192\u03b1Aa\u03b2. FOLLOW(A) for all nonterminals A can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW(S), where S is the start symbol.\r\n2. If there is a production A\u2192\u03b1B\u03b2, then everything in FIRST(\u03b2) - {\u03b5} is placed in FOLLOW(B)\r\n3. If there is a production A\u2192\u03b1B or a production A\u2192\u03b1B\u03b2 where FIRST(\u03b2) contains \u03b5, everything in FOLLOW(A) is in FOLLOW(B).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites.\r\n\r\nShift-reduce parsing is a category of commonly used methods for bottom-up parsing. Shift-reduce is based on two basic steps: shift and reduce. Shift advances in the input stream by one token, making the new symbol a new single-node parse tree. Reduce applies a completed grammar rule to one the recent parse trees, joining them together as one tree with a new root symbol.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. LR-parsing is a shift-reduce parsing method. Canonical LR-parsing, though fast, is prohibitively spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n\r\n\r\n\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Optimizations and when to apply them\r\n\r\nCopied from slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n### High IR\r\n\r\n#### Function Inlining\r\n\r\n#### Function cloning\r\n\r\n### Low IR\r\n\r\n#### Constant folding\r\n\r\nEvaluate constant expressions and change them with their value (change x = 3 * 2 to x = 6).\r\n\r\n#### Constant propagation\r\n\r\nIf we know that x has the value 6, we may change subsequent reads of x with the value 6 until x is overwritten.\r\n\r\n#### Value numbering\r\n\r\n#### Dead code elimination\r\n\r\n#### Loop-invariant code motion\r\n\r\n#### Common sub-expression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n#### Strength reduction\r\n\r\n#### Constant folding & propagation (?)\r\n\r\n#### Branch prediction/optimization\r\n\r\n#### Loop unrolling\r\n\r\n### Assembly\r\n\r\n#### Register allocation\r\n\r\n#### Cache optimization\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node d dominates a node n if every path from the start node to n must go through d. We write: d dom n, or d >> n. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 179, "article": 5}}, {"pk": 127, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T09:18:00.502Z", "edited_by": 3, "parent": 126, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [#genetic-template]() for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| ** Genetics term ** || ** Corresponding algorithmics term **                       ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [#genetic-template](template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$ P = {a_1, a_2, ..., a_k} $$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n* Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals ** lsjifasdfw ** and ** areighifpo ** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n* Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: ** lsjifhifpo ** could mutate to ** lsjifaifpo **, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n* (regrouping)\r\n* (colonization-extinction)\r\n* (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 128, "article": 10}}, {"pk": 134, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T09:25:55.387Z", "edited_by": 3, "parent": 133, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 136, "article": 10}}, {"pk": 185, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-15T09:35:25.366Z", "edited_by": 3, "parent": 184, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook, and even this compendium!). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n### Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP, with the restriction that the triangle inequality holds, can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **pseudo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(_x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Approximation algorithms\r\n\r\nApproximation algorithms are algorithms that calculate an approximate answer to an optimization problem. Informally, approximation algorithms are said to have an approximation ratio of $$\\rho$$ if the algorithm at worst produces an answer which is $$\\rho$$ times worse than the optimal answer. In a manner analogous to numerical stability, approximation algorithms are said to be stable if small changes in the approximation parameters result in correspondingly small changes in the answer. \r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n## The Primal-Dual schema\r\n\r\n(taken from cmu.edu)\r\n\r\nWe typically devise algorithms for minimization problems in the following way:\r\n\r\n1. Write down an LP relaxation of the problem, and find its dual. Try to find some intuitive meaning for the dual variables.\r\n2. Start with vectors x = 0, y = 0, which will be dual feasible, but primal infeasible.\r\n\r\n3. Until the primal is feasible:\r\n    1. increase the dual values y\\_i in some controlled fashion until some dual constraint(s) goes tight (i.e. until sum\\_i y_i * a\\_ij = c\\_j for some j), while always maintaining the deal feasibility of y.\r\n    2. Select some subset of the tight dual constraints, and increase the primal variable corresponding to them by an integral amount.\r\n\r\n4. For the analysis, prove that the output pair of vectors (x,y) satisfies c^[trans] * x <= rho * y^[trans] * b for as small a value of rho as possible. Keep this goal in mind when deciding how to raise the dual and primal variables.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$, where the edge weights satisfy the triangle inequality.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP with $$\\rho = 1.5$$. Why does this approximate TSP with $$\\rho = 1.5$$, you ask? Well:\r\n\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!\r\n\r\n\r\n# Appendix: list of algorithms featured in the textbook\r\n\r\n* DPKP\r\n* Ford-Fulkerson\r\n* Vertex cover algorithm\r\n* B&B for MAXSAT and TSP\r\n* D&C-3SAT\r\n* Local search scheme\r\n* Kernigan-Lin variable-depth search\r\n* Metropolis algorithm\r\n* Simulated Annealing\r\n* Tabu search\r\n* Randomized tabu search\r\n* Genetic algorithm scheme\r\n* Methods for relaxing problems to LP\r\n* Simplex\r\n* Primal-dual scheme\r\n* Primal-dual (MINVCP)\r\n* Greedy makespan schedule\r\n* Algorithm for greedy makespan schedule\r\n* Algorithm for VCP\r\n* Algorithm for SCP\r\n* Algorithm for WEIGHT-VCP\r\n* Algorithm for MAX-CUT\r\n* Greedy-simple KP\r\n* PTAS for SKP\r\n* modified PTAS for SKP\r\n* FPTAS for KP\r\n* TSP \u25b3-ineq 2-approx\r\n* Christofides algorithm\r\n* Sekanina's algorithm\r\n\r\n\r\n# Appendix: proofs\r\n\r\n(TODO: write the actual proofs)\r\n\r\n* HC $$\\le\\_p$$ RHC\r\n* RHC $$\\le\\_p$$ SUBOPT\\_TSP\r\n\r\n\r\n", "child": 186, "article": 10}}, {"pk": 141, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T19:00:58.048Z", "edited_by": 3, "parent": 140, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook, and even this article!). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n# Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP, with the restriction that the triangle inequality holds, can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$, where the edge weights satisfy the triangle inequality.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP with $$\\rho = 1.5$$. Why does this approximate TSP with $$\\rho = 1.5$$, you ask? Well:\r\n\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 142, "article": 10}}, {"pk": 148, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T10:59:23.563Z", "edited_by": 3, "parent": 147, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n    $$(\\sigma \\union N)*N(\\sigma \\union N)* \\rightarrow (\\sigma \\union \\N)*\r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Parsing\r\n\r\n\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 149, "article": 5}}, {"pk": 154, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T14:18:00.249Z", "edited_by": 3, "parent": 153, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n    $$(\\sigma \\union N)*N(\\sigma \\union N)* \\rightarrow (\\sigma \\union \\N)*\r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string is a valid input, allowing for \"spontaneous\" transitions between states without input.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 155, "article": 5}}, {"pk": 189, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-15T12:39:11.797Z", "edited_by": 9, "parent": 188, "title": "Artificial Intelligence Methods", "content": "*   Strong/weak AI\r\n*   Agents\r\n    *   Rationality\r\n*   Bayesian networks\r\n    *   Syntax and semantics\r\n    *   Uses and problems\r\n    *   Conditional independence\r\n*   Decision networks/Influence diagrams\r\n*   Reasoning\r\n    *   Case-based reasoning\r\n    *   Instance-based reasoning\r\n*   Markov processes\r\n    *   Markov assumptiom\r\n*   Neural networks\r\n*   Decision trees\r\n\r\n# Bayesian Networks\r\n\r\nA Bayesian network is a model of a set of random variables and their conditional dependencies through a directed acyclic graph. In the graph, the nodes represent the random variables and the edges conditional dependencies.\r\n\r\nMuch of the calculation in a bayesian network is based on Bayes' theorem: $$ P(A|B)=\\frac{P(B|A)P(A)}{P(B)} $$.\r\n\r\n## Bayesian networks in use\r\n\r\n* Good: Reason with uncertain knowledge. \"Easy\" to set up with domain experts (medicine, diagnostics).\r\n* Worse: Many dependencies to set up for most meaningful networks.\r\n* Ugly: Non-discrete models\r\n\r\nBad at for example image analysis, ginormous networks.\r\n\r\n# Markov Chains\r\n\r\nA Markov chain is a random (and generally discrete) process where the next state only depends on the current state, and not previous states. There also exists higher-order markov chains, where the next state will depend on the previous n states. Note that any k'th-order Markov process can be expressed as a first order Markov process.\r\n\r\nIn a Markov chain, you typically have observable variables which tell us something about something we want to know, but can not observe directly.\r\n\r\n## Operations on Markov Chains\r\n\r\n### Filtering\r\n\r\nCalculating the unobservable variables based on the evidence (the observable variables).\r\n\r\n### Prediction\r\n\r\nTrying to predict how the variables will behave in the future based on the current evidence.\r\n\r\n### Smoothing\r\n\r\nEstimate past states including evidence from later states.\r\n\r\n### Most likely explanation\r\n\r\nFind the most likely set of states.\r\n\r\n## Bernoulli scheme\r\n\r\nA special case of Markov chains where the next state is independent of the current state.\r\n\r\n# Learning\r\n\r\n## Decision trees\r\n\r\nDecision trees are a way of making decision when the different cases are describable by attribute-value pairs. The target function should be discretely valued. Decision trees may also perform well with noisy training data.\r\n\r\nA decision tree is simply a tree where each internal node represents an attribute, and it's edges represent the different values that attribute may hold. In order to reach a decision, you follow the tree downwards the appropriate values until you reach a leaf node, in which case you have reached the decision.\r\n\r\n### Building decision trees\r\n\r\nThe general procedure for building decision is a simple recursive algorithm, where you select an attribute to split the training data on, and then continue until all the data is classified/there are no more attributes to split on. How big the decision tree is depends on how you select which attribute you split on.\r\n\r\nFor an optimal solution, you want to split on the attribute which holds the most information, the attribute which will reduce the entropy the most.\r\n\r\n#### Overfitting\r\n\r\nWith training sets that are large, you may end up incorporating the noise from the training data which increases the error rate of the decision tree. A way of avoiding this is to calculate how good the tree is at classifying data while removing every node (including the nodes below it), and then greedily remove the one that increases the accuracy the most.", "child": 190, "article": 9}}, {"pk": 190, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-15T13:52:29.358Z", "edited_by": 9, "parent": 189, "title": "Artificial Intelligence Methods", "content": "*   Strong/weak AI\r\n*   Agents\r\n    *   Rationality\r\n*   Bayesian networks\r\n    *   Syntax and semantics\r\n    *   Uses and problems\r\n    *   Conditional independence\r\n*   Decision networks/Influence diagrams\r\n*   Reasoning\r\n    *   Case-based reasoning\r\n    *   Instance-based reasoning\r\n*   Markov processes\r\n    *   Markov assumptiom\r\n*   Neural networks\r\n*   Decision trees\r\n\r\n# Bayesian Networks\r\n\r\nA Bayesian network is a model of a set of random variables and their conditional dependencies through a directed acyclic graph. In the graph, the nodes represent the random variables and the edges conditional dependencies.\r\n\r\nMuch of the calculation in a bayesian network is based on Bayes' theorem: $$ P(A|B)=\\frac{P(B|A)P(A)}{P(B)} $$.\r\n\r\n## Bayesian networks in use\r\n\r\n* Good: Reason with uncertain knowledge. \"Easy\" to set up with domain experts (medicine, diagnostics).\r\n* Worse: Many dependencies to set up for most meaningful networks.\r\n* Ugly: Non-discrete models\r\n\r\nBad at for example image analysis, ginormous networks.\r\n\r\n# Markov Chains\r\n\r\nA Markov chain is a random (and generally discrete) process where the next state only depends on the current state, and not previous states. There also exists higher-order markov chains, where the next state will depend on the previous n states. Note that any k'th-order Markov process can be expressed as a first order Markov process.\r\n\r\nIn a Markov chain, you typically have observable variables which tell us something about something we want to know, but can not observe directly.\r\n\r\n## Operations on Markov Chains\r\n\r\n### Filtering\r\n\r\nCalculating the unobservable variables based on the evidence (the observable variables).\r\n\r\n### Prediction\r\n\r\nTrying to predict how the variables will behave in the future based on the current evidence.\r\n\r\n### Smoothing\r\n\r\nEstimate past states including evidence from later states.\r\n\r\n### Most likely explanation\r\n\r\nFind the most likely set of states.\r\n\r\n## Bernoulli scheme\r\n\r\nA special case of Markov chains where the next state is independent of the current state.\r\n\r\n# Learning\r\n\r\n## Decision trees\r\n\r\nDecision trees are a way of making decision when the different cases are describable by attribute-value pairs. The target function should be discretely valued. Decision trees may also perform well with noisy training data.\r\n\r\nA decision tree is simply a tree where each internal node represents an attribute, and it's edges represent the different values that attribute may hold. In order to reach a decision, you follow the tree downwards the appropriate values until you reach a leaf node, in which case you have reached the decision.\r\n\r\n### Building decision trees\r\n\r\nThe general procedure for building decision is a simple recursive algorithm, where you select an attribute to split the training data on, and then continue until all the data is classified/there are no more attributes to split on. How big the decision tree is depends on how you select which attribute you split on.\r\n\r\nFor an optimal solution, you want to split on the attribute which holds the most information, the attribute which will reduce the entropy the most.\r\n\r\n#### Overfitting\r\n\r\nWith training sets that are large, you may end up incorporating the noise from the training data which increases the error rate of the decision tree. A way of avoiding this is to calculate how good the tree is at classifying data while removing every node (including the nodes below it), and then greedily remove the one that increases the accuracy the most.\r\n\r\n# Case-based reasoning\r\n\r\nCase-based reasoning assumes that similar problems have similar solutions and that what works today still will work tomorrow. It uses this as a framework to learn and deduce what to do in both known and unknown circumstances.\r\n\r\n## Main components\r\n\r\n* Case base\r\n  * Previous cases\r\n* A method for retrieving relevant cases with solutions\r\n* Method for adapting to current case\r\n* Method for learning from solved problem\r\n\r\n## Approaches\r\n\r\n### Instance-based\r\n\r\nAn approach based on classical machine learning for classification tasks. Attribute-value pairs and a similarity metric.\r\n\r\n### Memory-based reasoning\r\n\r\nAs instance-based, only massively parallel(?), finds distance to every case in the database.\r\n\r\n### Analogy-based\r\n\r\nMotivated by psychological research. Cross-domain cases, problem solving. Background knowledge.", "child": 191, "article": 9}}, {"pk": 193, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-16T14:29:00.765Z", "edited_by": 3, "parent": null, "title": "Teknologiledelse", "content": "# Hvordan ikke stryke i tekled\r\n\r\n1. Gj\u00f8r oppgaver fra webutvilking.org/tekled\r\n2. Pugg matteformlene og regnereglene under\r\n\r\n# Formler\r\n\r\nTotalkostnad = TC = PQ\r\n\r\nPris = P = $$\\rho$$", "child": 194, "article": 13}}, {"pk": 197, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-16T14:47:46.481Z", "edited_by": 3, "parent": 194, "title": "Teknologiledelse", "content": "# Hvordan ikke stryke i tekled\r\n\r\n1. Gj\u00f8r oppgaver fra webutvilking.org/tekled\r\n2. Pugg matteformlene og regnereglene under\r\n3. (ikke forsov deg til eksamen)\r\n\r\n# Formler\r\n\r\nP = pris = en eller annen $$f(Q)$$, typisk P = 1000 - Q eller liknende.\r\n\r\nQ = produksjonsmengde.\r\n\r\nTC = totalkostnad = PQ\r\n\r\nMC = marginalkostnad = TC'\r\n\r\nTI = totalinntekt, typisk $$10000000 + 1000Q + Q^2$$ eller noe\r\n\r\nMI = marginalinntekt = TI'\r\n\r\n$$\\beta$$ = hvor mye en ting svinger i forhold til markedsverdien\r\n\r\nCAPM = KVM = capital asset pricing model, handler om at verdien til en aksje g\u00e5r mot kapitalmarkedslinjen\r\n\r\nWACC = after tax weighted average cost of capital\r\n\r\n\r\n\r\n\r\n", "child": null, "article": 13}}, {"pk": 84, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T11:42:54.022Z", "edited_by": 3, "parent": 83, "title": "Algorithm Construction, Advanced Course", "content": "# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n\r\n", "child": 85, "article": 10}}, {"pk": 100, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T17:01:59.627Z", "edited_by": 3, "parent": 99, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n## Spring 2013\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* Old exams can be found at: http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\n## Straight up regular local search\r\n## Variable-depth search\r\n## Kernighan-Lin variable depth search\r\n## Simulated annealing\r\n## Tabu search\r\n## Randomized Tabu search\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 101, "article": 10}}, {"pk": 142, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T19:01:08.413Z", "edited_by": 3, "parent": 141, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook, and even this compendium!). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n# Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP, with the restriction that the triangle inequality holds, can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$, where the edge weights satisfy the triangle inequality.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP with $$\\rho = 1.5$$. Why does this approximate TSP with $$\\rho = 1.5$$, you ask? Well:\r\n\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 167, "article": 10}}, {"pk": 107, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-08T19:13:55.644Z", "edited_by": 3, "parent": 106, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph has a vertex cover of size _k_.\r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\n## Variable-depth search\r\n## Kernighan-Lin variable depth search\r\n## Simulated annealing\r\n## Tabu search\r\n## Randomized Tabu search\r\n\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n", "child": 108, "article": 10}}, {"pk": 166, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-12T11:00:00.595Z", "edited_by": 3, "parent": 165, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n$$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string ($$\\epsilon$$) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols $$\\alpha$$ is the set of terminals that begin the strings derived from $$\\alpha$$. FIRST($$\\alpha$$) can be constructed like this for each symbol $$X$$ in $$\\alpha$$:\r\n\r\n1. If $$X$$ is a terminal, FIRST($$X$$) = {$$X$$}.\r\n2. If $$X \\rightarrow \\epsilon$$ is a production, add $$\\epsilon$$ to FIRST($$X$$)\r\n3. If $$X \\rightarrow Y\\_1 Y\\_2 ... Y\\_k$$ then add FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) to FIRST($$X$$)\r\n4. FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) is one of:\r\n    * FIRST($$Y\\_1$$) if FIRST($$Y\\_1$$) doesn't contain $$\\epsilon$$.\r\n    * FIRST($$Y\\_1$$) + FIRST($$Y\\_2 ... Y\\_k$$) - {$$\\epsilon$$}\r\n\r\nIf FIRST($$Y_i$$) contains $$\\epsilon$$ for all $$0 \\le i \\le k \\in \\mathbb{\\integers}$$, add $$\\epsilon$$ to FIRST($$X$$)\r\n\r\nThe **FOLLOW** set of a nonterminal $$A$$ is the set of terminals $$a$$ that can appear immediately to the right of $$A$$. In other words: the set of terminals $$a$$ such that there exists a derivation of the form $$S \\rightarrow \\alpha A a \\beta$$. FOLLOW($$A$$) for all nonterminals $$A$$ can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW($$S$$), where $$S$$ is the start symbol.\r\n2. If there is a production $$A \\rightarrow \\alpha B \\beta$$, then everything in FIRST($$\\beta$$) - {$$\\epsilon$$} is placed in FOLLOW($$B$$)\r\n3. If there is a production $$A \\rightarrow \\alpha B$$ or a production $$A \\rightarrow \\alpha B \\beta$$ where FIRST($$\\beta$$) contains $$\\epsilon$$, everything in FOLLOW($$A$$) is in FOLLOW($$B$$).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites.\r\n\r\nShift-reduce parsing is a category of commonly used methods for bottom-up parsing. Shift-reduce is based on two basic steps: shift and reduce. Shift advances in the input stream by one token, making the new symbol a new single-node parse tree. Reduce applies a completed grammar rule to one the recent parse trees, joining them together as one tree with a new root symbol.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. LR-parsing is a shift-reduce parsing method. Canonical LR-parsing, though fast, is prohibitively spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n\r\n\r\n\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 170, "article": 5}}, {"pk": 114, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-09T10:21:53.276Z", "edited_by": 3, "parent": 113, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search remembers the last _k_ visited candidate solutions, and avoids revisiting them. This prevents a possibility to be considered repeatedly, getting stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n\r\n\r\n\r\n", "child": 115, "article": 10}}, {"pk": 121, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-09T20:02:16.862Z", "edited_by": 3, "parent": 120, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\n## Initialization\r\n## Selection\r\n## Genetic operators\r\n* Crossover/recombination\r\n* Mutation\r\n* (regrouping)\r\n* (colonization-extinction)\r\n* (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 125, "article": 10}}, {"pk": 172, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-13T12:58:55.428Z", "edited_by": 9, "parent": 171, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n  $$(\\sigma \\union N)\\*N(\\sigma \\union N)\\* \\rightarrow (\\sigma \\union \\N)\\*$$ \r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string ($$\\epsilon$$) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols $$\\alpha$$ is the set of terminals that begin the strings derived from $$\\alpha$$. FIRST($$\\alpha$$) can be constructed like this for each symbol $$X$$ in $$\\alpha$$:\r\n\r\n1. If $$X$$ is a terminal, FIRST($$X$$) = {$$X$$}.\r\n2. If $$X \\rightarrow \\epsilon$$ is a production, add $$\\epsilon$$ to FIRST($$X$$)\r\n3. If $$X \\rightarrow Y\\_1 Y\\_2 ... Y\\_k$$ then add FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) to FIRST($$X$$)\r\n4. FIRST($$Y\\_1 Y\\_2 ... Y\\_k$$) is one of:\r\n    * FIRST($$Y\\_1$$) if FIRST($$Y\\_1$$) doesn't contain $$\\epsilon$$.\r\n    * FIRST($$Y\\_1$$) + FIRST($$Y\\_2 ... Y\\_k$$) - {$$\\epsilon$$}\r\n\r\nIf FIRST($$Y_i$$) contains $$\\epsilon$$ for all $$0 \\le i \\le k \\in \\mathbb{\\integers}$$, add $$\\epsilon$$ to FIRST($$X$$)\r\n\r\nThe **FOLLOW** set of a nonterminal $$A$$ is the set of terminals $$a$$ that can appear immediately to the right of $$A$$. In other words: the set of terminals $$a$$ such that there exists a derivation of the form $$S \\rightarrow \\alpha A a \\beta$$. FOLLOW($$A$$) for all nonterminals $$A$$ can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW($$S$$), where $$S$$ is the start symbol.\r\n2. If there is a production $$A \\rightarrow \\alpha B \\beta$$, then everything in FIRST($$\\beta$$) - {$$\\epsilon$$} is placed in FOLLOW($$B$$)\r\n3. If there is a production $$A \\rightarrow \\alpha B$$ or a production $$A \\rightarrow \\alpha B \\beta$$ where FIRST($$\\beta$$) contains $$\\epsilon$$, everything in FOLLOW($$A$$) is in FOLLOW($$B$$).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites.\r\n\r\nShift-reduce parsing is a category of commonly used methods for bottom-up parsing. Shift-reduce is based on two basic steps: shift and reduce. Shift advances in the input stream by one token, making the new symbol a new single-node parse tree. Reduce applies a completed grammar rule to one the recent parse trees, joining them together as one tree with a new root symbol.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. LR-parsing is a shift-reduce parsing method. Canonical LR-parsing, though fast, is prohibitively spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n\r\n\r\n\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 173, "article": 5}}, {"pk": 128, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-10T09:19:12.522Z", "edited_by": 3, "parent": 127, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete decision problems\r\n\r\nThis is a list of important or otherwise famous NP-complete decision problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nCan be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. \r\n\r\n# Optimization problems\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **psuedo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(-x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| ** Genetics term ** || ** Corresponding algorithmics term **                       ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$ P = {a_1, a_2, ..., a_k} $$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n* Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals ** lsjifasdfw ** and ** areighifpo ** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n* Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: ** lsjifhifpo ** could mutate to ** lsjifaifpo **, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n* (regrouping)\r\n* (colonization-extinction)\r\n* (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!", "child": 129, "article": 10}}, {"pk": 74, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-07T11:33:06.364Z", "edited_by": 9, "parent": 73, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 143, "article": 5}}, {"pk": 149, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T12:34:30.516Z", "edited_by": 3, "parent": 148, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n    $$(\\sigma \\union N)*N(\\sigma \\union N)* \\rightarrow (\\sigma \\union \\N)*\r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Parsing\r\n\r\n\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n* alias analysis\r\n* pointer analysis\r\n* shape analysis\r\n* escape analysis\r\n* array access analysis\r\n* dependence analysis\r\n* control flow analysis\r\n* data flow analysis\r\n    * use-define chain analysis\r\n    * live variable analysis\r\n    * available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: A node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go throug h $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$.\r\n\r\n### Interference graphs\r\n\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 150, "article": 5}}, {"pk": 179, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-13T14:04:56.997Z", "edited_by": 9, "parent": 178, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of [\"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum](http://www.cs.cornell.edu/Courses/cs412/2008sp/lectures/lec23.pdf).\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) + (int), but disallow (str) + (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar G = (N,\u03c3,P,S) consists of:\r\n\r\n* A finite set N of nonterminal symbols\r\n* A finite set \u03c3 of terminal symbols that is disjoint from N.\r\n* A finite set P of production rules, each rule on the form:\r\n  (\u03c3 \u222a N)* N(\u03c3 \u222a N)* \u2192 (\u03c3 \u222a N)* \r\n* A symbol S \u2208 N that is the start symbol\r\n\r\n\r\nThe language of a grammar G, L(G), is defined as all the sentences that can be derived in a finite number of steps from the start symbol S.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, \u03b5 or \u039b for the empty string, and S for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form V\u2192v, where V is a non-terminal and v may be a combination of terminals and non-terminals.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form \u03b1A\u03b2\u2192\u03b1\u03b3\u03b2, where \u03b1, \u03b2 and \u03b3 are strings of nonterminals and terminals, and \u03b3 is not \u03b5.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string (\u03b5) is a valid input, allowing for \"spontaneous\" transitions between states without input. Also a symbol can label several edges out of the same state.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n##### FIRST and FOLLOW sets\r\n\r\nWhen constructing a predictive parser, FIRST and FOLLOW sets are nice (read: necessary) to have. They allow us to fill in the entries of a predictive parsing table for a given grammar.\r\n\r\nThe **FIRST** set of a string of symbols \u03b1 is the set of terminals that begin the strings derived from \u03b1. FIRST(\u03b1) can be constructed like this for each symbol X in \u03b1:\r\n\r\n1. If X is a terminal, FIRST(X) = {X}.\r\n2. If X\u2192\u03b5 is a production, add \u03b5 to FIRST(X)\r\n3. If X\u2192Y1Y2\u2026Yk then add FIRST(Y1Y2\u2026Yk) to FIRST(X)\r\n4. FIRST(Y1Y2\u2026Yk) is one of:\r\n    * FIRST(Y1) if FIRST(Y1) doesn't contain \u03b5.\r\n    * FIRST(Y1) + FIRST(Y2\u2026Yk) - {\u03b5}\r\n\r\nIf FIRST(Yi) contains \u03b5 for all 0\u2264i\u2264k \u2208 \u2124, add \u03b5 to FIRST(X)\r\n\r\nThe **FOLLOW** set of a nonterminal A is the set of terminals a that can appear immediately to the right of A. In other words: the set of terminals a such that there exists a derivation of the form S\u2192\u03b1Aa\u03b2. FOLLOW(A) for all nonterminals A can be computed like this:\r\n\r\n1. Place $ (right endmarker) in FOLLOW(S), where S is the start symbol.\r\n2. If there is a production A\u2192\u03b1B\u03b2, then everything in FIRST(\u03b2) - {\u03b5} is placed in FOLLOW(B)\r\n3. If there is a production A\u2192\u03b1B or a production A\u2192\u03b1B\u03b2 where FIRST(\u03b2) contains \u03b5, everything in FOLLOW(A) is in FOLLOW(B).\r\n\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites.\r\n\r\nShift-reduce parsing is a category of commonly used methods for bottom-up parsing. Shift-reduce is based on two basic steps: shift and reduce. Shift advances in the input stream by one token, making the new symbol a new single-node parse tree. Reduce applies a completed grammar rule to one the recent parse trees, joining them together as one tree with a new root symbol.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. LR-parsing is a shift-reduce parsing method. Canonical LR-parsing, though fast, is prohibitively spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n\r\n\r\n\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Optimizations and when to apply them\r\n\r\nCopied from slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n### High IR\r\n\r\n#### Function Inlining\r\n\r\n#### Function cloning\r\n\r\n### Low IR\r\n\r\n#### Constant folding\r\n\r\nEvaluate constant expressions and change them with their value (change x = 3 * 2 to x = 6).\r\n\r\n#### Constant propagation\r\n\r\nIf we know that x has the value 6, we may change subsequent reads of x with the value 6 until x is overwritten.\r\n\r\n#### Value numbering\r\n\r\n#### Dead code elimination\r\n\r\n#### Loop-invariant code motion\r\n\r\n#### Common sub-expression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n#### Strength reduction\r\n\r\n#### Constant folding & propagation (?)\r\n\r\n#### Branch prediction/optimization\r\n\r\n#### Loop unrolling\r\n\r\n### Assembly\r\n\r\n#### Register allocation\r\n\r\n#### Cache optimization\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node d dominates a node n if every path from the start node to n must go through d. We write: d dom n, or d >> n. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 180, "article": 5}}, {"pk": 155, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-11T14:29:20.386Z", "edited_by": 3, "parent": 154, "title": "Compilers", "content": "What is a compiler?\r\nCompiling code is very useful.\r\n\r\n# Practical information\r\n\r\n# Recommended reading\r\n* Language classification\r\n  * Strong and weak typing\r\n  * Dynamic and static typing\r\n* Grammars and parsing\r\n  * Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n  * FIRST and FOLLOW sets\r\n  * LL-parsing\r\n    * Left-recursive grammars\r\n  * LR-parsing\r\n    * SLR\r\n    * LALR\r\n  * Handling ambiguity\r\n    * Operator precedence\r\n    * Shift/reduce\r\n    * The dangling else problem\r\n* Optimizations\r\n  * When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n  * Data-flow analysis\r\n    * Control flow graphs\r\n      * Transfer functions (monotonic, distributive)\r\n      * Different types of analysis\r\n      * Dominator trees\r\n    * Interference graphs\r\n* Put lattices somewhere in here\r\n\r\n# Compilers - how do they work?\r\n\r\nCompilers are programs which translate structured text from one language to another. This course studies compilers that compile computer programs. A compiler is usually built of several parts:\r\n\r\n## Scanner\r\n\r\nA scanner reads the input text as a stream of characters and translates them into a stream of tokens. Tokenizer is another word for scanner. The scanner is a lexical analyzer.\r\n\r\n## Parser\r\n\r\nA parser reads the stream of tokens from the scanner and translates them into an abstract syntax tree using the grammar of the target language as a guide, so to speak. The parser is a syntax/semantic analyzer.\r\n\r\n## Intermediate code generator\r\n\r\nAn intermediate code generator generates the abstract syntax tree for a simple intermediate code representation. This is to make things easier for the optimizer in the next step.\r\n\r\n## Intermediate code optimizer\r\n\r\nThe intermediate code optimizer optimizes the intermediate code using lots of nifty tricks and techniques that are described in detail later in this compendium.\r\n\r\n## Target language code generator\r\n\r\nThe target language code generator finally generates the final code in the target language, based on the optimized intermediate code.\r\n\r\n# Language classification\r\n\r\nProgramming languages can be classified into many different classes, based on things such as type system, paradigm, language generation, use areas and so on, but this course seems to focus on classifications based on type systems.\r\n\r\n## Dynamic vs static\r\n\r\nA programming language is **dynamically typed** when most of its type checking is performed at run-time instead of at compile-time. Values have types, but variables do not. Popular dynamic languages include Python, Ruby, PHP, Lisp, JavaScript, Objective-C, Oz (which happens to be the best language ever), and Smalltalk.\r\n\r\nA programming language is **statically typed** if it does most of its type checking at compile-time. Static typing is a form of program verification because it can in its purest form guarantee type safety before the program is run. Popular static languages include Pascal, Ada, C/C++, Java/C#, ML, Fortran.\r\n\r\nDynamic languages are typically more expressive, but static languages are safer (as in program verification) and often faster.\r\n\r\n## Strong vs weak\r\n\r\nA programming language is **strongly typed** if it specifies restrictions on which types can be co-operated upon by an operator. A language may for instance allow (int) $$+$$ (int), but disallow (str) $$+$$ (int), because str and int are not (for this example) defined to be compatible types for the + operator. Popular strongly typed languages include Pascal, C#/Java, Python, OCaml and Ada.\r\n\r\nA programming language is **weakly typed** if it does not specify restrictions on which types can be co-operated upon by an operator. Weakly typed programming languages often support features like implicit type conversion, ad-hoc polymorphism. Popular weakly typed languages include BASIC, JavaScript, Perl, PHP and C.\r\n\r\nWeakly typed languages are typically more expressive, but strongly typed languages are safer (as in program verification).\r\n\r\n# Grammars\r\n\r\nA grammar is a set of production rules for strings in a formal language. Grammars do not specify meaning of strings, or what can be done with them, or anything like that. Grammars only describe the form of the strings.\r\n\r\nA grammar $$G = (N,\\sigma,P,S)$$ consists of:\r\n* A finite set $$N$$ of nonterminal symbols\r\n* A finite set $$\\sigma$$ of terminal symbols that is disjoint from $$N$$.\r\n* A finite set $$P$$ of production rules, each rule on the form:\r\n    $$(\\sigma \\union N)*N(\\sigma \\union N)* \\rightarrow (\\sigma \\union \\N)*\r\n* A symbol $$S \\in N$$ that is the start symbol\r\n\r\nThe language of a grammar $$G$$, $$L(G)$$, is defined as all the sentences that can be derived in a finite number of steps from the start symbol $$S$$.\r\n\r\n## Notation\r\n\r\nWhen writing about grammars, the notation convention is to use capital letters for nonterminals, lowercase letters for terminals, $$\\epsilon$$ or $$\\Lambda$$ for the empty string, and $$S$$ for the start symbol. \r\n\r\n## The Chomsky hierarchy\r\n\r\nGrammars are classified into a hierarchy based on the restrictiveness of the production rules. A more restrictive class can express fewer languages than a less restrictive class. The Chomsky hierarchy is, in order or decreasing restrictiveness:\r\n    Regular \u2286 Context-free \u2286 Context-sensitive \u2286 Unrestricted\r\n\r\n### Regular grammars\r\n\r\nRegular grammars are the most restrictive grammars in the grammar hierarchy. Regular grammars describe a regular language.\r\n\r\n### Context-free grammars\r\n\r\nContext-free grammars are grammars where every production rule is of the form $$V \\rightarrow v$$.\r\n\r\n### Context-sensitive grammars\r\n\r\nContext-sensitive grammars are grammars where every production is of the form $$\\alpha A \\beta \\rightarrow \\alpha \\gamma \\beta$$, where $$\\alpha$$, $$\\beta$$ and $$\\gamma$$ are strings of nonterminals and terminals, and $$\\gamma$$ is not $$\\epsilon$$.\r\n\r\n### Unrestricted grammars\r\n\r\nAn unrestricted grammar is a grammar with no restrictions for the grammar productions.\r\n\r\n\r\n# Finite automata\r\n\r\nFinite automaton consist of:\r\n\r\n* A finite set of state\r\n* Transitions between states\r\n* A start start\r\n* A set of final states\r\n\r\nThere are two kinds of finite automaton studied in this course: NFA and DFA. Here is a series of random facts about finite automatons:\r\n* Every NFA has an equivalent DFA.\r\n* There exists algorithms which can convert NFA to DFA and back.\r\n* Every finite automaton is equivalent with a regular expression.\r\n* finite automatons can be thought of as deciders deciding the question \"is this input gramatically correct?\" for the language associated with the finite automaton.\r\n\r\n## Deterministic finite automata (DFA)\r\n\r\nDFA is a kind of finite automaton which has exactly one transition for each state for each possible input symbol.\r\n\r\n## Non-deterministic finite automata (NFA)\r\n\r\nNFA is a kind of finite automaton which can have more than one transition per state per input symbol. They do not have to have defined transitions for each state for each symbol, and the empty string is a valid input, allowing for \"spontaneous\" transitions between states without input.\r\n\r\n# Parsing\r\n\r\n## Top down parsing\r\n\r\nTop down parsers look for left-most derivations of an input-stream by searching for parse trees using a top-down expansion of a given grammar's production rules.\r\n\r\n### Recursive descent parser\r\n\r\nA top-down parser built from a set of mutually recursive procedures where each such procedure usually implements one of the production rules of the grammar.\r\n\r\n#### Predictive parser (LL-parser)\r\n\r\nA predicitve parser is a recursive descent parser that does not require backtracking. Predictive parsers only work for LL(k)-grammars.\r\n\r\n## Bottom-up parsing\r\n\r\nBottom-up parsers start with the input stream and attempt to reach the start symbol through a series of rewrites. Shift-reduce parsing is another name for bottom-up parsing.\r\n\r\n### LR-parser\r\n\r\nLR-parsers are left-reading, rightmost-derivating parsers that are O(n) for deterministic context-free languages. Canonical LR-parsing, though fast, is prohibitly spaceous. Luckily, memory-usage-improved parsers exist, such as the SLR and LALR parsers.\r\n\r\n#### Simple LR-parser (SLR-parser)\r\n\r\nAn SLR-parser is an LR(1)-parser with a relatively simple parser generator algorithm. SLR lookahead set generators calculate lookahead by an easy approximation method (follow sets) based direcly on the grammar, ignoring everything else.\r\n\r\n#### Look-Ahead LR parser (LALR-parser)\r\n\r\nAn LALR-parser is an LR(k)-parser (k>0), and a simplified version of a canonical LR-parser. LALR lookahead set generators calculate lookahead by a more precise method based on exploring the graph of parser states and their transitions. \r\n\r\n## FIRST and FOLLOW sets\r\n## shift/reduce\r\n## Handling ambiguity\r\n### Operator precedence\r\n### The dangling else problem\r\n\r\n\r\n\r\n# Optimizations\r\n\r\nWhen compiling code, a compiler should generally try to optimize its output as much as possible. When a compiler optimizes its output, it is called an optimizing compiler. Optimization in the context refers to the processing of minimizing some attributes of the output program, most commonly execution time, but also memory footprint, without changing the semantics of the program. Optimization is typically computationally intense, with some problems being NP-complete or even undecidable.\r\n\r\n## Data-flow analysis\r\n\r\nData-flow analysis-based optimizations are optimizations based on data-flow analysis. A data-flow analysis is a technique for gathering information about the possible set of values calculated at various points in a program. \r\n\r\n### Common subexpression elimination\r\n\r\nWikipedia says it best: \"In the expression (a + b) - (a + b)/4, \"common subexpression\" refers to the duplicated (a + b). Compilers implementing this technique realize that (a + b) won't change, and as such, only calculate its value once.\"\r\n\r\n### Constant folding and propagation\r\n\r\nConstant folding and propagation is the process of pre-computing expressions consisting of constants and replacing them with their final value. As an example, the expression $$3 + 2$$ can be replaced by the expression $$5$$.\r\n\r\n### Additional techniques\r\n\r\nThese might not be part of the curriculum, I am not sure:\r\n* Induction variable recognition and elimination\r\n* Alias classification and pointer analysis\r\n* Dead store elimination\r\n\r\n### Control flow graphs (CFG)\r\n\r\nA control flow graph represents all code paths that might be traversed through a program during its execution. Each node in the graph represents a basic block. A basic block is a continuous piece of code without jumps or jump targets. Possible jumps are represented as directed edges between nodes. Additionally, on block is marked as an entry block where execution starts, and one block is marked as an exit block, where execution ends.\r\n\r\n#### Reachability\r\n\r\nSubgraphs that are reachable from the entry node can safely be discarded from the graph, as they can never be executed. This is an important technique for dead code elimination.\r\n\r\nIf the exit block is unreachable, the program has an infinite loop. Although not all infinite loops can be detected, it is still nice to detect some of them!\r\n\r\n#### Transfer functions \r\n\r\nI have no idea what this is.\r\n\r\n#### Different types of analysis\r\n\r\n##### alias analysis\r\n##### pointer analysis\r\n##### shape analysis\r\n##### escape analysis\r\n##### array access analysis\r\n##### dependence analysis\r\n##### control flow analysis\r\n##### data flow analysis\r\n###### live variable aka liveness analysis\r\n\r\nA variable in a program is live at a point of the program if it holds a value that may be needed in the future. Read up on lattices, because they are useful when doing live variable analysis.\r\n\r\nA use-define chain (UD chain) is a data structure that is used in liveness analysis. UD chains are data structures that consist of a use of a variable (U) and all the definitions of said variable that can reach that use without any other intervening definitions (D). Usually, a definition means an assignment of a value to a variable.\r\n\r\nA UD chain makes it possible to lookup for each usage of a variable, which earlier assignments its current value depends on. This facilitates splitting variables into several different variables when variables have a definition chain of length 1. This is very useful for putting things into static single assignment form.\r\n\r\n###### available expression analysis\r\n\r\n\r\n#### Dominator trees\r\n\r\nLemme explain: In a CFG, a node $$d$$ dominates a node $$n$$ if every path from the start node to $$n$$ must go through $$d$$. We write: $$d$$ dom $$n$$, or $dd$$ >> $$n$$. A dominator tree is tree where each node's chidren are those nodes it immediately dominates. The start node is the root of the tree. Dominator trees are used for computing static single assigment form for an IR - that is that every variable is assigned exactly once. \r\n\r\n### Interference graphs\r\n\r\nInterference graphs are used for register allocation. An interference graph is a graph where nodes represent variables in a program, and edges between them represent intferences between them. If two variables do not interefere, the same register can be used for both of them, reducing the number of registers needed.\r\n\r\n# Nice tips for the final\r\n\r\n* The difference between DFAs and NFAs\r\n\r\n* Scope of identifiers in object-oriented class systems. See slide number 12 of \"Lecture 12: Symbol Tables\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* Classifying different well-known programming languages as strong/weakly typed, static/dynamic typing (\" given these languages, where would you put them? Ex.: fortran, cobol, matlab, python etc\")\r\n* Function call on objects, dynamic dispatch vector, see slide number 6 of \"Lecture 22: Implementing Objects\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n* When to implement what optimization, see slide number 27 of \"Lecture 23: Introduction to Optimizations\", CS412/413 Spring 2008 Slide Set by Tim Teitelbaum.\r\n\r\n* FP \u2291 MFP \u2291 MOP \u2291 IDEAL, viktig i komptek; lecture 27, slide 22; ogs\u00e5 slide 15 i neste sett\r\n* multiple choice tip for compilers: is c weakly typed? yes.\r\n\r\n\r\n", "child": 156, "article": 5}}, {"pk": 135, "model": "wiki.articlecontent", "fields": {"lang": "nb", "updated": "2013-05-14T12:12:01.881Z", "edited_by": 5, "parent": null, "title": "Software Engineering", "content": "# Eksamen i SU 2013\r\n- Skal ikke bli noen overraskelser\r\n- \"\u00c5pen bok\"-eksamen -> Alle trykte og h\u00e5ndskrevende hjelpemidler (kode A)\r\n\r\n- Print slides og ta med bok\r\n\r\n- L\u00f8s eksamene som ligger ute\r\n\r\n- Lag diagrammer for de tre eksempelsystemene, en av hver UML-type. Ta med p\u00e5 eksamen\r\n\r\n# Eksamen:\r\n1. 30 %: True/false +1.5/-0.5\r\n2. 30 %: Modell, spec, design: kap. 4, 5, 6, 7 + fellesprosjektcase\r\n3. 20 %: Testing: kap 8 (se oppgave 8.7, s. 232)\r\n4. 20 %: Prosess, proj. management: (kap 1, 2, 3, 22, 23) Gantt, rescue-plan", "child": null, "article": 12}}, {"pk": 191, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-15T14:02:14.641Z", "edited_by": 9, "parent": 190, "title": "Artificial Intelligence Methods", "content": "*   Strong/weak AI\r\n*   Agents\r\n    *   Rationality\r\n*   Bayesian networks\r\n    *   Syntax and semantics\r\n    *   Uses and problems\r\n    *   Conditional independence\r\n*   Decision networks/Influence diagrams\r\n*   Reasoning\r\n    *   Case-based reasoning\r\n    *   Instance-based reasoning\r\n*   Markov processes\r\n    *   Markov assumptiom\r\n*   Neural networks\r\n*   Decision trees\r\n\r\n# Bayesian Networks\r\n\r\nA Bayesian network is a model of a set of random variables and their conditional dependencies through a directed acyclic graph. In the graph, the nodes represent the random variables and the edges conditional dependencies.\r\n\r\nMuch of the calculation in a bayesian network is based on Bayes' theorem: $$ P(A|B)=\\frac{P(B|A)P(A)}{P(B)} $$.\r\n\r\n## Bayesian networks in use\r\n\r\n* Good: Reason with uncertain knowledge. \"Easy\" to set up with domain experts (medicine, diagnostics).\r\n* Worse: Many dependencies to set up for most meaningful networks.\r\n* Ugly: Non-discrete models\r\n\r\nBad at for example image analysis, ginormous networks.\r\n\r\n# Markov Chains\r\n\r\nA Markov chain is a random (and generally discrete) process where the next state only depends on the current state, and not previous states. There also exists higher-order markov chains, where the next state will depend on the previous n states. Note that any k'th-order Markov process can be expressed as a first order Markov process.\r\n\r\nIn a Markov chain, you typically have observable variables which tell us something about something we want to know, but can not observe directly.\r\n\r\n## Operations on Markov Chains\r\n\r\n### Filtering\r\n\r\nCalculating the unobservable variables based on the evidence (the observable variables).\r\n\r\n### Prediction\r\n\r\nTrying to predict how the variables will behave in the future based on the current evidence.\r\n\r\n### Smoothing\r\n\r\nEstimate past states including evidence from later states.\r\n\r\n### Most likely explanation\r\n\r\nFind the most likely set of states.\r\n\r\n## Bernoulli scheme\r\n\r\nA special case of Markov chains where the next state is independent of the current state.\r\n\r\n# Learning\r\n\r\n## Decision trees\r\n\r\nDecision trees are a way of making decision when the different cases are describable by attribute-value pairs. The target function should be discretely valued. Decision trees may also perform well with noisy training data.\r\n\r\nA decision tree is simply a tree where each internal node represents an attribute, and it's edges represent the different values that attribute may hold. In order to reach a decision, you follow the tree downwards the appropriate values until you reach a leaf node, in which case you have reached the decision.\r\n\r\n### Building decision trees\r\n\r\nThe general procedure for building decision is a simple recursive algorithm, where you select an attribute to split the training data on, and then continue until all the data is classified/there are no more attributes to split on. How big the decision tree is depends on how you select which attribute you split on.\r\n\r\nFor an optimal solution, you want to split on the attribute which holds the most information, the attribute which will reduce the entropy the most.\r\n\r\n#### Overfitting\r\n\r\nWith training sets that are large, you may end up incorporating the noise from the training data which increases the error rate of the decision tree. A way of avoiding this is to calculate how good the tree is at classifying data while removing every node (including the nodes below it), and then greedily remove the one that increases the accuracy the most.\r\n\r\n# Case-based reasoning\r\n\r\nCase-based reasoning assumes that similar problems have similar solutions and that what works today still will work tomorrow. It uses this as a framework to learn and deduce what to do in both known and unknown circumstances.\r\n\r\n## Main components\r\n\r\n* Case base\r\n  * Previous cases\r\n* A method for retrieving relevant cases with solutions\r\n* Method for adapting to current case\r\n* Method for learning from solved problem\r\n\r\n## Approaches\r\n\r\n### Instance-based\r\n\r\n*Answer:* Describe the four main steps of the case-based reasoning (CBR) cycle. What\r\nis the difference between \u201cinstance-based reasoning\u201d and \u201ctypical CBR\u201d?\r\n\r\nAn approach based on classical machine learning for classification tasks. Attribute-value pairs and a similarity metric.\r\n\r\n### Memory-based reasoning\r\n\r\nAs instance-based, only massively parallel(?), finds distance to every case in the database.\r\n\r\n### Analogy-based\r\n\r\nMotivated by psychological research. Cross-domain cases, problem solving. Background knowledge.", "child": 192, "article": 9}}, {"pk": 186, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-15T09:41:03.892Z", "edited_by": 3, "parent": 185, "title": "Algorithm Construction, Advanced Course", "content": "# Practical information\r\n\r\n* The exam is held on the 22. of May 2013 at 9 am.\r\n* All printed or hand-written materials may be used during the exam (including the textbook, and even this compendium!). Citizen SR-270X is also permitted.\r\n* [Old exams](http://algkons-wiki.idi.ntnu.no/index.php/Old_Exams)\r\n\r\n# Curriculum / reading list\r\n\r\nThis year (2013), the curriculum is chapters 3, 4 and 6, except 4.3.6-4.5, of Hromcovic's _Algorithmics for hard problems_.\r\n\r\nThe topics covered are:\r\n\r\n* Deterministic approaches\r\n    * Pseudo-polynomial-time algorithms\r\n    * Parametrized complexity\r\n    * Branch and bound\r\n    * Lowering worst case complexity of exponential algorithms\r\n    * Local serach\r\n    * Relaxation to linear programming\r\n* Approximation algorithms\r\n    * Fundamentals: Stability, Dual approximation etc\r\n    * Algorithm design: lots of approximations for known hard problems\r\n* Heuristics\r\n    * Simulated annealing\r\n    * Tabu search\r\n    * Genetic algorithms\r\n\r\n# Formal languages\r\n\r\n## Alphabet\r\n\r\n> Any non-empty, finite set is called an **alphabet**. Every element of an alphabet \u03a3 is called a **symbol** of \u03a3.\r\n\r\nExample alphabets:\r\n\r\n* \u03a3\\_bool = {0, 1}\r\n* \u03a3\\_latin = {a, b, c, d, e , ... , z }\r\n* \u03a3\\_logic = {0, 1, (,), \u2227, \u2228, \u00ac, x }\r\n\r\n## Word\r\n\r\n> Let \u03a3 be an alphabet. A **word** over \u03a3 is any finite sequence of symbols of \u03a3. The **empty word** \u03bb is the only word consisting of zero symbols. The set of all words over the alphabet \u03a3 is denoted by \u03a3*.\r\n\r\n## Language\r\n\r\nLet \u03a3 be an alphabet. Every set _L_ \u2286 \u03a3* is called a **language** over \u03a3.\r\n\r\n## Time\\_A(x) and Space\\_A(x)\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be alphabets. Let *A* be an algorithm that realizes a mapping from \u03a3\\_input* to \u03a3\\_output*. For every _x_ \u2208 \u03a3\\_input*, _Time\\_A(x)_ denotes the time complexity of the computation _A_ on the input _x_, and _Space\\_A(x)_ denotes the space complexity of the computation _A_ on _x_.\r\n\r\n## Time complexity of an algorithm\r\n\r\n> Let \u03a3\\_input and \u03a3\\_output be two alphabets. Let _A_ be an algorithm that computes a mapping from \u03a3\\_input* to \u03a3\\_output*. ** The worst case time complexity of _A_ ** is a function _Time\\_A_: (\u2115 - {0}) \u2192 \u2115  defined by\r\n>     _Time\\_A(n)_ = max{_Time\\_A(x)_ | _x_ \u2208 \u03a3\\_input\\_n\r\n> for every positive integer _n_.\r\n\r\n\r\n# Complexity classes\r\n\r\n## Time\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DTIME(_f_(_n_))      || Deterministic Turing machine         || Time _f_(_n_)           ||\r\n|| P                    || Deterministic Turing machine         || Time poly(_n_)          ||\r\n|| EXPTIME              || Deterministic Turing machine         || Time 2^(poly(_n_))      ||\r\n|| NTIME(_f_(_n_))      || Non-deterministic Turing machine     || Time _f_(_n_)           ||\r\n|| NP                   || Non-deterministic Turing machine     || Time poly(_n_)          ||\r\n|| Co-NP                || TBA                                  || TBA                     ||\r\n|| NEXPTIME             || Non-deterministic Turing machine     || Time 2^(poly(_n_))      ||\r\n\r\n## Space\r\n\r\n|| **Complexity class** || **Model of computation**             || **Resource constraint** ||\r\n|| DSPACE(_f_(_n_))     || Deterministic Turing machine         || Space _f_(_n_)          ||\r\n|| L                    || Deterministic Turing machine         || Space O(log _n_)        ||\r\n|| PSPACE               || Deterministic Turing machine         || Space poly(_n_)         ||\r\n|| EXPSPACE             || Deterministic Turing machine         || Space 2^(poly(_n_))     ||\r\n|| NSPACE(_f_(_n_))     || Non-deterministic Turing machine     || Space _f_(_n_)          ||\r\n|| NL                   || Non-deterministic Turing machine     || Space O(log _n_)        ||\r\n|| NPSPACE              || Non-deterministic Turing machine     || Space poly(_n_)         ||\r\n|| NEXPSPACE            || Non-deterministic Turing machine     || Space 2^(poly(_n_))     ||\r\n\r\n# Cost measurement\r\n\r\nTime efficiency estimates depends on what is defined to be a single step, which takes a single time unit to execute. Two cost models are generally used:\r\n\r\n## Uniform-cost measurement\r\n\r\nEvery machine operation is assigned a single constant time cost. This means that an addition between two integers, as an example, is assumed to take the same amount of time regardless of the size of the integers. This is often the case in practical systems for sensibly sized integers. This is the cost measurement used throughout Hromkovi\u010d.\r\n\r\n## Logarithmic-cost measurement\r\n\r\nCost is assigned to the number of bits involved in each operation. This is more cumbersome to use, and is therefore usually only applied when necessary. \r\n\r\n\r\n# Decision problems\r\n\r\nA decision problem is a question with a yes-or-no answer, depending on the input parameters. Formally, it is represented as a triple (_L_,_U_,\u03a3) where \u03a3 is an alphabet, _L_ and _U_ are languages , and _L_ \u2286 _U_ \u2286 \u03a3*.  When _U_ = \u03a3*, which happens quite often, (_L_,\u03a3) can be used as a shorthand. An algorithm A solves (_L_,_U_,\u03a3) if for every _x_ \u2208 _U_:\r\n\r\n* A(_x_) = 1 if _x_ \u2208 _L_\r\n* A(_x_) = 0 if _x_ \u2208 _U_ - _L_\r\n\r\nA decision problem is equivalent to a language.\r\n\r\n## Famous NP-complete problems\r\n\r\nThis is a list of important or otherwise famous NP-complete problems, which are probably nice to know by heart, as they assumed to be known, and can therefore be used freely in proofs.\r\n\r\n### Hamiltonian cycle problem (HC)\r\n\r\nThe Hamiltonian cycle problem is the problem of determining wheter a given graph contains a Hamiltonian cycle.\r\n\r\n### Satisfiability problem (SAT)\r\n\r\nThe boolean satisfiability problem is the problem of determining whether a given boolean expression in CNF can be satisfied. An expression in CNF is an expression of the form: (_A_ \u2228 _B_ \u2228 ...) \u2227 (\u00ac_A_ \u2228 ...) \u2227 ...  SAT was the first known NP-complete problem.\r\n\r\n### Clique problem (CLIQUE)\r\n\r\nThe clique problem is the problem of determining whether there exists a clique of size _k_ in a given graph _G_ = (_V_,_E_). A clique is a subset of _V_ _c_ such that all vertices in _c_ are connected to all other vertices in _c_ by an edge in _E_.\r\n\r\n### Vertex cover problem (VCP)\r\n\r\nThe vertex problem is the problem of determining whether a given graph _G_ = (_V_,_E_) has a vertex cover of size _k_. A vertex cover is a subset of _V_ such that all edges in _E_ are adjacent to a node in _V_. VCP is a special case of SCP. \r\n\r\nVCP can be approximated by Maximum Matching by a factor \u03c1 = 2.\r\n\r\n### Set cover problem (SCP)\r\n\r\nThe set cover problem is the problem of determining whether, given a set of elements _U_ and a set _S_ of sets of the elements in _U_ whose union equals _U_, it is possible to select _k_ or less sets from _S_ such that their union equals _U_. The optimization problem tries to minimize $$k$$.\r\n\r\nSCP can be approximated greedily by repeatedly adding the set that contains the largest number of uncovered elements to the set cover. This has an approximation factor of $$\\rho = H(s)$$, $$s$$ is the size of $$U$$ and $$H(n)$$ is the $$n$$-th harmonic number.\r\n\r\nSCP can be represented as an IP:\r\n\r\nMinimize sum $$cost(s) \\dot x\\_s$$ for all $$s \\in S$$\r\n\r\nsubject to\r\n* sum $$x\\_s$$ for $$s:e \\in S\\_s \\ge 1$$ for all $$e \\in U$$ \r\n* $$x\\_ \\in \\left\\{0,1\\}\\right$$\r\n\r\n### Travelling salesman problem (TSP)\r\n\r\nThe travelling salesman problem is the problem of finding the the shortest Hamiltonian cycle in a complete graph. While the optimization version of TSP is NP-hard, the decision version of TSP is NP-complete.\r\n\r\nTSP, with the restriction that the triangle inequality holds, can be approximated by Christofides algorithm with a ratio $$\\rho = 1.5$$. Regular MST constructive heuristic has $$\\rho = 2$$.\r\n\r\n\r\n# Pseudo-Polynomial Time Algorithms\r\n\r\nA numeric algorithm runs in pseudo-polynomial time if its running time is polynomial in the numeric value of the input. NP-complete problems with pseudo-polynomial time algorithms are called weakly NP-complete. NP-complete problems that are proven to not have a pseudo-polynomial time algorithm are called strongly NP-complete. Strong and weak kinds of NP-hardness are defined analogously.\r\n\r\nFormally:\r\n\r\n> Let _U_ be an integer-valued problem, and let _A_ be an algorithm that solves _U_. _A_ is a **pseudo-polynomial-time algorithm for _U_** if there exists a polynomial _p_ of two variables such that _Time\\_A_(_x_) = _O_(_p_(|_x_|, _Max-Int_(_x_))) for every instance _x_ of _U_.\r\n\r\n## Example: Primality test\r\n\r\nConsider the decision problem of whether a number _n_ is a prime number. The na\u00efve approach of checking whether each number from 2 to \u221a_n_ evenly divides n is sub-linear in the value of _n_, but exponential in the size of _n_.\r\n\r\n# Parametrized complexity\r\n  \r\n## Fixed-parameter polynomial time algorithms\r\n\r\nA parametrized problem is a language _L_ \u2286 \u03a3* \u00d7 \u2115. The second component is called the parameter of the problem. A parametrized problem _L_ is _fixed-parameter tractable_ if the question (_x_,_k_) \u2208? _L_ can be decided in running time _f_(_k_) \u00b7 |_x_|^_O_(1), where _f_ is an arbitrary function depending only on _k_. In such cases, it is often practical to fix the parameter _k_ to a small(ish) constant.\r\n\r\n### Example: SAT\r\n\r\nSAT can be parametrised by the number of variables. A given boolean expression of size _x_ with _k_ variables can be checked by brute force in time _O_(2^_k_ \u00b7 _x_). Here, _f_(_k_) = 2^_k_, and |_x_|^_O_(1) = _x_.\r\n\r\n\r\n# Approximation algorithms\r\n\r\nApproximation algorithms are algorithms that calculate an approximate answer to an optimization problem. Informally, approximation algorithms are said to have an approximation ratio of $$\\rho$$ if the algorithm at worst produces an answer which is $$\\rho$$ times worse than the optimal answer. In a manner analogous to numerical stability, approximation algorithms are said to be stable if small changes in the approximation parameters result in correspondingly small changes in the answer. \r\n\r\n## Polynomial-time approximation scheme (PTAS)\r\n\r\nA polynomial-time approximation algorithm is a PTAS if the approximation error is bounded for each input, and Time_A(x, 1/error) is polynomial in |x|.\r\n\r\n## Fully polynomial-timme approximation scheme (FPTAS)\r\n\r\nAn PTAS is FPTAS is it is polynomial in |x| and 1/error.\r\n\r\n\r\n# Local Search\r\n\r\nLocal search is a metaheuristic method for optimization problems. Local search moves from solution to solution in the search space by applying local changes until an sufficiently optimal solution is found, or until a time bound is elapsed.\r\n\r\n## Straight up regular local search\r\n\r\nFor your basic local search you need the following:\r\n\r\n* A neighbor graph showing the relation between neighboring solution candidates\r\n* A fitness function evaluating candidate solutions\r\n* Good luck\r\n\r\nStraight up regular local search will quickly find a local optimum from where the search is started, but does not guarantee finding a global maximum, nor does it know if a global maximum has been found.\r\n\r\nThere are several ways of selecting a neighbor to follow when doing a local search, two of which are *first improvement* and *steepest descent*. Steepest descent evaluates all neighbors, and selects the best neighbor. First improvement selects the first neighbor which is better than the current solution candidate. First improvement is often faster to calculate, but Steepest descent usually converges faster to a local optimum.\r\n\r\n## Variable-depth search\r\n\r\nVariable depth search provides a nice compromise between first improvement and steepest descent. In variable depth search, changed variables in a solution are locked, preventing their further modification in that branch. Kernighan-Lin is the canonical example of variable-depth search.\r\n\r\n## Simulated annealing\r\n\r\nSimulated annealing introduces probability and randomness to which neighbor is selected. This reduces the chance of getting stuck in a non-global local maximum.\r\n\r\n## Tabu search\r\n\r\nTabu search marks certain solutions as tabu after processing, and never revisits them. This is often done on an LRU or LFU basis. A common implementation is a search which remembers the last _k_ visited candidate solutions, and avoids revisiting them.\r\n\r\nTabu search prevents a candidate solution to be considered repeatedly, getting the search stuck in plataus.\r\n\r\n## Randomized Tabu search\r\n\r\nLike regular Tabu, but randomized (or so the name seems to suggest).\r\n\r\n## Intensification vs Diversification\r\n\r\nIntensification is the intense search of a relatively small area - that is, the exploitation of a discovery of a good area. Diversification is looking at many diverse regions, exploring uncharted territories. Intensification is often quicker at reaching a local optimum, but diversification is better geared towards discovering global optimums.\r\n\r\n# Branch-and-bound (BB)\r\n\r\nA branch-and-bound algorithm consists of systematic enumeration of all candidate solutions, where large subsets of fruitless candidates are discarded _en masse_, by using upper and lower estimated bounds of the quantity being optimized.\r\n\r\nCanonically, BB is used to minimize a function _f_(_x_), where _x_ \u2208 (S = { some set of candidate solutions }). BB requires:\r\n\r\n* For branching: a splitting procedure that splits a candidate solution set _S_ into _S_\\_1, _S_\\_2, ..., _S\\_n_ such that S = \u222a{_S_\\_1, _S_\\_2, ..., _S\\_n_}, n \u2265 2 and the minimum of _f_(_x_) over _S_ is min{_v_\\_1, _v_\\_2, ..., _v\\_n_}, where each _v\\_i_ is the minimum of _f_(_x_) within _S\\_i_.\r\n* For bounding: a procedure that computes upper and lower bounds for the minimum value of _f_(_x_) over a given subset of _S_.\r\n\r\nFor an example of branch and bound used with Integer Programming, see [Algorithm Animations for _Practical Optimization: A gentle Introduction_](http://optlab-server.sce.carleton.ca/POAnimations2007/BranchAndBound.html)\r\n\r\n\r\n# Genetic algorithms\r\n\r\nGenetic algorithms mimic evolution to obtain practical acceptable solutions to optimization problems. See [](#genetic-algorithm-template) for an overview of how to make a genetic algorithm.\r\n\r\nSome genetic-domain-specific words need a mapping into the algorithmics domain:\r\n\r\n|| **Genetics term**   || **Corresponding algorithmics term**                         ||\r\n|| an individual       || a string (or vector) representation of a candidate solution ||\r\n|| fitness value       || cost function                                               ||\r\n|| population          || a subset of the set of feasible candidate solutions         ||\r\n|| mutation            || a random local transformation of an individual              ||\r\n\r\nHere, the different stages of the genetic algorithm as described in the [template](#genetic-algorithm-template) are explained in more detail:\r\n\r\n## Initialization\r\n\r\nInitialization is the step of creating a starting population $$P = \\left\\{a\\_1, a\\_2, ..., a\\_k\\right\\}$$ which becomes the first generation of the algorithm. Random generation is a good way of doing this, but there are other approaches.\r\n\r\n## Selection\r\n\r\nSelection is the step of selecting $$ n \\le k $$ indivudials from the population, on which genetic operators will be applied. A good selection strategy is crucial for success in a genetic algorithm. Using the fitness value to select the $$ n $$ best individuals from the population is common ('survival of the fittest'), and throwing in some randomness as well is usually a good move.\r\n\r\n## Genetic operators\r\n\r\nGenetic operators work on individuals to make new, different individuals.\r\n\r\n### Crossover/recombination\r\n\r\nCrossover is when you combine two individuals to make an offspring individual which contains combined parts from both its parents. One way of performing a crossover is to take use the first half of the first individual (a string or vector representation of a candidate solution, remember), and and the second half of the second individual.\r\n\r\nAs a simple example: consider a genetic algorithm trying to find a string of length 10 that contains only vowels. The two individuals **lsjifasdfw** and **areighifpo** are recombined using the previously described method would make a new individual **lsjifhifpo**\r\n\r\n### Mutation\r\n\r\nMutation does a slight modification of a single individual to form a new individual. Methods include flipping a single bit of the individual, adding a random number to one of the individual's vector elements, generation-number-dependant modifications, etc.\r\n\r\nFollowing with the offspring individual from the recombination example: **lsjifhifpo** could mutate to **lsjifaifpo**, using the mutation rule that a single character in the string should be swapped with a random character.\r\n\r\n### (regrouping)\r\n### (colonization-extinction)\r\n### (migration)\r\n\r\n## Termination\r\n\r\n# Linear programming (LP)\r\n\r\nAlso known as linear optimization. LP is the process of finding the \"best\" values obtainable for a function of variables constrained by linear inequalities. LP is solvable in polynomial time.\r\n\r\n## Integer programming (IP)\r\n\r\nIP is linear programming where the variables can only take integer values. IP is NP-hard.\r\n\r\n## Binary linear programming (01LP)\r\n\r\n01LP is linear programming where the variables can only take 0 or 1 as values. 01LP is NP-hard.\r\n\r\n\r\n## Relaxation to LP\r\n\r\nMany interesting optimizaition problems with useful solutions are easily reduced to IP or 01LP. Because IP and 01LP are both NP-hard, and LP is P, relaxing problems to LP is a good idea when possible. Here is how:\r\n\r\n1. Express an optimization problem _U_ as an instance _I_ of IP or 01LP.\r\n2. Pretend that _I_ is an instance of LP, and find a solution _a_.\r\n3. Use _a_ to solve the original problem.\r\n\r\nEasy, right?\r\n\r\n\r\n\r\n\r\n\r\n# Layman's guides\r\n\r\nThis section contains some useful guides to various methods and techniques for algorithm design.\r\n\r\n## Parametrized complexity applications\r\n\r\nSo you have an NP-hard problem. Nice. And difficult. So difficult that it is impossible to solve in polynomial time (unless P = NP). Anyway, here is a general plan that might help:\r\n\r\n1. Define a subproblem mechanism for defining subproblems according to their supposed subproblem difficulty.\r\n2. Use this mechanism to define a class of easy subproblems.\r\n3. Define a requirement for an algorithm to have a nice time complexity with respect to the subproblem difficulty, in such a way that a nice algorthm can solve an easy problem in polynomial time.\r\n4. Then, either:\r\n* design a nice algorithm, and prove that is nice, or\r\n* prove that it is impossible to design a nice algorithm, unless P = NP.\r\n\r\nLook at these categories for ideas for the different subproblem mechanisms, proofs, etc:\r\n\r\n### Pseudo-polynomial-time algorithms\r\n\r\n|| Subproblem mechanism || Include only instances where the maximal integer in the input is bounded by a non-decreasing function _h_ of the input size. ||\r\n|| Easy subproblems || Those where he bouding _h_ function is a polynomial. ||\r\n|| Nice algorithms || Pseudo-polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || Prove that there exists a polynomial _h_ such that _Value_(_h_) - _U_ is NP-hard. Then, the problem is strongly NP-hard. ||\r\n\r\n### Parametrized complexity\r\n\r\n|| Subproblem mechanism || Define a parametrization function _Par_ to give each input instance an integer score to measure the difficulty of the input instance. Higher _Par_(_x_) means harder difficulty. Subproblems can be defined by including only instance where _Par_(_x_) takes a specific value or range. ||\r\n|| Easy subproblems || Those where _Par_(_x_) is small and does not depend on input size. ||\r\n|| Nice algorithms || Par-parametrized polynomial-time algorithms. ||\r\n|| Designing the algorithm || Just design it ||\r\n|| Proving that there are no nice algorithms || choose a constant _k_ and prove that subproblem where _Par_(_x_) = _k_ is NP-hard. ||\r\n\r\n\r\n## Local search algorithm design\r\n\r\n0. Make sure you have an optimization problem and not a decision problem.\r\n\r\n1. Define a neighborhood function. This function maps for an input instance _x_ each feasible solution _a_ for _x_ to other feasible solutions for _x_ called the neighbors of _a_. Typically, neighboring solutions differ only by some small modification.\r\n\r\n2. Start the search anywhere you want. Randomly choosing some place is a nice strategy.\r\n\r\n3. Repeat the following: go to best neighbor (or don't, if all neighbors suck, comparitively).\r\n\r\n4. In the case where all neighbors suck, give up. You have a local optimum and the search is done.\r\n\r\n## Genetic algorithm template\r\n\r\n1. Randomly generate a population of individuals.\r\n2. Apply a fitness function to calculate a fitness score for each individual in the current generation.\r\n3. Select individuals for reproduction based on fitness and a little randomness.\r\n4. Apply crossover and mutation to the selected individuals to produce the next generation.\r\n5. Stop whenever you feel like it.\r\n\r\n\r\n## Proving that a decision problem is undecidable\r\n\r\nYou have a decision problem _E_ which you suspect is undecidable. Unfortunately, you need proof. A general method is:\r\n1. Find a different decision problem _H_ which is known to be undecidable. I recommend the halting problem.\r\n2. Suppose a decider _R_ decides _E_. Define a decider _S_ that decides _H_ using _R_.\r\n3. If _R_ exists, _S_ can solve _H_. However _H_ cannot be solved. Therefore, _R_ cannot exist.\r\n\r\n## The Primal-Dual schema\r\n\r\n(taken from cmu.edu)\r\n\r\nWe typically devise algorithms for minimization problems in the following way:\r\n\r\n1. Write down an LP relaxation of the problem, and find its dual. Try to find some intuitive meaning for the dual variables.\r\n2. Start with vectors x = 0, y = 0, which will be dual feasible, but primal infeasible.\r\n\r\n3. Until the primal is feasible:\r\n    1. increase the dual values y\\_i in some controlled fashion until some dual constraint(s) goes tight (i.e. until sum\\_i y_i * a\\_ij = c\\_j for some j), while always maintaining the deal feasibility of y.\r\n    2. Select some subset of the tight dual constraints, and increase the primal variable corresponding to them by an integral amount.\r\n\r\n4. For the analysis, prove that the output pair of vectors (x,y) satisfies c^[trans] * x <= rho * y^[trans] * b for as small a value of rho as possible. Keep this goal in mind when deciding how to raise the dual and primal variables.\r\n\r\n\r\n## Christofides algorithm\r\n\r\nUse this to approximate the TSP $$G = (V,w)$$ with $\\rho = 1.5$$, where the edge weights satisfy the triangle inequality.\r\n\r\n1. Create the MST $$T$$ of $$G$$.\r\n2. Let $$O$$ be the set of vertices with odd degree in $$T$$ and find a minimal matching $$M$$ in the complete graph over $$O$$.\r\n3. Combine the edges of $$M$$ and $$T$$ to form a multigraph $$H$$.\r\n4. Form an eulerian circuit in $$H$$.\r\n5. Make the circuit found in previous step Hamiltonian by skipping visited nodes ('shortcutting').\r\n\r\nYou now have an approximation of TSP with $$\\rho = 1.5$$. Why does this approximate TSP with $$\\rho = 1.5$$, you ask? Well:\r\n\r\n\r\n\r\n## Converting problems\r\n\r\n### VCP to SCP\r\n\r\nThis is how to turn a VCP into an SCP. We use the variables_G_ = (_V_,_E_) for the VCP graph, and _U_ and _S_ for the SCP variables.\r\n\r\n1. Let _U_ be _E_.\r\n2. Let _S\\_i_ be the set of edges touching vertex _i_.\r\n\r\nAnd that's it!\r\n\r\n\r\n# Appendix: list of algorithms featured in the textbook\r\n\r\n* DPKP\r\n* Ford-Fulkerson\r\n* Vertex cover algorithm\r\n* B&B for MAXSAT and TSP\r\n* D&C-3SAT\r\n* Local search scheme\r\n* Kernigan-Lin variable-depth search\r\n* Metropolis algorithm\r\n* Simulated Annealing\r\n* Tabu search\r\n* Randomized tabu search\r\n* Genetic algorithm scheme\r\n* Methods for relaxing problems to LP\r\n* Simplex\r\n* Primal-dual scheme\r\n* Primal-dual (MINVCP)\r\n* Greedy makespan schedule\r\n* Algorithm for greedy makespan schedule\r\n* Algorithm for VCP\r\n* Algorithm for SCP\r\n* Algorithm for WEIGHT-VCP\r\n* Algorithm for MAX-CUT\r\n* Greedy-simple KP\r\n* PTAS for SKP\r\n* modified PTAS for SKP\r\n* FPTAS for KP\r\n* TSP \u25b3-ineq 2-approx\r\n* Christofides algorithm\r\n* Sekanina's algorithm\r\n\r\n\r\n# Appendix: proofs\r\n\r\n(TODO: write the actual proofs)\r\n\r\n* HC $$\\le\\_p$$ RHC\r\n* RHC $$\\le\\_p$$ SUBOPT\\_TSP\r\n\r\n\r\n", "child": 195, "article": 10}}, {"pk": 198, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-16T14:51:24.784Z", "edited_by": 3, "parent": null, "title": "Teknologiledelse", "content": ".", "child": 199, "article": 14}}, {"pk": 188, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-15T12:17:45.776Z", "edited_by": 9, "parent": 187, "title": "Artificial Intelligence Methods", "content": "*   Strong/weak AI\r\n*   Agents\r\n    *   Rationality\r\n*   Bayesian networks\r\n    *   Syntax and semantics\r\n    *   Uses and problems\r\n    *   Conditional independence\r\n*   Decision networks/Influence diagrams\r\n*   Reasoning\r\n    *   Case-based reasoning\r\n    *   Instance-based reasoning\r\n*   Markov processes\r\n    *   Markov assumptiom\r\n*   Neural networks\r\n*   Decision trees\r\n\r\n# Bayesian Networks\r\n\r\nA Bayesian network is a model of a set of random variables and their conditional dependencies through a directed acyclic graph. In the graph, the nodes represent the random variables and the edges conditional dependencies.\r\n\r\nMuch of the calculation in a bayesian network is based on Bayes' theorem: $$ P(A|B)=\\frac{P(B|A)P(A)}{P(B)} $$.\r\n\r\n## Bayesian networks in use\r\n\r\n* Good: Reason with uncertain knowledge. \"Easy\" to set up with domain experts (medicine, diagnostics).\r\n* Worse: Many dependencies to set up for most meaningful networks.\r\n* Ugly: Non-discrete models\r\n\r\nBad at for example image analysis, ginormous networks.\r\n\r\n# Markov Chains\r\n\r\nA Markov chain is a random (and generally discrete) process where the next state only depends on the current state, and not previous states. There also exists higher-order markov chains, where the next state will depend on the previous n states. Note that any k'th-order Markov process can be expressed as a first order Markov process.\r\n\r\nIn a Markov chain, you typically have observable variables which tell us something about something we want to know, but can not observe directly.\r\n\r\n## Operations on Markov Chains\r\n\r\n### Filtering\r\n\r\nCalculating the unobservable variables based on the evidence (the observable variables).\r\n\r\n### Prediction\r\n\r\nTrying to predict how the variables will behave in the future based on the current evidence.\r\n\r\n### Smoothing\r\n\r\nEstimate past states including evidence from later states.\r\n\r\n### Most likely explanation\r\n\r\nFind the most likely set of states.\r\n\r\n## Bernoulli scheme\r\n\r\nA special case of Markov chains where the next state is independent of the current state.", "child": 189, "article": 9}}, {"pk": 200, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-16T14:51:54.286Z", "edited_by": 3, "parent": 199, "title": "Teknologiledelse", "content": "# Hvordan ikke stryke i tekled\r\n\r\n1. Gj\u00f8r oppgaver fra webutvilking.org/tekled\r\n2. Pugg matteformlene og regnereglene under\r\n3. (ikke forsov deg til eksamen)\r\n\r\n# Formler\r\n\r\nP = pris = en eller annen $$f(Q)$$, typisk P = 1000 - Q eller liknende.\r\n\r\nQ = produksjonsmengde.\r\n\r\nTC = totalkostnad = PQ\r\n\r\nMC = marginalkostnad = TC'\r\n\r\nTI = totalinntekt, typisk $$10000000 + 1000Q + Q^2$$ eller noe\r\n\r\nMI = marginalinntekt = TI'\r\n\r\n$$\\beta$$ = hvor mye en ting svinger i forhold til markedsverdien\r\n\r\nCAPM = KVM = capital asset pricing model, handler om at verdien til en aksje g\u00e5r mot kapitalmarkedslinjen\r\n\r\nWACC = after tax weighted average cost of capital\r\n\r\n\r\n\r\n", "child": null, "article": 14}}, {"pk": 199, "model": "wiki.articlecontent", "fields": {"lang": "en", "updated": "2013-05-16T14:51:46.989Z", "edited_by": 3, "parent": 198, "title": "Teknologiledelse", "content": ".# Hvordan ikke stryke i tekled\r\n\r\n1. Gj\u00f8r oppgaver fra webutvilking.org/tekled\r\n2. Pugg matteformlene og regnereglene under\r\n3. (ikke forsov deg til eksamen)\r\n\r\n# Formler\r\n\r\nP = pris = en eller annen $$f(Q)$$, typisk P = 1000 - Q eller liknende.\r\n\r\nQ = produksjonsmengde.\r\n\r\nTC = totalkostnad = PQ\r\n\r\nMC = marginalkostnad = TC'\r\n\r\nTI = totalinntekt, typisk $$10000000 + 1000Q + Q^2$$ eller noe\r\n\r\nMI = marginalinntekt = TI'\r\n\r\n$$\\beta$$ = hvor mye en ting svinger i forhold til markedsverdien\r\n\r\nCAPM = KVM = capital asset pricing model, handler om at verdien til en aksje g\u00e5r mot kapitalmarkedslinjen\r\n\r\nWACC = after tax weighted average cost of capital\r\n\r\n\r\n\r\n", "child": 200, "article": 14}}]